<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Building a JavaScript Development Environment</title>
  </head>
  <body>
    <h1>Building a JavaScript Development Environment</h1>
    <p>Starting a new JavaScript project from scratch is overwhelming. This course provides a playbook outlining the key
      decisions you need to make. Build a robust development environment that handles bundling, linting, transpiling,
      testing, and much more.</p>

    <h1>Git and Github</h1>
    <h2>Git</h2>
    <img src="images/buildingajavascriptdevelopmentenvironment/git.jpg" height="225" width="225"/>
    <p>Git is a version control system (VCS) for tracking changes in computer files and coordinating work on those
      files among multiple people. It is primarily used for software development, but it can be used to keep track of
      changes in any files.</p>
    <h3>Installing Git</h3>
    <p>We are going to install Git for source control. To install Git browse to
      <a href="https://git-scm.com">https://git-scm.com</a> and install Git accepting the defaults during
      installation.</p>
    <h2>Github</h2><img src="images/buildingajavascriptdevelopmentenvironment/github.jpg" height="242" width="208"/>
    <p>Browse to <a href="https://github.com">https://github.com</a> and setup an account (or use an existing one).
      Create a new repository (making sure you Initialize the repository with the <code class="hljs">.gitignore:
        Node</code> set so that Git
      ignores the <code class="hljs">node_modules</code> folder. Its huge so you dont want to check it in).</p>
    <p>The final step is to clone the repository to your development machine. You do this by clicking the green
      Clone or download button and then copying the Url that is displayed.</p>
    <p>Now run the <code class="hljs">git clone</code> command from a terminal session:</p>
    <pre><code>git clone https://github.com/maxsage/pluralsight-js-dev-env.git</code></pre>
    <p>This command will clone your repository into a local directory with the same name as your repository.</p>
    <p>There are three commands that let you commit your work to Git and Github</p>
    <ul>
      <li>1. Stage all the files you changed:</li>
      <pre><code>git add . </code></pre>
      <li>2. Commit your changes locally:</li>
      <pre><code>git commit -m "your message here"</code></pre>
      <li>3. Push your work to Github:</li>
      <pre><code>git push</code></pre>
    </ul>

    <h1>Editors</h1>
    <h2>WebStorm</h2>
    <p>There is a wonderful selection of powerful editors these days but the one I will use is WebStorm:</p>
    <img src="images/buildingajavascriptdevelopmentenvironment/webstorm.jpg" height="225" width="225"/>
    <p>WebStorm is many peoples favourite but it is also the only paid option on this list.</p>
    <h2>.editorconfig</h2>
    <p>In WebStorm, VSCode etc. you can drop a file named <code class="hljs">.editorconfig</code> and place it in the
      root of your project you can
      specify how your editor should handle common settings such as tabs v spaces, indent size, line feeds, char sets
      and trailing white space.</p>

    <h1>Package Managers</h1>
    <h2>What is a Package Manager?</h2>
    <p>A package manager keeps track of what software is installed on your computer, and allows you to easily install
      new software, upgrade software to newer versions, or remove software that you previously installed. As the name
      suggests, package managers deal with packages: collections of files that are bundled together and can be installed
      and removed as a group.</p>
    <p>Packages contain metadata, such as the software's name, description of its purpose, version number, vendor,
      checksum, and a list of dependencies necessary for the software to run properly. Upon installation, metadata is
      stored in a local package database. Package managers typically maintain a database of software dependencies and
      version information to prevent software mismatches and missing prerequisites. They work closely with software
      repositories, binary repository managers, and app stores.</p>
    <p>These days virtually every popular language has it's own package manager - yet JavaScript soldiered on for years
      without any compelling package management story.</p>
    <p>Thankfully these days we have a variety of JavaScript package manager's to choose from including:</p>
    <ul>
      <li>Bower</li>
      <li>npm</li>
      <li>JSPM</li>
      <li>Jam</li>
      <li>volo</li>
    </ul>
    <h3>npm</h3>
    <img src="images/buildingajavascriptdevelopmentenvironment/npm.png" height="149" width="338"/>
    <p>npm has matured by fixing friction points for front end developers. Node has grown enormously in popularity and
      bundlers have continued to become more powerful. Today npm is the most popular JavaScript package manager. We
      will use npm for the remainder of this course.</p>
    <h2>Installing Node and npm</h2>
    <p>We are going to install Node and npm. We will also create a <code class="hljs">package.json</code> file that is
      going to store the
      references to all the packages that we are going to use for our development environment.</p>
    <h3>Installing Node</h3>
    <img src="images/buildingajavascriptdevelopmentenvironment/node.png" height="176" width="287"/>
    <p>Browse to <a href="https://nodejs.org/en/">https://nodejs.org/en/</a> and download Node. npm will be installed
      along with Node.</p>
    <h3>Adding package.json</h3>
    <p>Node's package manifest is <code class="hljs">package.json</code>. <code class="hljs">package.json</code> stores
      the list of npm packages that we are using, as well
      as the npm scripts which we will end up setting up later. The top of a <code class="hljs">package.json</code> file
      defines the following
      items:
    </p>
    <ul>
      <li>name - the name of the project</li>
      <li>version - the version of the project</li>
      <li>description - a description of the project</li>
      <li>scripts - scripts (which we will add later) to help automate our processes</li>
      <li>author - the author(s) of the project</li>
      <li>license - the license model for this project</li>
      <li>dependencies - a list of production dependencies</li>
      <li>devDependencies - a list of development dependencies (npm packages with version numbers)</li>
    </ul>
    <p>Now that we have a <code class="hljs">package.json</code> file let's install the npm packages. Open a terminal
      session and type:</p>
    <pre><code>npm install</code></pre>
    <p>This will take a minute or two to download all the packages specified in the <code
      class="hljs">package.json</code> file and place them in
      a folder called <code class="hljs">node_modules</code> in your project directory.</p>
    <h2>Package Security</h2>
    <p>It's worth remembering that packages can be published to npm by anyone. That might make you a bit paranoid.
      However there are people working on this problem - retire.js and The Node Security Platform are two ways that you
      can check your project's dependencies for know vulnerabilities.</p>
    <p>Node Security Platform is my preference at this time. It offers a simple command line interface that you can
      use to automate checking for security vulnerabilities. All you need to do is call <code class="hljs">nsp
        check</code>
      as part of your build.</p>
    <p>Later in the course we are going to run <code class="hljs">nsp check</code> when we run <code class="hljs">npm
      start</code>.
      Admittedly this will slow the start time
      slightly but it has the advantage of notifying you quickly when a security issue exists.</p>
    <p>To install The Node Security Project globally issue the following command from a terminal session:</p>
    <pre><code>npm install -g nsp</code></pre>
    <p>Once the installation is complete you can run a security check for your current project by issuing the following
      command from a terminal session in your project directory:</p>
    <pre><code>nsp check</code></pre>


    <h1>Development Web Servers</h1>
    <h2>Express</h2>
    <img src="images/buildingajavascriptdevelopmentenvironment/express.png"/>
    <p>Express is lightweight but still feature rich. It is highly configurable and can be used for more than just
      static files. It can be used as a dev and production server. If your building API's in Node then this is a big win
      which makes using Express as your development server an easy choice.</p>

    <a href="http://expressjs.com/">http://expressjs.com/</a>
    <h2>Browsersync</h2>
    <img src="images/buildingajavascriptdevelopmentenvironment/browsersync.png" height="100" width="158"/>
    <p>Browsersync makes your browser testing workflow faster by synchronising URLs, interactions and code changes
      across multiple devices.</p>
    <p>Browsersync automatically sets up a dedicated IP address on you network so that you or anyone that can hit the
      IP on your LAN can see your app. The second feature allows you to hit that IP on multiple devices and all the
      devices will remain in sync. This is great for cross-device testing. You can think of Browsersync as a powerful
      and
      free development web server or as a way to augment your existing web server with additional features.</p>
    <a href="https://www.browsersync.io/">https://www.browsersync.io/</a>
    <h2>Setting up Express as a development web server</h2>
    <p>Let's use Express as our development web server because it's powerful, configurable and extremely popular.</p>
    <p>We already installed express when we ran <code class="hljs">npm install</code> earlier because express is one of
      the many
      packages listed in our <code class="hljs">package.json</code> file so now we just need to configure express.</p>
    <p>I like to keep all my build related tools in a single folder called <code class="hljs">buildScripts</code> which
      I keep in the
      root of the project. In this folder create a file called <code class="hljs">srcServer.js</code>.This file will
      configure a
      web server that will serve up the files in our src directory.</p>
    <p>Once this file has been created you can issue the following command from a terminal session:</p>
    <pre><code class="hljs">node buildScripts/srcServer.js</code></pre>
    <h2>Sharing Work-in-progress</h2>
    <h3>localtunnel</h3>
    <img src="images/buildingajavascriptdevelopmentenvironment/localtunnel.png" height="225" width="225"/>
    <p>You can use localtunnel to expose your localhost via a public Url. It punches a hole in your firewall so that
      your local machine can operate as a web server. To get started issue the following command in a terminal
      session: </p>
    <pre><code>npm install localtunnel -g</code></pre>
    <p>Start your web application.</p>
    <pre><code>npm start</code></pre>
    <p>Use the command line interface to request a tunnel to your local server:</p>
    <pre><code>lt --port 3000</code></pre>
    <p>Running Browsersync and localtunnel together can be a compelling combination.</p>

    <h1>Automation</h1>
    <p>The automation tool we will use in this course is NPM Scripts. NPM scripts are declared directly in your
      <code class="hljs">package.json</code> file in the <code>scripts</code> section.</p>
    <img src="images/buildingajavascriptdevelopmentenvironment/npmscripts.png"/>
    <h2>Creating a startup script using npm scripts</h2>
    <p>In a previous module we created <code class="hljs">srcServer.js</code> which configures express to serve up our
      <code class="hljs">index.html</code>. Now let's create an npm script that will start our development environment.
      By
      convention this script should be called start. That way by convention we can just type <code class="hljs">npm
        start</code>
      and it will run our command. We will just put the exact command that we have been typing manually under our
      scripts section:</p>
    <pre>
<code class="json">"scripts": {
    "start": { "node buildScripts/srcServer.js"
},
</code></pre>
    <p>Now we can just open up our command line and issue the command:</p>
    <pre><code>npm start</code></pre>
    <p>It is also worth noting
      that you do not need to specify the <code class="hljs">run</code> keyword when using <code class="hljs">npm
        start</code> or
      <code class="hljs">npm test</code>. Both of those are so commonly used that <code class="hljs">npm</code> doesn't
      require you to type the
      <code class="hljs">run</code> keyword for either of those scripts.</p>
    <h2>Displaying a friendly message when starting development environment</h2>
    <p>It would be nice if we received a friendly message when starting up our development environment. To do that
      let's create a file in buildScripts called <code class="hljs">startMessage.js</code>:</p>
    <pre>
<code class="hljs javascript">var chalk = require('chalk');
console.log(chalk.green('Starting app in dev mode...'));
</code></pre>
    <p>You can see we are referencing chalk which let's us specify the colour of the output that you display to the
      console.</p>
    <p>You can take advantage of a handy convention to ensure that our <code class="hljs">startMessage.js</code> file
      runs before the web server
      starts up. If we create a script in our <code class="hljs">package.json</code> called <code
        class="hljs">prestart</code> then by convention it will run before
      our start script.</p>
    <pre>
<code class="json">"prestart": "node buildScripts/startMessage.js",
</code></pre>
    <p>npm scripts support convention based pre and post hooks so any scripts you prefix with the word pre will run
      before the script with the same name and any script you prefix with the word post will run after the script with
      the same name.</p>
    <h2>Running node security check from npm scripts</h2>
    <p>Add a script called <code class="hljs">security-check</code> to the scripts section that will run the node
      security
      platform security check:</p>
    <pre><code class="json">"security-check": "nsp-check"</code></pre>
    <p>So this is more typing than simply typing <code class="hljs">nsp check</code> so what are the advantages of this
      method? </p>
    <p>Firstly it is more descriptive to type <code class="hljs">npm run security-check</code> than it is to type <code
      class="hljs">nsp check</code>. Secondly, and most
      importantly, remember how previously we had to install nsp globally so we could run it from a command line. With
      npm
      scripts it is not required to install tools globally in order to run them.</p>
    <p>When you run <code class="hljs">npm install</code> to install all the packages that are listed in
      <code class="hljs">package.json</code> they get added
      to a <code class="hljs">.bin</code> folder which is automatically in path when called from npm scripts. This means
      there is no requirement
      to install these packages globally.</p>
    <p>Adding <code class="hljs">security-check</code> to the npm scripts in <code class="hljs">package.json</code>
      means you can run the security check as
      part of the application startup process if required.</p>
    <h2>Adding localtunnel to the npm scripts</h2>
    <p>Add another npm script called <code class="hljs">share</code> to the scripts section of package.json file:</p>
    <pre><code>"share": "lt --port 3000"</code></pre>
    <h2>Running scripts in parallel</h2>
    <p>The <code class="hljs">pre</code> and <code class="hljs">post</code> hooks are handy for running things before or
      after scripts but you will likely find that you
      also want to run multiple things at the same time. To do that we use a package called <code class="hljs">npm run
        all</code>. This is a handy
      way to run multiple items at the same time in a cross platform way - so in other words it will run on Unix,
      Windows and Linux.</p>
    <p>Let's say that we want to run the security check each time we start the app. To do this we can change our start
      script to instead call <code class="hljs">npm run all</code> specifying that we want to run some tasks in parallel
      (the
      security check and start the web server). We create another script called <code class="hljs">open:src</code> to
      start the
      webserver and call that from our <code class="hljs">start</code> script.</p>
    <pre>
<code class="json">"start": "npm-run-all --parallel security-check open:src",
"open:src": "node buildScripts/srcServer.js"
</code></pre>
    <p>Now if we run <code class="hljs">npm start</code> the web server will be started and our security check will be
      performed. We
      also see the message:</p>
    <pre><code>"Starting app in dev mode..." </code></pre>
    <p>from the pre script that calls <code class="hljs">startMessage.js</code></p>
    <p>You will also notice some other messages appearing. If these messages bother you it is possible to run
      <code class="hljs">npm start -s</code> which stands for silent. This will suppress most of the other messages and
      leave you
      with just the messages that you have explicitly requested.</p>
    <p>I would like to create another task that starts up the web server and shares my work via localtunnel without
      having to start two terminal sessions. To do this let's rename the <code class="hljs">share</code> task to
      <code class="hljs">localtunnel</code>. Next we will create a new script called <code class="hljs">share</code>
      below that will handle the
      entire sharing process.</p>
    <pre><code>"share": "npm-run-all --parallel open:src localtunnel</code></pre>
    <p>We can test this out by opening up the terminal and running <code class="hljs">npm share</code>. You can see the
      web server
      starts and localtunnel starts in parallel.</p>


    <h1>Choosing a Transpiler</h1>
    <p>Modern JavaScript is great but you don't have to write your application using JavaScript. You can choose from a
      long list of Transpilers - there are literally over one hundred languages that compile down to JavaScript.</p>
    <p>The standout options from this list are:</p>
    <ul>
      <li>Babel</li>
      <li>TypeScript</li>
      <li>Elm</li>
    </ul>
    <h2>Babel</h2>
    <img src="images/buildingajavascriptdevelopmentenvironment/babel.png"/>
    <p>Babel's selling point is that it allows you to use all the new features of JavaScript (even those that are
      still experimental) in a standards based way. Babel transpiles the latest version of JavaScript down to ES5 so
      you can use all these new features but run them everywhere ES5 is supported. Some of the advantages of Babel
      are:</p>
    <ul>
      <li>Write standardized JS</li>
      <li>Leverage full JS Ecosystem</li>
      <li>Use experimental features earlier</li>
      <li>No type defs, annotation required</li>
      <li>ES6 imports are statically analyzable</li>
    </ul>
    <h2>TypeScript</h2>
    <img src="images/buildingajavascriptdevelopmentenvironment/typescript.png"/>
    <p>TypeScript is a superset of JavaScript so just as ES6 added features to JavaScript TypeScript adds additional
      functionality to JavaScript. Most notably, type annotations which add type safety. This type safety means you
      can enjoy rich auto completion support and safer refactoring by leaning on explicit type signatures. Some of the
      advantages of TypeScript are:</p>
    <ul>
      <li>Enhanced Autocomplete</li>
      <li>Enhanced readability</li>
      <li>Safer refactoring</li>
      <li>Additional non-standard features</li>
    </ul>
    <p>I have found that both type safety and auto completion are quite good when working in a modern editor like
      WebStorm. Remember because ES6 imports are statically analyzable the editor can deterministically index your
      entire codebase and thus provide reliable intellisense support on import and the functions inside.</p>
    <h2>Configuring Babel</h2>
    <p>Babel offers two methods for configuration: a dedicated <code class="hljs">.babelrc</code> file placed in the
      project's root or placing the
      configuration within the <code class="hljs">package.json</code> file. The differences between the two methods are:
    </p>
    <table>
      <tr>
        <td align="right">
          <p>.babelrc</p>
          <ul style="list-style: none">
            <li>Not npm specific</li>
            <li>Easier to read since isolated</li>
          </ul>
        </td>
        <td></td>
        <td align="left">
          <p>package.json</p>
          <ul style="list-style: none">
            <li>One less file in your project</li>
            <li>Place the configuration in a section called babel</li>
          </ul>
        </td>
      </tr>
    </table>
    <p>For this course I am going to transpile our build scripts as well so we can enjoy modern JavaScript when writing
      our build tooling.</p>
    <p>Lets set up our project to transpile our code the moment we hit save. Since we have decided to use the latest
      version of JavaScript in this project we need to ensure it transpiles down to ES5 to ensure it runs in
      environments
      that don't yet fully support the latest versions of JavaScript.</p>
    <p>Create a new file in the root of the project called <code class="hljs">.babelrc</code>. If we look in
      <code class="hljs">package.json</code>
      we can see a number of babel related packages.</p>
    <p>To configure babel doesn't actually take much code. All we need to do is declare one preset that specifies we
      are using the latest standardized JavaScript features (ES 2016 at this time):</p>
    <pre>
<code class="json">{
  "presets": [
    "latest"
  ]
}</code></pre>
    <p>Let's confirm this works by using <code class="hljs">babel-node</code> to transpile one of our build scripts. All
      of our code is currently
      written in ES5 but let's load up <code class="hljs">startMessage.js</code> and change one item to ES6. Currently
      we are using the common js
      pattern which is used for node but we can now use the module syntax instead: </p>
    <pre><code class="javascript">import chalk from 'chalk';</code></pre>
    <p>If we call <code class="hljs">npm run</code> now the build will fail because we are calling some code that node
      doesn't
      understand (the <code class="hljs">import</code> keyword). To fix this we can call <code
        class="hljs">babel-node</code> in our <code class="hljs">prestart</code>
      script instead of calling <code class="hljs">node</code> directly.</p>
    <p>So now we know babel is successfully transpiling our build script. We can also modify the <code
      class="hljs">open:src</code>
      script to run <code class="hljs">babel-node</code> as well. We can then edit <code
        class="hljs">srcServer.js</code> to use the <code class="hljs">import</code> style syntax for
      JavaScript modules and the <code class="hljs">const</code> keyword which was introduced in ES6 as well.</p>

    <strong>FULL DETAIL STARTED HERE I THINK</strong>
    <h1>Bundling</h1>
    <p>These days when you write JavaScript it likely needs to be bundled up before usage. Why? If you are a JavaScript
      Developer its important to understand that npm packages use the commonjs pattern. Node can handle this just
      fine but browsers don't understand it. So you need to bundle npm packages into a format that the browser can
      consume.</p>
    <p>But bundler's aren't just for apps that run in the browser. You may use a bundler to package any JavaScript into
      a single file or strategically into separate files for different portions of your app.</p>
    <p>Imagine you've created an app with five different pages. A powerful bundler can intelligently create separate
      bundles of JavaScript for each page. That way the user only has to download the relevant JavaScript for the first
      page on initial load. This saves bandwidth and speeds page loads.</p>
    <p>Finally remember that bundlers aren't just for the web. You may want to use bundlers if your coding in Node as
      well since Nodes <code class="hljs">require</code> is slow. By bundling your code for Node you can compile away
      the <code class="hljs">require</code> calls which can often improve performance.</p>
    <h2>Module formats</h2>
    <p>There are several module formats to consider including:</p>
    <ul>
      <li>IIFE - Immediately Invoked Function Expressions</li>
      <li>AMD - Asynchronous Module Definition</li>
      <li>CJS - Common JS</li>
      <li>UMD - Universal Module Definition</li>
      <li>ES6 Modules</li>
    </ul>
    <p>Let's look at examples of each of these approaches:</p>
    <h3>Global Variables</h3>
    <pre><code class="javascript">myGlobal;</code></pre>
    <p>We have known for years that global variables should be avoided because it makes code harder to read and
      maintain. When we write these people pick on us in code reviews - and rightly so.</p>
    <h3>IIFE</h3>
    <pre>
<code class="javascript">(function() {
  // my code here
})();</code></pre>
    <p>So we came up with interesting techniques like Immediately Invoked Function Expressions - also called IIFEs.
      This is a way to encapsulate our JavaScript.</p>
    <h3>AMD</h3>
    <pre><code class="javascript">define(['jq'], function (jq) {});</code></pre>
    <p>We also use tools like RequireJS which utilizes the Asynchronous Module Definition pattern to encapsulate our
      code - also know as AMD.</p>
    <p>All the above approaches should now be considered a thing of the past because better more standardized approaches
      exist. So what are those formats?</p>
    <h3>Common JS</h3>
    <p>If you are working in Node you can continue to use Common JS:</p>
    <pre><code class="javascript">var jquery = require('jquery')</code></pre>
    <h3>ES6 Modules</h3>
    <p>If you are working in ES6 or newer versions of JavaScript you can finally enjoy the power of ES6 modules. Yes
      we finally have a standards based way of encapsulating our modules:</p>
    <pre><code class="javascript">import jQuery from 'jquery'</code></pre>
    <h3>Why Use ES6 Modules?</h3>
    <p>ES6 modules are also known as ES 2015 modules since ES 2015 was the official name of the 2015 release which
      released this feature. Now we discussed in a previous module all future JavaScript versions will be named after
      the year of their release.</p>
    <p>So why should we choose ES6 modules? Well first they are standardized. This means, in the future, when the
      platforms you run on have full support for ES6 and modules you wont have to transpile your code. It also means
      anyone joining your team is more likely to feel comfortable with your code.</p>
    <p>ES6 modules are also a win because you cant declare them dynamically. Now this sounds like a drawback because
      it reduces power but this reduced power was a deliberate design decision because it makes our code statically
      analyzable. That's a fancy way of saying that our code can be read and analyzed in a predictable way because the
      behaviour of our imports can't be changed at runtime. <a
        href="https://medium.com/@kentcdodds/misunderstanding-es6-modules-upgrading-babel-tears-and-a-solution-ad2d5ab93ce0#.ghjpkiipp">Misunderstanding
        ES6 Modules, Upgrading Babel, Tears, and a Solution</a></p>
    <p>When code can be analyzed in this way we get benefits that I alluded to earlier when discussing editors. We
      get improved auto completion support since your editor can determine clearly what functions are in scope from each
      imported module. This power leads to other wins such as the ability to quickly alert you to invalid imports, to
      functions that don't exist and so on.</p>
    <p>Effectively choosing ES6 modules mean that your code fails fast. You find out about your mistakes more quickly
      and often in a clearer manner.</p>
    <p>ES6 imports also enable tree shaking (also known as dead code elimination). This is a feature coming soon in
      Webpack 2 and also available in other bundlers such as rollup. In short, this feature reduces the size of your
      final production code by eliminating unused code. For tree shaking to work we need statically analyzable code
      which
      is exactly what we get with ES6 modules.</p>
    <p>ES6 Modules are also easier to read than the more redundant alternatives such as AMD and UMD. You can further
      clean up your code using named imports. Named imports allow you to easily declare variables that reference pieces
      of
      the file that you are importing and default exports which specify clearly how others can consume your module.</p>
    <p>Bottom line - although there are many ways to handle modules in JavaScript. If you are writing new code today
      ES6 modules are the clear, logical and attractive way to get things done. So ES6 modules are the format that we
      will
      use to modulize our code through the rest of the tutorial.</p>
    <h2>Choosing a Bundler</h2>
    <p>Then there are several bundlers to consider such as:</p>
    <ul>
      <li>Require.js</li>
      <li>Webpack</li>
      <li>Browserify</li>
      <li>Roll up</li>
      <li>JSPM</li>
    </ul>
    <img src="images/buildingajavascriptdevelopmentenvironment/webpack.jpg"/>
    <p>In this tutorial we will implement ES6 modules and bundle them via Webpack. Webpack handles more than just
      Javascript. It is even smart enough to intelligently bundle and generate your CSS, images, fonts and even HTML.
      This means you can do things like inline images via base 64 encoding when there small enough to justify saving
      an http request. You can hot reload your CSS changes in memory via the built-in web server.</p>
    <p>Webpack offers strategic bundle splitting. This saves your users downloading all of your JavaScript upfront.
      Instead you can generate separate bundles for different sections of your app so they are downloaded on demand.</p>
    <p>Webpack also offers a built-in web server that supports hot module reloading which means depending on the
      library or framework you are using you may be able to hit save and immediately see the changes without having
      to do a full refresh. This helps speed development because you don't loose client side state. This is especially
      useful on complicated user interfaces and multi-step forms because you don't have to re-fill the form every time
      to
      test your changes. The code is hot reloaded in place and your existing input values and so on are maintained.</p>
    <p>Finally Webpack 2 is nearly here and will offer tree shaking as one of it's key new features.</p>
    <h2>Setting up Webpack</h2>
    <p>Now that we have chosen Webpack for the path forward in this course let's set it up to bundle our code. Webpack
      does so much that it is hard to describe in a sentence but basically Webpack will bundle all our assets up into
      a single file that runs in our target environment. For our demo app our target environment is the web but as you
      will see later we can also use Webpack to generate multiple bundles instead of just one.</p>
    <p>Webpack is configured via <code class="hljs">webpackconfig.js</code> which by convention is placed at the root of
      the
      project. We will
      call the file <code class="hljs">webpack.config.dev.js</code> but later in the course we will create a production
      build which will give a
      clear picture of how to use all this tooling to create a lightweight bundled and minified app for production but
      since this config file is for dev we will add some development specific configurations here. Since we have Babel
      configured to transpile our code we can write our webpack config using ES6 features.</p>
    <pre>
<code class="javascript">import path from 'path';

export default {
  debug: true,
  devtool: 'inline-source-map',
  noInfo: false,
  entry: [
    path.resolve(__dirname, 'src/index')
  ],
  target: 'web',
  output: {
    path: path.resolve(__dirname, 'src'),
    publicPath: '/',
    filename: 'bundle.js'
  },
  plugins: [],
  module: {
    loaders: [
      {test: /\.js$/, exclude: /node_modules/, loaders: ['babel']},
      {test: /\.css$/, loaders: ['style','css']}
    ]
  }
}
</code></pre>
    <p>Webpack is configured via a single object that we define in webpack config. The Webpack docs go into great
      detail on the wide variety of options that you can configure but I'm showing a very simple setup for this course.
      Let's walk through each setting briefly so we can see what it's doing. As you can see Webpack is configured by
      exporting a single object. As you can see <code class="hljs">debug</code> is set to true - this enables some
      debugging
      information
      to be produced as we run our build. The <code class="hljs">devtool</code> is set to <code class="hljs">inline-source-map</code>.
      There are
      a number of dev tools to consider. The basic trade off here is compilation speed vs source map quality. Higher
      quality source map settings take longer to generate.</p>
    <p>Im setting <code class="hljs">noInfo</code> to false. I know this name is unfortunate but setting it to false
      will mean that
      Webpack will display a list of all the files it is bundling. I typically turn off this data during real
      development
      since it adds a lot of noise to the command line but we will have it on at first just so we can see what is going
      on.</p>
    <p>Now we need to define the <code class="hljs">entry</code> point of our application. You can pass an array of
      entry points
      and this is a good way to inject middleware for hot reloading but I am going to ignore hot reloading just to keep
      things simple here and just define our applications entry point as <code class="hljs">src/index</code> and I use
      the magic
      global <code class="hljs">__dirname</code> which is part of node so I can make sure we get a full path here and I
      am using the
      <code class="hljs">path</code> package that comes with node to get this done. And you can see I use the same
      approach anywhere
      that I'm resolving paths within this file.</p>
    <p>We set the <code class="hljs">target</code> to <code class="hljs">web</code> for this demo but we could, of
      course, set this to node if
      we were using webpack to build an app running in node and that would change the way Webpack bundles our code so
      that node could work with it instead of the browser. There are other notable targets that you could include here
      like electron which is useful for building desktop style apps with JavaScript.</p>
    <p>Now we need to define the <code class="hljs">output</code> so here we tell Webpack where it should create our dev
      bundle.
      Now this is confusing because, as you will see, with our development configuration Webpack wont actually generate
      any physical files. It will just create a bundle in memory and serve it to the browser but we need to define a
      path and name so it can simulate the physical file's existence. We will use nodes <code
        class="hljs">__dirname</code> variable
      to get the current directory and specify that our app will ultimately run from the src folder but again note that
      this wont actually write any files. We will setup a build process that generate files for production later in this
      course. We set the <code class="hljs">filename</code> to <code class="hljs">bundle.js</code>.</p>
    <p>We can optionally define some <code class="hljs">plugins</code> to enhance Webpack's powers. Examples include hot
      reloading,
      catching errors, linting styles and much more. We will add a plugin here later in the course but for now we dont
      need one so we can leave this array empty.</p>
    <p>Ok, one last property. We need to tell Webpack the file types that we want it to handle. Webpack calls this
      concept loaders. Loaders teach Webpack how to handle different file types. As you can see we want to handle
      JavaScript. The great thing about Webpack is that is can handle a lot more than JavaScript. As you can see with
      the
      second loader. I have taught Webpack to handle CSS as well.</p>
    <p>I could add other loaders to handle SASS, images, Less and More. So why teach Webpack to handle more than
      JavaScript? Well adding a loader here means I can import those file types at the top of my JavaScript files and
      Webpack will intelligently bundle the files together for me. It's also worth noting there are various alternative
      syntaxes that work here so other examples you see may look a little bit different.</p>
    <p>Now I'm just scratching the surface with what you can do with Webpack. I show a more complex approach in my
      <a href="'https://app.pluralsight.com/library/courses/react-redux-react-router-es6/table-of-contents">Building
        Application with React and Redus in ES6</a> course or there is an excellent <a
        href="https://app.pluralsight.com/library/courses/webpack-fundamentals/table-of-contents">Webpack
        Fundamentals</a>
      course here on Pluralsight if you want to dive deeper.</p>
    <p>Now if you've never seen Webpack before that probably felt intimidating but Ive come to really appreciate how
      terse Webpack's setup is. We just declared many decisions in under thirty lines of code. Webpack is designed to
      cover our use case really well which means we don't have to write much code to get a lot of power from our
      build process.</p>
    <p>Ok we have setup Webpack to bundle our app's JavaScript but we need to update our development web server to
      run Webpack and serve up our bundled JavaScript.</p>
    <h2>Demo: Configure Webpack with Express</h2>
    <p>We have configured Webpack but to actually put it to use we need to also setup our development server to serve
      out Webpack bundle so let's configure express to make that happen.</p>
    <p>To do so we will go over into the <code class="hljs">buildScripts</code> folder and open <code class="hljs">srcServer.js</code>
      since this is where we configure
      express. First I will add two imports to the file. We will import webpack and the <code class="hljs">webpack.config.dev</code>
      that we
      just created:</p>
    <pre>
<code class="javascript">import webpack from 'webpack';
import config from '../webpack.config.dev';
</code></pre>
    <p>Then below where we have created an instance of express:</p>
    <pre><code class="javascript">const app = express();</code></pre>
    <p>Let's call webpack and pass it the config that we referenced up above so we have a reference to the Webpack
      compiler:</p>
    <pre><code class="javascript">const compiler = webpack(config);</code></pre>
    <p>We can put that to use by calling <code class="hljs">app.use</code> which is a way for us to tell express other
      things we
      would like to use. We are going to tell express to use our Webpack dev middleware and we will pass it the
      compiler we defined earlier on line 9.</p>
    <pre>
<code>app.use(require('webpack-dev-middleware')(compiler, {
  noInfo: true,
  publicPath: config.output.publicPath
}));
</code></pre>
    <p>Inside the call to <code class="hljs">app.use</code> we will define two options. The first one will tell it not
      to display
      any special info. We will also configure our public path so here we are just referencing a variable that we
      defined when we setup our Webpack config. That's all it takes for us to integrate Webpack with express. Of course
      this isn't very useful yet since our demo app doesn't contain any JavaScript yet. Our application does nothing
      except say Hello World! so in the next clip let's fix that by creating an application entry point to test all this
      out.</p>
    <h2>Demo: Create App Entry Point</h2>
    <p>We've wired up Webpack to bundle our JavaScript and we've setup express as our dev server to serve our app but
      we haven't actually written any JavaScript yet. So now let's create our JavaScript file and see Webpack in action.
    </p>
    <p>When we setup webpack.config we set our entry point to index.js:</p>
    <pre>
<code>entry: [
  path.resolve(__dirname, 'src/index')
]
</code></pre>
    <p>So let's create <code class="hljs">index.js</code> in the root of our <code class="hljs">src</code> directory.
      Now in an upcoming module
      we'll build an app to show how the entire development environment works but for now let's do something simple just
      to show how Webpack is able to bundle two JavaScript files into a single bundle.</p>
    <pre>
<code>import numeral from 'numeral';

const courseValue = numeral(1000).format('0,0.00');
console.log(`I would pay ${courseValue} for this awesome course!`(l
</code></pre>
    <p>Here you can see that I'm using the numeral package that was installed at the beginning of the course with all
      our other dependencies that are listed in package.json. This is a handy library for formatting numbers. So I set
      a constant called <code class="hljs">courseValue</code> with a value of one thousand and then I formatted it
      giving it the
      format string <code class="hljs">'$0,0.00'</code>. I am just going to use <code class="hljs">console.log</code> to
      output a message. Also,
      take note, I am using the ES6 tipplet string feature so make sure you are using back ticks around the string. This
      tells JavaScript to parse any variable placeholders that it finds inside.</p>
    <p>So now we've actually written some JavaScript for Webpack to bundle but of course we need to reference the
      final bundle in our HTML file so let's open that up and we'll add the appropriate script tag just before the
      ending <code class="hljs">&lt;body&gt;</code> tag:</p>
    <pre><code>&lt;script src="bundle.js"&gt;&lt;/script&gt;</code></pre>
    <p>And remember when we configured Webpack (<code class="hljs">webpack.config.dev.js</code>) we told it to place
      <code
        class="hljs">bundle.js</code> in
      the root of our <code class="hljs">src</code> directory. Again, it's not going to actually write a physical file
      for
      development but it will simulate it's existence in this directory and that's why we can just reference
      <code class="hljs">bundle.js</code> in our <code class="hljs">index.html</code> file.</p>
    <p>Alright, let's fire up the app and see if we've made magic happen:</p>
    <pre><code>npm -start</code></pre>
    <p>We still see our Hello World message but if we hit inspect now we can see we are writing to the console like
      we expected to. And if we go to the Network tab and refresh we can see that bundle.js is getting sent down
      like we would expect it to. So numeraljs (the library which we used to format this number) should be here inside
      of
      our bundle. I am going to scroll down to line 63 in the bundle here's the code that we wrote: </p>
    <pre>
<code class="javascript">var courseValue = (0, _numeral2.default)(1000).format('$0,0.00');
console.log('I would pay ' + courseValue + ' for this awesome course!');
</code></pre>
    <p>You can see that the template string that we used has been transpiled down to ES5 and if we scroll down a little
      bit further to line 70 you can find the <code class="hljs">numeral.js</code> package so we know that all of our
      JavaScript is getting
      bundled into a single file by WebPack.</p>
    <p>Now remember how we also taught Webpack how to handle CSS by defining a CSS loader in our Webpack config. Let's
      put that to use as well. Let's handle a stylesheet with Webpack in the next clip.</p>
    <h2>Demo: Handling CSS with Webpack</h2>
    <p>In a previous clip we taught Webpack how to handle css by defining a CSS loader in our Webpack config. So now
      let's put it to use. Let's create a simple stylesheet in the <code class="hljs">src</code> folder. I will just
      paste
      in two simple styles to prove this is working:</p>
    <pre>
<code class="css">body {
  font-family: Sans-Serif;
}

table th {
  padding: 5px
}</code></pre>
    <p>Now since we taught Webpack how to handle css now all we have to do to put this to use is add a single line
      in our application's entry point which is <code class="hljs">index.js</code>. All I have to add to the top this
      file is:</p>
    <pre><code class="hljs">import './index.css';</code></pre>
    <p>Just like I import a JavaScript file I can import a css file. Look's pretty weird but trust me it works.</p>
    <p>So if we restart our app:</p>
    <pre><code>npm start</code></pre>
    <p>Now we can see that the "Hello World!" message has a different font applied because our css stylesheet is now
      being applied.</p>
    <p>So might be wondering how did that work? Well behind the scenes Webpack parsed our stylesheet and then it used
      JavaScript to inject the stylesheet onto the page.</p>
    <p>Let's come over to the network tab and reload</p>
    <p>If we pull up bundle.js. Let's see if we can find our stylesheet right in here. Search for the word padding.
      There it is on line 102 you can see that our stylesheet was bundled into our JavaScript and then is being
      injected into the page.</p>
    <p>Now we could use a similar approach to handle SASS, Less, images and more. Of course, for production, you will
      likely want to generate a traditional separate file and I will show how to do that in the production build
      module at the end of this course. </p>
    <p>Now as I scroll through the code you might notice it doesn't look like our source code. That's because this is
      the transpiled and bundled code that Babel and Webpack produced. So you might be wondering how we can debug
      our transpiled and bundled JavaScript code. Well the answer is sourcemaps so let's explore sourcemaps in the next
      clip.</p>
    <h2>Sourcemaps</h2>
    <p>Now that we are bundling our code there is another important tool that we need - Sourcemaps. Once we start
      bundling, minifying and transpiling our code we create a new problem. Our code becomes an impossible to read
      mess when it's running in the browser. So you might be wondering - how do I debug? Hey that's commendable that
      you are concerned. Thankfully this is solveable problem. The solution is to generate a Sourcemap. Sourcemaps
      map the bundled, transpiled and minified code back to the original source.</p>
    <p>This means that when we open our browsers developer tools and try to inspect our code we'll see the original
      ES6 source code that we used. It's like magic. Now the sourcemaps can be generated automatically as part of our
      build process. You might be wondering how minifying the code actually saves any bandwidth if we have to generate
      a big map back to the original source. That's a good question. The beauty of sourcemaps is that they are only
      downloaded if you open the developer tools. So this way you users wont even download the sourcemaps but they
      will be available for you in case an issue arises in either your development environment or in production.</p>
    <p>So effectively sourcemaps give you all the benefits of being able to read your original code without any
      additional cost to regular users.</p>
    <h2>Demo: Debugging via Sourcemaps</h2>
    <p>Alright, now that we have set the stage let's configure our build to automatically generate sourcemaps as
      part of the buildProcess. When we setup Webpack for development we told Webpack to generate a sourcemap by
      specifying the <code class="hljs">devtool</code> setting in <code class="hljs">webpack.config.dev.js</code>
      as shown here:</p>
    <pre><code class="javascript">devtool: 'inline-source-map',</code></pre>
    <p>There are many potential settings to consider by I am using inline source map for this course. I encourage you
      to experiment with the different settings to find one that's best for you. As you can see from this table:</p>
    <p>The basic trade off is between sourcemap quality and speed. Higher quality sourcemaps take a little bit longer
      to create so you will notice a little more delay on larger apps. </p>
    <p>Let's jumpback to our source code. My preferred approach for setting breakpoints is to type the word
      <code class="hljs">Debugger</code> on the line where I would like the breakpoint to hit.So let's set debugger
      right here in <code class="hljs">index.js:</code></p>
    <pre>
<code class="javascript">import './index.css';

import numeral from 'numeral';

const courseValue = number(1000).format('$0,0.00');
debugger;
console.log(`I would pay $(courseValue) for this awesome course!`);
</code></pre>
    <p>I'll hit save and now let's jump back over to the browser and reload. When we do, we can see our breakpoint
      hit.</p>
    <p>As you can see the browser sees the debugger statement and it breaks on a line where I typed debugger and
      since we are using Sourcemaps the original code that we wrote is displayed in the console even though the actual
      code that is running in the browser looks like the code that I showed you earlier that was to read.</p>
    <p>This is really handy because in a later module we'll set up a production build that minifies our code for
      production
      and still, because of our sourcemap, we'll be able to easily read the code just like you see here:</p>
    <p>because we'll be seeing our original source code.</p>
    <p>And again, if you want to see the source code that is running in the browser you can click on
      <code class="hljs">bundle.js</code> to see the transpiled and bundled code that is actually being parsed by the
      browser.</p>
    <p>Alright let's wrap up this module. It's time for another summary.</p>
    <h2>Summary</h2>
    <p>In this module we began by considering our options for bundling our code into reusable and encapsulated
      modules.</p>
    <p>We briefly looked at IIFE, AMD, UMD and CommonJS but we saw that ES6 is the future because its standardized and
      statically analyzable which enables rich features such as Autocompletion support, Deterministic refactoring and
      reduced bundle sizes via tree shaking assuming that you select a bundler that supports it.</p>
    <p>Then we moved on to discussing bundlers. I chose Webpack for this course because it's very popular,
      extremely powerful and highly configurable but Browserify, Rollup and JSPM are all excellent alternatives to
      consider.</p>
    <p>Then we implemented ES6 modules and bundled our code via Webpack. We closed out this module by discussing and
      generating our sourceMaps. Sourcemaps are awesome because they allow us to see our original source code in the
      browser when we open the developer tools. This way we can set breakpoints and debug in the browser even after our
      code has been transpiled, bundled and minified and since they are only requested by the browser when dev tools are
      open they don't slow down the customer's experience in any way.</p>
    <p>In the next module let's protect ourselves from mistakes, and enforce consistency in our codebase. It's time
      to setup an automatic linting process so we are notified when we make typos and errors as soon as possible.</p>


    <h1>Linting</h1>
    <h2>Intro</h2>
    <p>You shouldn't have to keep track of your teams entire list of coding standards in your head. As much as possible
      we should automate the task away and we should be notified immediately when we make a typo. JavaScript linters can
      deliver on both of these concerns. JavaScript linters are so powerful they can catch many errors at compile time.
      It's wonderful finding out that you made a mistake the moment you hit save instead of waiting till runtime to hunt
      some cryptic issue down.</p>
    <p>So in the module we will start by asking why do we need a linter at all? Then we will quickly consider the
      linting
      tools available. We will spend the rest of the module discussing the most popular linter ESLint. We will discuss
      the
      long list of configuration approaches to consider and we will setup linting to run all our rules every time we hit
      save.</p>
    <p>Simple enough - let's get rolling.</p>

    <h2>Why Lint?</h2>
    <p>So why do you need a linter. I see two core reasons. First, a linter programmatically enforces consistency.
      Once you've chosen JavaScript coding standards as a team a linter can help enforce those programmatically and
      provide rapid feedback so issues are caught during development instead of potentially slipping by during code
      reviews.</p>
    <table>
      <tr>
        <td align="right">
          <h3>Enforce Consistency</h3>
        </td>
        <td>&nbsp;</td>
        <td>
          <h3>Avoid Mistakes</h3>
        </td>
      </tr>
      <tr>
        <td align="right">Curly brace position</td>
        <td>&nbsp;</td>
        <td>Extra parenthesis</td>

      </tr>
      <tr>
        <td align="right">confirm/alert</td>
        <td>&nbsp;</td>
        <td>Overwriting function</td>
      </tr>
      <tr>
        <td align="right">Trailing commas</td>
        <td>&nbsp;</td>
        <td>Assignment in conditional</td>
      </tr>
      <tr>
        <td align="right">Globals</td>
        <td>&nbsp;</td>
        <td>Missing default case in switch</td>
      </tr>
      <tr>
        <td align="right">eval</td>
        <td>&nbsp;</td>
        <td>debugger/console.log</td>
      </tr>
    </table>
    <p>Examples include enforcing the position of curly braces or warning about the use of built-in features that
      your team has decided to avoid like confirm and alert. Many teams prefer to use nicely styled dialogue boxes
      instead of the native implementation of these features. Linting helps programmatically restrict their usage.</p>
    <p>Leaving in a trailing comma or forgetting to add a trailing comma if your team prefers trailing commas as a
      a standard, or declaring a global variable or disallowing the use of eval since it is potentially dangerous and
      often misused. All of these are examples of enforcing consistency through a linter</p>
    <p>Second a linter helps avoid mistakes. Just consider the long list of potential mistakes you can make writing
      JavaScript today. Such as adding an extra parenthesis when wrapping a statement or overriding a function when
      you don't realise that the function with the same name already exists. How about performing an assignment in a
      conditional when you almost certainly meant to perform a comparison instead - it's really easy to leave out that
      extra equals sign. Or forgetting to define a default case in a switch statement which can lead to hard to debug
      fall through issues. How about leaving debugging related junk in your code like <code class="hljs">debugger</code>
      or <code class="hljs">console.log</code>. It's easy to accidentally leave these in and have them slip into
      production. With a linter you will know immediately when you do and you can even fail the build on your continuous
      integration server when a developer overlooks warnings and commits any of these issues.</p>
    <p>This list just scratches the surface. So assuming your sold on using a linter the next question is which linter
      linter should you use. Let's explore that in the next clip.</p>


    <h2>Linters</h2>
    <p>Ok now that you're hopefully sold on the benefits it's time to pick a linter.</p>
    <h3>JSLint</h3>
    <p>JSLint was the original, created by Douglas Crockford many years ago. It's extremely opinionated and while
      some consider that a feature the public has largely moved on to more powerful and configurable alternatives
      today.</p>
    <h3>JSHing</h3>
    <p>One such alternative is JSHint. JSHint is an improvement on JSLint which offers more configurability but in
      recent years the most popular linter by far has become ESLint.</p>
    <h3>ESLint</h3>
    <p>ESLint has become so powerful and configurable that I am not aware of any good reason to choose JSLint or JSHint
      anymore. ESLint has become the defacto standard. So bottom line, this choice is really quite easy, my suggestion
      is
      to just use ESLint. Now actually I cant quite say just use ESLint because if you choose to use a language that
      compiles to JavaScript you might have to use an alternative Linter. For example, at the time of this writing, none
      of
      these linters support linting TypeScript although ESLint is expected to add support in the near future. If your
      working in TypeScript then for now you will need to choose TSLint instead. Now that we have chosen ESLint let's
      explore our different options for configuring it in the coming clips.</p>

    <h2>ESLint Configuration Decisions Overview</h2>
    <p>Much like the rest of life in JavaScript land ESLint is filled with decisions. Let's explore some of the key
      decisions we will make configuring it. Choosing ESLint was the easy part, the hard part is the next set of
      decisions we need to make. We need to:</p>
    <ol>
      <li>We need to choose a config format?</li>
      <li>We need to decide which built-in rules that we would like to enable?</li>
      <li>We need to decide whether to use warnings or errors?</li>
      <li>We need to consider extending ESLint's power by adding plugins for the framework or your choice.</li>
      <li>Finally if all this sounds overwhelming you can just punt on the whole thing and use a preset instead</li>
    </ol>
    <p>Let's discuss each of these decisions so you are clear how to configure ESLint best for your needs.</p>
    <h2>Decision 1: Configuration File Format</h2>
    <p>Let's begin with decision 1. Where should you put your configuration. The number of different ways to configure
      ESLint is just plain silly. There are five different filenames that it currently supports for configuration or you
      can add your configuration to package.json. So how do you decide? Well the most common universal approach is to
      create a dedicated .eslintrc file using one of the filename and formats mentioned on the previous slide but
      assuming you are already using npm you can also configure ESLint in package.json. The advantage of configuring
      via a separate file is that it is universal. It is not tied to npm but using package.json avoids using yet another
      file in your application.</p>
    <p>To configure ESLint via package.json add a section called eslintConfig. The contents of this section will be
      the same as the .eslintrc approach and we will walk through the contents of the eslintrc in a moment. </p>
    <h2>Decision 2: Which Rules?</h2>
    <p>After choosing a configuration method decision 2 is: which rules should we enable? ESLint catches dozens of
      potential errors out of the box such as:</p>
    <ul>
      <li>comma-dangle: require or disallow trailing commas</li>
      <li>no-dupe-args: disallow duplicate arguments in function definitions</li>
      <li>no-extra-semi: disallow unnecessary semicolons</li>
    </ul>
    <p>I suggest gathering as a team and deciding once and for all which of these rules are worth enabling. Yes it
      will be a painful meeting but once you get it done and it is in your starter kit, it's settled an you can
      enjoy the benefits.</p>
    <h2>Decision 3: Warnings or Errors?</h2>
    <p>Now that you've decided which rules you want to enable you have yet another decision to make. Which of your
      rules should merely produce warnings and which rules are a big enough deal to justify throwing errors instead.
      Let's consider the implications of Warnings versus Errors.</p>
    <table>
      <tr>
        <td align="right">
          <h3>Warning</h3>
        </td>
        <td>&nbsp;</td>
        <td>
          <h3>Error</h3>
        </td>
      </tr>
      <tr>
        <td align="right">Can continue development</td>
        <td>&nbsp;</td>
        <td>Breaks the build</td>

      </tr>
      <tr>
        <td align="right">Can be ignored</td>
        <td>&nbsp;</td>
        <td>Cannot be ignored</td>
      </tr>
      <tr>
        <td align="right">Team must agree: Fix warnings</td>
        <td>&nbsp;</td>
        <td>Team is forced to comply</td>
      </tr>
    </table>
    <p>With a warning it doesn't break the build so you can continue with development without fixing the issue. In
      contrast errors actually break the build which can be helpful when the linter finds a more critical issue that
      should catch your attention immediately and keep you from moving forward. But since warnings don't stop
      development
      they can ultimately be ignored. This is handy in the moment when your focused on implementing a feature and don't
      want to stop your flow to fix a minor issue but it also means that warnings can potentially be committed because
      they may not break the build. Errors, in contrast, are a clear wall to moving forward - they can't be ignored.
      Due to these trade offs I have seen some shops only use warnings because they favour moving as fast as possible
      and
      I have seen other shops use only errors because they favour stopping any work that isn't good enough to commit at
      that moment.</p>
    <p>Now I suggest using both. Warnings are good for minor stylistic issues and errors are useful for items that are
      likely to produce bugs. The bottom line is it's important that your development team agrees that warnings are not
      acceptable. If you commit code that produces warnings then quickly your linting output will get so noisy that it's
      useless and it will mask other helpful output like test results that also display on the same command line.</p>
    <p>If you choose errors then you don't have to worry about people ignoring linting issues. They will be forced to
      comply because the application wont build. In summary I recommend choosing carefully based on context. You'll
      likely decide that only some rules warrant throwing an error versus a warning.</p>
    <h2>Decision 4: Plugins?</h2>
    <p>Now that you have implemented ESLint's built in rules you have another decision to make. Should you enhance
      ESLint with some plugins for your library or framework of choice and if so which ones? For instance I primarily
      work in React and in my React courses you can see how I use eslint-plugin-react to perform a number of additional
      checks that are specific to React and similar plugins are available for other popular frameworks like angular as
      well as for node.js. There is a handy list of ESLint configs, plugins, parsers and more at this url:</p>
    <a href="github.com/dustinspecker/awesome-eslint">github.com/dustinspecker/awesome-eslint</a>
    <p>As you'll see there are plugins for many popular frameworks and libraries available. These plugins are useful
      because they help enforce a consistent style in the way that your team works with the framework of your
      choice.</p>
    <h2>Decision 5: Preset</h2>
    <p>Did those first four decisions feel pretty overwhelming. If so then this option allows you to avoid the
      decisions that I just talked about. Instead you can just decide to use somebody else's preset. There are multiple
      popular ways to handle declaring your ESLint rules. The most obvious option is to start from scratch as we just
      discussed. This way you can take the full list of rules, work through it one by one and build up your own list of
      settings that's perfect for the way your own team want's to do development. </p>
    <p>However, if everything we just talked about sounds like too much work, there is certainly some good ways around
      it. ESLint comes with a preset that implements a logical set of defaults that can save you a lot of time. It
      decides what is an error or warning for you and enables the rules that it thinks make the most sense for most
      people. I prefer to use ESLint's standard rules as a good shortcut to get started then I tend to tweak a few of
      the
      settings based on our team's feedback. This is a nice compromise because it avoids the work of starting from
      scratch but it still offers complete power in tweaking the settings as your team sees fit. You can even go a step
      farther and use an existing set of rules like airbnb, xo or standard js. Assuming that you don't mind decisions
      that they have made this is a great way to avoid spending a lot of time arguing about all the decisions that I
      have
      just discussed. It is also worth noting that standard JS isn't actually a standard! In fact, ironically, many of
      the
      rules that it enforces like disallowing semicolons and only allowing single quotes for strings are actually quite
      unpopular in the JavaScript community.</p>
    <p>All of these options use ESLint but they enforce strong opinions so you don't have to make any decisions
      configuring rules. In fact, with standard JS, you can't change any rules at all. Assuming that you are willing to
      accept the strong opinions in these presets you have a very low friction way to get started. Now that we have
      talked about configuration of ESLint we also need to discuss a couple of issues that are likely to trip you
      up.</p>
    <h2>Watching Files with ESLint</h2>
    <p>Now that we've decided we are using ESLint there are a few different ways to actually run it. Of course the
      simplest way to run ESLint is via the command line, however, there is one obvious limitation with this approach.
      ESLint doesn't currently include a watch setting built in so if you want to automatically run ESLint each time
      that you hit save running ESLint by itself wont work. Here are two ways to get around ESLint's lack of file
      watching capability.</p>
    <table>
      <tr>
        <td align="right">
          <h3>eslint-loader</h3>
        </td>
        <td>&nbsp;</td>
        <td>
          <h3>eslint-watch</h3>
        </td>
      </tr>
      <tr>
        <td align="right">Re-lints all files upon save.</td>
        <td>&nbsp;</td>
        <td>ESLint wrapper that adds file watch</td>

      </tr>
      <tr>
        <td align="right"></td>
        <td>&nbsp;</td>
        <td>Not tied to webpack</td>
      </tr>
      <tr>
        <td align="right"></td>
        <td>&nbsp;</td>
        <td>Better warning/error formatting</td>
      </tr>
      <tr>
        <td align="right"></td>
        <td>&nbsp;</td>
        <td>Displays clean message</td>
      </tr>
      <tr>
        <td align="right"></td>
        <td>&nbsp;</td>
        <td>Easily lint tests and build scripts too</td>
      </tr>
    </table>
    <p>First, since we are using Webpack one option is to use eslint-loader so webpack will run ESLint each time we
      run our build. The advantage of this approach is all files being bundled by webpack are re-linted every time that
      you hit save so you see an ongoing summary of any linting issues. However, I recommend going a different route
      and using an npm package called eslint-watch.</p>
    <p>This npm package is simply a wrapper around eslint that adds file watching capability. So this stands alone
      and isn't tied to webpack in anyway. So you can use this approach to linting regardless of the bundler that you
      choose. eslint-watch also adds some other nice tweaks like better looking warnings and error messaging and it
      displays a message when linting comes back clean unlike eslint-loader which is completely silent when there are
      not linting issues. Finally, the biggest win with this approach is that you can easily lint all your files even if
      they are not being bundled as part of your app. So this means, you can lint your tests, webpack.config, and any
      build scripts as well. I really like this so that I can ensure that all the code in my project is held to the same
      standard and has a consistent style.</p>
    <h2>Linting Experimental Features</h2>
    <p>Another issue you may run into is ESLint doesn't support many experimental JavaScript features at this time.
      Now ESLint supports all of ES6 and ES7 natively and is expected to continue adding support for standardized
      features as JavaScript progresses. It also supports object spread even though that is currently an experimental
      feature. What if you want to use other experimental JavaScript features? Well then you will likely want to
      use babel-esLint instead because it also lints stage 0-4 features.</p>
    <p>You can view links to the different
      experimental JavaScript features under the plugins page on Babels web site. As you can see JavaScript features
      progress through different stages. They are initially just ideas which are stage 0. Stage 1 is a formal proposal.
      Stage 2 is a draft spec. Stage 3 is a completed spec with initial browser implementations. So as you can see
      risk goes down as the stages progress. Bottom line, if you want to use these experimental features you'll want
      to use babel-eslint. For this course I am going to use only standardized JavaScript features so we will use
      plain ESLint and run it via eslint-watch. Of course if you choose to use experimental features you will need
      to configure Babel with the appropriate plugins to transpile your code as well.</p>
    <h2>Why Lint Via an Automated Build?</h2>
    <p>Now as we've been talking through this process you might be wondering why we should be bothering to lint via an
      automated build process because many editors offer ESLint integration built in so they just monitor your code and
      output results inline right there within the editor. However I prefer to integrate ESLint with my build process
      for multiple reasons:</p>
    <ol>
      <li>Outputting all feedback on my code to the command line gives me one single place to check for all the
        feedback related to my code quality. This means I have one place to check, not just for linting issues, but also
        any compile time errors or any testing errors.
      </li>
      <li>This is especially helpful on teams where developers use
        different editors. We all have the same development workflow because we all utilize the same starter kit and
        a command line. Pair programming is also easier when everyone has the same development process.
      </li>
      <li> Most importantly
        ESLint should be part of your build process so that the build is broken on your continuous integration server
        when someone commits any code that commits a linting error. This helps protect your application from slowly
        getting sloppy. Even if a developer ignores ESLint locally the build can be rejected automatically by your
        continuous integration server.
      </li>
      ,
    </ol>
    <h2>Demo: ESLint Set Up</h2>
    <p>Alright let's jump back into the editor and configure ESLint so that we get rapid feedback as we code. We just
      went over a long list of decisions so here's the plan: </p>
    <ul>
      <li>We are going to use ESLint's built in rules so we don't have to waste time.</li>
      <li>We will use eslint-watch to add watch capability so that our files are linted and reported to the command
        line the moment that we hit save.
      </li>
    </ul>
    <p>To help us quickly catch mistakes, maintain consistency and enforce best practices we are going to use ESLint to
      lint our code. Every time that we hit save it will run. To configure ESLint we can either place a .estlintrc file
      in the root of our project or we can configure it in our package.json. I prefer the separate file so lets create
      a .eslintrc.json.</p>
    <p>As a quick note in my previous course on React and Redux in ES6 I used .eslintrc file without an
      extension but recently ESLint has declared a filename without an extension to be deprecated so I have added the
      .json extension that you see here.</p>
    <p>To get the rules that we are going to use I recommend coming over to GitHub Gist at this shortened url:</p>
    <a href="http://bit.ly/jsdeveslint">bit.ly/jsdeveslint</a>
    <pre>
 <code class="json">{
  "root": true,
  "extends": [
    "eslint:recommended",
    "plugin:import/errors",
    "plugin:import/warnings"
  ],
  "parserOptions": {
    "ecmaVersion": 7,
    "sourceType": "module"
  },
  "env": {
    "browser": true,
    "node": true,
    "mocha": true
  },
  "rules": {

  }
}
</code>
    </pre>
    <p>
      So lets walk through our eslintrc.json file. First you can see that I am declaring this as the root eslint file.
      By default ESLint will look for configuration files in all parent folders up to the root directory. So just in
      case you have a eslint file in some parent directory I am adding this for safety. This tells eslint that this is
      the project root so it shouldn't look in any parent folders for other configuration files. So this will help
      assure that you don't get odd behaviour, this will be the only eslint file applying to our project. As you can
      see we are using ESLint's recommended rules. This enables many warnings and errors based on ESLints
      recommendation. As we saw in the slides we could of course use alternatives like airbnb's rules as a baseline
      if preferred.</p>
    <p>We can also augment the recommended settings with plugins that provide enhanced linting. In our
      case I am going to add plugins for enhanced linting of ES6 imports. This assures that if we create an invalid
      import statement we'll find out about it the moment we hit save.</p>
    <p>Now the parser option section defines the version of JavaScript that we are using. We're using ES7 also known
      as ES2016 since that is the latest standard at the time of this recording. I am declaring that we are using
      standard JavaScript modules.</p>
    <p>The environment section declares some different environments that ESLint should be aware of. These environments
      tell ESLint to expect certain global variables. As you can see we are expecting to work with the browser, node,
      and to run our tests with mocha. Now if you view the full list of environments on ESLint you can see that many
      other frameworks are listed including:</p>
    <ul>
      <li>QUnit</li>
      <li>Jasmine</li>
      <li>Jest</li>
      <li>A variety of other environments</li>
    </ul>
    <p>Finally we can define any rules that we want to override down at the bottom of the file. Let's add a single rule
      just so you can see how to override ESLint's standard rules:</p>
    <pre>
<code class="json">
"rules": {
  "no-console": 1
}
</code>
    </pre>
    <p>I am going to set no-console to a warning. 1 means warning, 2 means error and 0 means off. So if you feel
      strongly about a rule you can set it to 2 and break the build but since writing to the console can be useful
      during development I prefer to set this to a warning so it doesn't break the build. </p>
    <p>Alright, now we have ESLint configured let's set it up to run via an npm script in our package.json file.</p>
    <p>Now we could simply run eslint directly in an npm script but eslint lacks watch functionality so we will use a
      handy npm package called eslint-watch. eslint-watch adds file watching functionality to eslint and also offers
      enhanced command line output. So let's create an npm script that calls eslint-watch:</p>
    <pre><code class="json">"lint": "esw webpack.config.* src buildScripts",</code></pre>
    <p>We will call it <code class="hljs">lint</code> and we'll call <code class="hljs">eslint-watch</code> and we want
      it to watch <code class="hljs">webpack.config.*</code> since soon we will be adding a production version of our
      <code class="hljs">webpack.config</code>, we want it to watch everything in <code class="hljs">src</code> and
      also everything within our <code class="hljs">buildScripts</code> folder. So this should effectively lint all of
      our JavaScript.</p>
    <p>So to clarify <code class="hljs">esw</code> is the executable for
      <code class="hljs">eslint-watch</code> and we're just passing <code class="hljs">eslint-watch</code> the list of
      files we would like it to watch.</p>
    <p>Here is an important point that can trip you up. If your editor has linting built in be sure to disable it.
      Otherwise your editors built in linting may override the rules that we are going to define.</p>
    <p>Now we should be ready to run this on the command line:</p>
    <pre><code>npm run lint</code></pre>
    <p>We can see that we do get some errors back from <code class="hljs">srcServer.js</code> and
      <code class="hljs">startMessage.js</code>. Since we configured ESLint to throw a warning for the
      <code clas="hljs">no-console</code> rule rather than an error which is the default in the ESLint recommended
      rules. You can see that it currently just displaying black text but if we come back to
      <code class="hljs">package.json</code> and add <code class="hljs">--color</code> on the end of the lint command
      this will tell lint to display our issues with colors.</p>
    <img src="asdf"/>
    <p>Now we get nice yellow warnings here and some color here on our counts on the right. Now this first linting
      error is from srcServer.js and it is complaining about the line where we are writing to the console:</p>
    <pre><code class="javascript">console.log(err);</code></pre>
    <p>One way that I can handle this is just to add a comment to the top of this file that disables this rule in this
      file since I am ok with writing to the console in any buildScript files. I just want to make sure that I am not
      writing to the console in my actual application. So we can see that this fixed one of the linting issues.</p>
    <p>Now we can go to <code class="hljs">startMessage.js</code> and we can see that we are writing to the console:</p>
    <pre><code class="javascript">console.log(chalk.green('Starting app in dev mode...'));</code></pre>
    <p>The other way to disable this is to put in a comment at the end of the line:</p>
    <pre><code class="javascript">// eslint-disable-line no-console</code></pre>
    <p>This tells ESLint to disable the rule specified (<code class="hljs">no-console</code> in this example). So
      this can be useful when there are rare exceptions to the rule that we've defined.</p>
    <p>So now if we run <code class="hljf">npm run lint</code> again we should see that linting comes back clean:</p>
    <img src="asdf"/>
    <p>And it does, it reports back clean with this nice green arrow. This is another I like about
      <code class="hljs">eslint-watch</code> is that we get confirmation that it is clean. I like this little extra
      piece of output. </p>
    <p>Linting is now running as expected but it is only running once and then stopping so let's set it up to watch
      our files in the next clip.</p>
    <h2>Demo: Watching Files</h2>
    <p>Let's create another handy script. Oddly <code class="hljs">eslint-watch</code> doesn't watch our files by
      default. Instead you have to pass it a command line flag to enable watch. So let's create a separate npm script
      that will watch our files. We'll call it <code class="hljs">lint:watch</code></p>
    <pre><code class="json">"lint:watch": npm run lint -- --watch",</code></pre>
    <p>So we're passing the watch flag along to our existing lint script. So this is saying run the
      <code class="hljs">npm lint</code> script but pass the watch flag to <code class="hljs">eslint-watch</code>. So
      let's hit save and see if it works:</p>
    <pre><code class="hljs">npm run lint:watch</code></pre>
    <p>So now if I come over to srcServer.js and take out the disable comment we can see ESLint re-runs immediately and
      throws a warning about me using the console statement in the file</p>
    <p>So now we have linting watching our files and now that linting is setup we will know when we make many common
      mistakes in our code. The linting errors will display immediately in our console when we hit save. Now there is
      one final piece that is missing here. We'd like ESLint to run each time that we start our app so we just need to
      add our <code class="hljs">lint:watch</code> task to our start script in <code class="hljs">package.json</code>
    </p>
    <pre><code class="json">"start": "npm-run-all --parallel security-check open:src lint:watch",</code></pre>
    <p>And it will run in parallel since we are already using <code class="hljs">npm-run-all</code> and telling it
      to run any of the scripts that we list on the right in parallel.</p>
    <p>Now if I type:</p>
    <pre><code class="hljs">npm start -s</code> </pre>
    <p>We should see that linting is part of our start now. We get our message, we get our security check and there
      our linting is running as well.</p>
    <img src="asdf"/>
    <p>So now when we type <code class="hljs">npm start</code> it displays the start message, runs webpack, starts our
      development web server, opens the app in our default browser, lints our files and reruns webpack and eslint
      anytime
      that we hit save. That's a lot of power in so little code. Let's wrap up this module with a quick summary.</p>
    <h2>Summary</h2>
    <p>In this module we saw two core reasons to lint.</p>
    <ul>
      <li>Enforce consistency</li>
      <li>Avoid mistakes</li>
    </ul>
    <p>Linting helps enforce consistency so that our code is easier to read and it helps us avoid many common
      mistakes related to typos, globals and accidental assignments. We saw that there are multiple linters but we
      chose ESLint because it is currently the most popular, configurable and extensive linter available. </p>
    <p>We reviewed a variety of configuration choices including: </p>
    <ul>
      <li>Config format</li>
      <li>Which rules?</li>
      <li>Warnings vs errors?</li>
      <li>Which plugins?</li>
      <li>Use preset instead?</li>
    </ul>
    <p>We wrapped up by our enhancing our development environment to use ESLint's recommended rules and run
      ESLint every time we hit save using <code class="hljs">eslint-watch</code>. </p>
    <p>Our development environment is really coming together now but we haven't covered a critical aspect yet. What
      about testing and continuous integration. Let's explore these topics in the next module.</p>


    <h1>Testing and Continous Integration</h1>
    <h2>Intro</h2>
    <p>It's a shame how uncommon automated testing and continuous integration are in JavaScript and I believe
      it's because people don't see a clear picture of how to get quickly started. Since JavaScript has no built in
      opinions on handling testing you need to spend a lot of time browsing the web and investigating strategies before
      you can get rolling.</p>
    <p>In this module I would like to outline the landscape to help you understand the key decisions that you need to
      make because if you'r new to automated testing just picking your tools and deciding how to use them is a major
      hurdle. So we'll begin by reviewing six key decisions that you'll need to make including:</p>
    <ul>
      <li>Testing frameworks</li>
      <li>Assertion libraries</li>
      <li>Helper libraries</li>
      <li>and more...</li>
    </ul>
    <p>Once we've clarified our testing stack we'll jump back into the editor and setup our test environment and write
      our first example tests. We'll close out this module by discussing continuous integration services so that we are
      notified immediately anytime someone breaks the build. We'll setup two different continuous integration
      servers.</p>
    <p>This is a big topic with a lot of ground to cover so let's get rolling.</p>
    <h2>Test Decisions Overview</h2>
    <p>Comprehensively covering JavaScript testing would require multiple courses so in this module I am going to
      focus on the style of testing that is most commonly configured in JavaScript development environments today which
      is unit testing. Unit testing focuses on testing a single function or module in an automated fashion. Unit tests
      often assert that a certain function returns an expected value when passed certain parameters. Unit tests mock
      out external dependencies like apis, database calls and file system interactions so the results are fast and
      deterministic. There are other types of useful automated testing styles for JavaScript that I wont have time to
      cover in this module.</p>
    <p>Two other styles that are worth looking into that we wont cover in this module are integration testing which
      focuses on testing the interactions between multiple modules and automated UI testing which tests the application
      by automating clicks and keystrokes in the actual UI and asserting that it interacts in expected ways. Tools like
      Selenium which automate browser interactions are popular in this space. </p>
    <p>There are various other testing approaches as well but in this module I'll focus on automated unit testing.</p>
    <p>There are no less than six important decisions that you need to consider when setting up automated testing in
      JavaScript:</p>
    <ol>
      <li>You have to choose a framework</li>
      <li>Assertion Library</li>
      <li>Helper Libraries</li>
      <li>What environment do you want to run your tests in</li>
      <li>Where to place your test files.</li>
      <li>When to run your tests.</li>
    </ol>
    <p>In the next six clips let's run through each of these decisions.</p>
    <h2>Decision 1: Testing Framework</h2>
    <p>The first decision we need to make is what testing framework to use. There are a wide variety of testing
      frameworks available. Let's consider the top 6</p>
    <h3>Mocha</h3>
    <p>Mocha is the most popular because it is highly configurable and has a large ecosystem of support.</p>
    <h3>Jasmine</h3>
    <p>Jasmine is nearly as popular as Mocha and quite similar but Jasmine includes an assertion library built in. I
      find Mocha to be more configurable that Jasmine so I personally prefer Mocah over Jasmine.</p>
    <h3>Tape</h3>
    <p>Tape is the leanest and simplest of the bunch. It's simplicity and minimal configuration are it's key strengths.
      QUnit is the oldest on this list and was actually created for testing JQuery by JQuery creator John Resig. Today
      other frameworks like Mocha and Jasmine are much more popular.</p>
    <h3>AVA</h3>
    <p>AVA is a new framework that offers some interesting features. AVA runs your tests in parallel and it only
      re-runs impacted tests - both of which help speed results.</p>
    <h3>Jest</h3>
    <p>Finally, Jest is from Facebook and has recently become quite popular for React developers but since it's really
      just a nice wrapper over Jasmine it's actually quite useful for anyone. Jest has code coverage, JSDOM and popular
      conventions for finding your test files all built in. It's also recently gotten much better so if your looked at
      Jest before and were turned off - it's time to look again. Of course it's easy to feel overwhelmed with all these
      options but I like to think of choosing a testing framework like choosing a gym. Sure, some gyms are nicer than
      others, but you can greatly improve your health at any gym. So the important part is picking a gym - any gym so
      that you can start exercising. In the same way the right answer here is to quickly review this list and pick one.
      There really isn't a loser here so don't worry too much that you've picked the wrong framework. Just like gyms
      it's
      easy to switch to a different one later. In fact switching frameworks often involves merely trivial syntax
      changes.
      The only clear wrong choice is to be this guy:</p>
    <img src="asdf" a>
    <p>Coding and praying. Don't laugh because we all do it. I've spoken to countless developers on this topic and
      I believe that decision overload is largely what's holding people back. So don't be this guy. Pick one of the
      six frameworks above and get moving.</p>
    <p>For this course I'm going to use Mocha because it's popular, mature, flexible and boasts a large eco system of
      support.</p>
    <h2>Decision 2: Assertion Libraries</h2>
    <p>Many test frameworks such as Jasmine and Jest come with assertions built in but some such as Mocha don't come
      with an assertion library so we have to pick our own. </p>
    <p>So what's an assertion. An assertion is a way to declare what you expect. For example here:</p>
    <pre><code class="javascript">expect(2+2).to.equal(4)</code></pre>
    <p>I am asserting that 2+2 should equal 4. This is an assertion because I am telling my test what I expect to
      happen. If the statement is false then the test fails. There are many potential ways to declare assertions in
      test.
      Some frameworks might look like the above example. Other frameworks might use a keyword like assert instead of
      expect:</p>
    <pre><code class="javascript">assert(2+2).equals(4)</code> </pre>
    <p>Don't let the minor differences confuse you. Most of the choice between assertion libraries come down to minor
      syntactic differences.</p>
    <p>The most popular assertion library is chai but there are other assertion libraries out there to consider like
      Should.js and expect. Most frameworks include their own assertions built in but since Mocha doesn't we need to
      choose one. Again the core differences between these are relatively minor syntax differences so dont spend too
      much time worrying about this. In this course we'll use Chai for assertions because it's popular and offers an
      array of assertion styles to choose from. </p>
    <h2>Decision 3: Helper Libraries</h2>
    <p>There's another question to answer before we start writing tests - should we use a starter library and if so -
      which one?</p>
    <h3>JSDOM</h3>
    <p>JSDOM is one interesting library to consider. JSDOM is an implementation of the browsers DOM that you can run in
      Node.js. So with JSDOM we can run tests that rely on the DOM without opening an actual browser. This keeps your
      testing configuration for automated tests simpler and often means that tests run faster because their not reliant
      about running in the browser. So JSDOM is useful when you want to write tests that involve HTML and interactions
      in the browser using Node. We'll write a test using JSDOM later in this module.</p>
    <h3>Cheerio</h3>
    <p>Cheerio is another interesting library worth mentioning. You can think of Cheerio as jQuery for the server. This
      is really handy if your using JSDOM because you can write tests that assert that certain HTML is where you expect
      it. And the great news is - if you understand jQuery you already know how to work with Cheerio because it uses
      jQuery selectors for querying the DOM. Imagine you wrote a test that expects a specific DOM element to exist on
      the page. With Cheerio you can query JSDOM's virtual DOM using jQuery selectors. If you already know jQuery this
      can save some typing compared to writing traditional DOM queries.</p>
    <h2>Decision 4: Where to Run Tests</h2>
    <p>Feeling overwhelmed with options yet? Hopefully not because we are in JavaScript land. So we also need to
      decide where to run our tests. There are three popular approaches to running JavaScript based tests. The most
      obvious option is running our tests in the browser. Karma and Testem are popular test runners for testing in an
      actual browser. However opening an actual browser requires more configuration and is slower than the alternatives
      so I prefer to avoid this approach.</p>
    <p>Instead we an utilize a headless browser like phantomJS to run our tests. So whats a headless browser? A headless
      browser is a browser that doesn't have a visible user interface. PhantomJS is full real browser running the V8
      JavaScript engine behind the scenes but you can't see PhantomJS because it has no visible interface. This is
      useful because often writing automated tests you don't need to see the actual interface. You just need something
      fast that simulates a real browser. I've used this approach successfully in the past as well.</p>
    <p>The third option is to utilize an in memory DOM. As we just discussed JSDOM is a library that simulates an
      actual browser by creating a DOM in memory that we can interact with. You can think of JSDOM as a lighter weight
      alternative to PhantomJS because JSDOM doesn't have a full browser behind the scenes. It's just focused on
      simulating a DOM in memory. The advantage of this approach is it's fast and quick to setup. That said both
      PhantomJS and JSDOM are great options to consider. We're going to write our tests using JSDOM in node in an
      upcoming clip.</p>
    <h2>Decision 5: Where Do Test Files Belong?</h2>
    <p>Decision 5 is where should I put all my tests. There are two popular schools of thought on organizing your test
      files. Let's overview the merits of each approach. One popular approach is to centralize all your tests in a
      folder
      called test or something similar. So all your tests are completely separate from your source code. Mocah pushes
      you in this direction because it defaults to looking for tests in the root of your project in a folder called
      test.
      Now the primary benefit that I hear people claim about this approach is that it avoids adding noise to your source
      code directory. But I find this mindset misguided. Tests aren't noise, they're complimentary to the files their
      testing, there important. To me, if they're worth writing, they're worth seeing regularly in my source instead of
      tucked away in a separate folder.</p>
    <p>Another common reason that I hear is that I don't want my test files deployed to production. As you'll see in the
      production build module at the end of this course this concern is unmerited. We'll only deploy the final bundled
      HTML, JavaScript and CSS for the app to the actual production server. Ultimately I think much of the reason people
      are using a separate test folder is simply inertia. It's popular to create a separate test folder or project in
      many
      server side technologies for other reasons but the separation just doesn't make sense on JavaScript apps so I
      prefer to place my tests alongside the file under test. Here's why. I find it makes importing easier because the
      paths are trivial to work with. Since the test and the file under test are in the same path, imports are clean.
      It's
      always dot, slash, file under test. This sure beats managing a lot of dot, dot slashes to reference some source
      code folder that's in a totally different spot. Second, it provides clear visibility to our tests, they're not
      buried in a separate folder, they're right there in our source, so it's quite easy to notice a file that lacks a
      corresponding test file. Again, to me, test file visibility is asset not a liability. Third, placing them together
      makes it easy to open them both at the same time. If you open a file to write code, you should probably be writing
      some tests at the same time so co-locating files that your work on at the same time just makes sense. Co-location
      also avoids having to maintain two separate directory structures. When you separate tests from your source you
      often end up having to create new folders with the same name in two different places. I'm not a fan of repeating
      myself or saying the same thing twice. Ok bad pun,, moving on. Finally it's more convenient when we refactor and
      move files as well. When tests are centralized, moving the file under test requires updating the path in the
      corresponding test file. When tests are placed alongside the file under test it's easy to simply drag files to
      their
      new location without making a path change. The relative paths remain the same.</p>
    <p>This is a minor side note but I was also curious about the naming conventions people are using for naming
      JavaScript files.</p>
    <p>As you can see naming test files with a suffix of .spec and .test are very popular conventions.</p>
    <h2>Decision 6: When Should Tests Run?</h2>
    <p>Ok one final decision to make regarding testing. When should our tests run? Well if we're talking about unit
      tests the answer is simple. Unit tests should run everytime that you hit save. This rapid feedback loop ensures
      that your notified immediately of any regressions and running tests each time you hit save facilitates test driven
      development since you can quickly see your tests go from red to green by hitting Ctrl+S. If you run your tests
      manually it creates unnecessary friction. When the test suite is run manually it's easy to forget to run the test
      suite after making a change. So make it automatic and reduce friction.</p>
    <p>Finally running tests on save increases the visibility of the tests that do exist. It helps to keep testing in
      the forefront of your mind. So I believe this is an easy decision. Your unit tests should run automatically when
      you hit save. I know what your'e thinking - I can't run my test suite every time I save. That would be way too
      slow. Well, I should emphasize, I'm talking about unit tests here. Unit tests should run extremely fast.
      Integration
      tests are also useful and admittedly slower so you'll want to run those separately. But your unit tests should
      be fast because they should'nt hit external resources. Now let me backup for a moment and clarify the differences
      between unit tests and integration tests. Unit testing is about testing a single small unit of code in isolation.
      Integration testing is about testing the integration of multiple items. So unit testing often involves testing a
      single function all by itself while integration testing often means firing up a browser and clicking on the real
      UI using an automation tool like Selenium and often making actual calls to a Web API though you can of course
      write integration tests using just Node and JSDOM for instance. And since unit tests seek to test a small portion
      of code in isolation they run extremely quickly. Quick enough that you should be able to run all your unit tests
      every time that you hit save. In contrast integration tests are slower because they often require real external
      resources like browsers, web apis and databases that take much longer to spin up and respond than native function
      calls. Now since unit tests run fast they should be run every time you hit save. If you're unit tests don't run
      fast enough to re-run every time that you hit save that is often a sign that they're not really unit tests. But
      since integration tests typically interact with slow external resources they're often run on demand or in QA. In
      summary the answer to when your tests should run comes down to whether your'e writing unit tests or integration
      tests. We're going to run unit tests in this module so we'll run our tests every time that we hit save.</p>

    <p>Ok, so we've walked through the six big decisions you need to make for JavaScript testing. Now let's summarize
      the decisions I've settled on for this course. I am going to use:</p>
    <ul>
      <li>Testing Framework - Mocha</li>
      <li>Assertion Library - Chai</li>
      <li>Helper Library - JSDOM</li>
      <li>Where to run tests - Node</li>
      <li>Where to place tests - Alongside source files</li>
      <li>When to run tests - Upon save</li>
    </ul>
    <h2>Demo: Testing Setup</h2>
    <p>As you just saw one of the hardest parts of JavaScript testing is just choosing an approach but now that we have
      a planned path let's set things up. We'll begin by creating a file that will configure our tests. We'll put it
      under <code class="hljs">buildScripts</code> and call it <code class="hljs">testSetup.js</code> and I'll paste in
      the two lines of code with some comments:</p>
    <pre>
      <code class="javascript">
        // This file isn't transpiled, so must use CommonJS and ES5

        // Register babel to transpile before our tests run.
        require('babel-register')();

        // Disable webpack features that Mocha doesn't understand.
        require.extensions['.css'] = function () {};
      </code>
    </pre>
    <p>So this file does two things. First we are requiring <code class="hljs">babel-register</code> so this will tell
      Mocha that first babel should transpile our tests before Mocha runs those tests and then second we're going to
      disable any webpack specific features that Mocha doesn't understand. In this case the css extension because
      remember in our <code class="hljs">index.js</code> we are requiring <code class="hljs">index.css</code>. This is a
      feature that webpack understands but Mocha does not. So we're just telling Mocha that if it's sees this just
      treat it like an empty function. Now there are other things we could do here like setup a JSDOM environment but
      instead of doing that here I will show how to handle it in an example test. Now that we have our initial test
      setup script configured let's jump over to <code class="hljs">package.json</code> and a script that will run
      our tests via Mocha:</p>
    <pre><code
      class="json">"test": mocha --reporter progress buildScripts/testSetup.js \"src/**/*.test.js\""</code></pre>
    <p>First we specify the reporter we want to use, the reporter setting determines how the test output should
      display. I prefer to use the progress reporter because it's clean and simple but Mocha offers a variety of
      interesting reporters to choose from. However I recommend sticking with progress for our setup because many of
      the other reporters write so much information to the terminal that it can make it hard to see the linting issues
      reported in that same terminal.</p>
    <p>Next we tell Mocha to run the test setup script that we just setup and then after it's finished running that
      it should run any tests that it finds within our source directory and any subdirectories. And we define test files
      as any file that ends in <code class="hljs">.test.js</code>. And now that we've set this up we can open up the
      terminal and type <code class="hljs">npm test</code> to run Mocha.</p>
    <code class="hljs">Error: cannot resolve path (or pattern) 'src/**/*.test.js'</code>
    <p>When we do we see it fail. This is Mocha's way of saying that it cant find any test files. That's not surprising
      because we haven't written any tests yet. So let's create one test. We'll assume that we're writing tests for our
      index file so I'll create a file in the same path that's called <code class="hljs">index.test.js</code> so we're
      following the convention of naming tests after the file under test but with <code class="hljs">.test.js</code> on
      the end. Some prefer <code class="hljs">.spec.js</code> but what ever you like is fine.</p>
    <p>Now, Mocha doesn't come with an assertion library so we're going to use Chai and more specifically we'll use the
      expect style that comes with Chai so I will use a named import so that we have a reference to expect.</p>
    <pre><code class="javascript">import {expect} from 'chai';</code> </pre>
    <p>Now we can describe our first test. </p>
    <pre>
<code class="javascript">describe('Our first test', () => {

  });
</code>
    </pre>
    <p>We will provide it a function. You can, of course, use the function keyword if you prefer. I am just using an
      arrow function here for brevity. Inside let's add our first test:</p>
    <pre>
<code class="javascript">describe('Our first test', () => {
  it('should pass', () => {
      expect(true).to.equal(true);
    })
  });
</code>
    </pre>
    <p>So now we have our first test so if we come back down here and run <code class="hljs">npm test</code> again we
      should see it pass:</p>
    <pre>
<code class="hljs">[-------------------------------------]

  1 passing (111ms)
</code>
    </pre>
    <p>This is the output of the progress reporter. We should also see that if I set this to false that our test fails.
      Sure enough it does, we get a useful error message that shows the line that it failed:</p>
    <pre>
  <code class="hljs">
  1) Our first test should pass:

    AssertionError: expected true to equal false
    + expected - actual

    -true
    +false

    at Context.&lt;anonymous&gt; (src/index.test.js:5:21)
  </code>
    </pre>
    <p>We thought we'd get true but we got false instead. So I will undo my change there. So we now have our test
      passing. In the next clip let's create a test for something in the DOM using JSDOM.</p>
    <h2>Demo: DOM Testing</h2>
    <p>Let's add a second test that put's JSDOM to use. To do that let's first import JSDOM and we'll also need to
      import <code class="hljs">fs</code> which comes along with node. It stands for filesystem and let's us interact
      with the filesystem using node. So let's describe this one as <code class="hljs">index.html</code> because that
      is going to be the file that we are going to want to test in this case.
    <pre>
<code class="javascript">import {expect} 'chai';
import jsdom from 'jsdom';
import fs from 'fs';

describe('index.html', () => {

});
</code>
    </pre>
    <p>And inside we're going to say that it should say hello:</p>
    <pre>
<code class="javascript">
it('should say hello', () => {

});
</code>
    </pre>
    <p>Remember that we have a "Hello World" sitting inside of here so we're just going to write a test that confirms
      that markup is there. So first let's get a reference to our <code class="hljs">index.html</code> file and hold
      it in memory. To do that I am going to say:</p>
    <pre>
<code class="javascript">
const index = fs.readFileSync('./src/index.html', "utf-8");
</code>
    </pre>
    <p>The above code references our index.html file and specifies it is in utf-8 format. So now we have the contents
      of our index.html file held in memory within a constant called index. So we're ready to now use JSDOM. So let's
      say:</p>
    <pre>
<code>
jsdom.env(index, function(err, window) {
  const h1 = window.document.getElementsByTagName('h1')[0];
  expect(h1.innerHTML).to.equal("Hello World!");
  window.close();
});
</code>
    </pre>
    <p><code class="hljs">jsdom.env()</code> is our way of defining the JSDOM environment. We will pass it our
      index.html file because this constant represents the contents of index.html. If you want JavaScript to run as part
      of your JSDOM environment you can pass an array of JavaScript files as the second parameter but note that if any
      of those files utilize fetch you need to use isomorphic fetch instead because fetch is a browser feature so it
      wont be available by default in the node environment. We don't need any JavaScript for this particular test so
      I'll just omit the second parameter.</p>
    <p>The second parameter for this is a callback function which is run after JSDOM is completed pulling index.html
      into memory making a virtual DOM in memory. And it takes two parameters: an error and a window argument. The
      window here represents the window in the browser. Just like you could say window when your'e in the browser now
      you can do so right here in node because we have a virtual DOM in memory. So we're going to write a test that
      confirms this text is there. To do so we want to get a reference to that <code class="hljs">h1</code> so let's
      define a constant called <code class="hljs">h1</code> and then say
      <code class="hljs">window.document.getElementsByTagName('h1')[0];</code> and the tagname that we are looking
      for is <code class="hljs">h1</code>. Now this returns an array like object so I am just going to say give me the
      first <code class="hljs">h1</code> on the page because we know that our <code class="hljs">h1</code> is the first
      one on the page.</p>
    <p>So we now have a reference to the <code class="hljs">h1</code> on the page we are ready to write our assertion.
      So we will say we expect the <code class="hljs">innerHtml</code> of <code class="hljs">h1</code> to equal it's
      value which is "Hello World!". Finally we'll go ahead and close the window just to free up memory that was taken
      when we created our in memory DOM.</p>
    <p>So we'll hit save and then we should be ready to come down here and enter <code class="hljs">npm test</code> or
      if you want to save some typing <code class="hljs">npm t</code> does the same thing.</p>
    <pre>
<code class="hljs">
2 passing (203ms)
</code>
    </pre>
    <p>And you can see that we have two tests passing. Now just to prove that this is working let's go ahead and change
      the exclamation point to a question mark:</p>
    <pre><code>expect(h1.innerHTML).to.equal("Hello World?");</code></pre>
    <p>If we hit save then re-run our test we should see it fail but we don't. Interesting, and you're probably
      surprised like I am that that's still passing but there is a good reason for this. When we call JSDOM there is an
      asynchronous call that occurs here. We have to setup our test to be asynchronous. To do that, when we call it
      our function that we define here takes a parameter, and we'll call this done:</p>
    <pre>
<code class="javascript">
describe('index.html', () => {
    it('should say hello', (done) => {
    const index = fs.readFileSync('./src/index.html', "utf-8");
    jsdom.env(index, function(err, window) {
      const h1 = window.document.getElementsByTagName('h1')[0];
      expect(h1.innerHTML).to.equal("Hello World!");
      done();
      window.close();
    });
  });
});
</code></pre>
    <p>What we need to do is tell Mocha that our test is done and then it will run the expect and report our results
      after it sees done here. If we do this then re-run our tests we should see it fail:</p>
    <pre><code class="hljs"> 1 passing (208ms)
 1 failing

 1) index.html should say hello:

   Uncaught AssertionError: expected 'Hello World!' to equal 'Hello World?'
   + expected - actual

   -Hello World!
   +Hello World?

   at Object.done (src/index.text.js:16:31)
   at node_modules/jsdom/lib/jsdom.js:320:18
 </code></pre>
    <p>And indeed now we do, and the world makes sense again. You can see it was expecting it to be an exclamation
      point but what it received was a question mark. So just remember when your doing an asynchronous test, one that
      involves having a callback you need to add the done parameter so Mocha now knows it's now safe to evaluate
      whether your expect is now true or false.</p>
    <p>We can now change the test back to an exclamation point and re-run and we can see it's passing. Of course I'm
      just scratching the surface here but now we have a pattern for testing real DOM interactions without having
      to fire up a browser. </p>
    <p>Now that we have a couple of tests written it would be nice if our tests ran every time that we hit save. So
      let's set that up in the next clip. </p>
    <h2>Demo: Watching Tests</h2>
    <p>Another detail is missing in our tests. We shouldn't have to run our tests manually. So let's run them every
      time that we hit save. To to that we'll add a script to <code class="hljs">package.json</code> called
      <code class="hljs">test:watch</code>:</p>
    <pre><code class="json">"test:watch" "npm run test -- --watch"</code></pre>
    <p>And you should see this looks almost identical to <code class="json">lint:watch</code>. We're using the same
      pattern of telling the test script to run but we're passing another parameter to it with this
      <code class="hljs">-- --watch</code> syntax. So it's just as though I had taken this
      <code class="hljs">--watch</code> flag and added it in <code class="hljs">test</code> script above.</p>
    <p>Now of course we also want to run this as part of our <code class="hljs">start</code> script:</p>
    <pre><code class="json">"start" "npm-run-all --parallel security-check open:src lint:watch test:watch",</code></pre>
    <p>So now let's run the app and see how it works.</p>
    <pre><code class="hljs">npm start -s</code></pre>
    <p>And we can see the app starts up just fine. Of course there's much more to testing than this. You could setup
      mocking, code coverage, reporting and more. Now that we have testing setup it would be nice if we could fail the
      build if someone commits broken tests so in the next clip let's begin by exploring continuous integration.</p>
    <h2>Why Continuous Integration?</h2>
    <p>While we're talking about assuring quality with testing there's another important practice to consider -
      continuous integration. When your team commits code it's handy to confirm immediately that the commit works as
      expected when on another machine. That's what a continuous integration server is for or CI server for short. So
      let's wrap up this module by setting up a continuous integration server to ensure that we're notified when
      someone breaks the build. </p>
    <p>Im sure youve heard that annoying guy Jimmy say this to you multiple times:</p>
    <p>"Weird it works on my machine"</p>
    <p>Well thanks Jimmy that was super helpful. Wouldn't it be nice to find out right away when someone has broken the
      build or you made a bad commit that has broken the build and ruined someone else's day. That's what a continuous
      integration server is for. </p>
    <p>Now the question that you might be asking is how do we end up in a situation where it works on our machine but
      it breaks on the CI server. Well the CI server catches a number of potential mistakes.</p>
    <ul>
      <li>Have you ever forgotten to commit a new dependency</li>
      <li>Have you ever installed an npm package but forgotten to save the package reference to package.json</li>
      <li>Maybe you added a new step to the build process but perhaps it doesn't run cross platform</li>
      <li>Perhaps the version of node that you are running locally is different than the one your using in production.
        So the app might work just fine when running on your machine but fail on the Continuous Integration Server
      </li>
      <li>Maybe someone just completed a merge but made a mistake somewhere along the way that broke the build.</li>
      <li>Finally, perhaps someone on your team committed a change without running the test suite. In this case I
        typically recommend covering their whole desk in aluminum foil.
      </li>
    </ul>
    <p>The good news is that with a CI Server you don't have to worry. It will catch the culprit and notify the
      developer of his or her transgression. These are just a few great reasons to run a CI Server. The point is a
      CI Server catches mistakes quickly.</p>
    <h2>What Does Continuous Integration Do?</h2>
    <p>So what does a CI Server do to provide all these benefits? Well first it builds your application automatically
      the moment that you commit. This ensures that your application builds on another machine. Sure beats the all too
      common alternative where hours or days later someone get's latest and complains that someone broke the build. A
      CI Server makes it clear who broke the build by checking every commit.</p>
    <p>It also runs your test suite. Of course, you should be running your tests before committing but a CI Server
      ensures it always happens. And it ensures that the tests pass on multiple machines. If your tests don't pass then
      your commit has issues so it's important to have a separate server run your tests to make sure they pass on more
      than just your machine.</p>
    <p>A CI Server can run tasks like code coverage and reject a commit if code coverage is below a specified
      threshold. And finally, although it's not required, you can even consider automating deployment using a CI Server.
      With this scenario, if all these aforementioned checks pass, your application is automatically deployed to
      production.</p>
    <h2>Choosing a CI Server</h2>
    <p>There are multiple continuous integration servers to consider that work great for JavaScript apps.</p>
    <img src="travis"/>
    <p>Travis CI is a Linux based continuous integration server. </p>
    <img src="appveyor"/>
    <p>Appveyor is a Windows based continuous integration server.</p>
    <img src="jenkins"/>
    <p>Jenkins is another popular and highly configurable option.</p>
    <img src="circleci"/>
    <p>Circle CI is another interesting option to consider.</p>
    <img src="semaphore"/>
    <p>Semaphore is another interesting option to consider.</p>
    <img src="SnapCI"/>
    <p>Snap CI is another interesting option to consider.</p>
    <p>Performance, features and configurability of course differ between these options. Travis and Jenkins are the
      most popular and thus have the largest ecosystem of support but Travis is a hosted solution while Jenkins is a
      good choice if you prefer to host your CI Server on your own. Appveyor is notable because of it's Windows
      support.</p>
    <p>In this course we'll setup two continuous integration servers: TravisCI and Appveyor. Why two? Because
      TravisCI runs on Linus and Appveyor runs on Windows. This means we can be assured that our build process runs on
      MAC, Linux and Windows. On my current team, developers run both MAC and Windows so using both TravisCI and
      Appveyor helps assure that our build runs on both platforms.</p>
    <h2>Demo: Travis CI</h2>
    <p>Let's get back in the code and setup continuous integration using both TravisCI and Appveyor. TravisCI is a
      popular continuous integration server. It offers handy integration with GitHub which makes it quick and easy to
      add to your project. Assuming that you're using GitHub for your source control it's quite straightforward to
      integrate TravisCI as your continuous integration server on your JavaScript app.</p>
    <p>Now, as you can see, I've been using GitHub for this demo. I showed you how to install Git and setup a
      repository in the intro module of this course. Now you can sign in to TravisCI using your GitHub account. When you
      do you should see an empty list of repositories:</p>
    <img src="travis"/>
    <p>Obviously you can see I have some existing repositories. You can click this plus sign to add a new repository.
      When you do you will see a list of all you repositories and I have to scroll down a bit because I have a lot of
      them. The one that I want to turn on is <code class="hljs">js-dev-env-demo</code>. To turn it on I just click the
      X and that enables Travis CI. I can click the gear icon to change some settings but we don't need to change
      anything in here. We do want it to run on Build pushes and for Build pull requests. The defaults are just fine.
    </p>
    <p>And now that we have set this up on the website we can finish configuring Travis by going back into the editor
      and creating a configuration file for Travis. Travis is configured via a .travis.yml file. Inside we're going
      to declare just two things:</p>
    <pre>
<code class="yml">
language: node_js
node_js:
  - "6"
</code>
    </pre>
    <p>We going to declare that the language we are working with is node.js and then we can add a list of versions. I
      am just going to check version 6 but I could have added other lines here to have it check version 5, version 4 and
      so on. And that's all it takes to configure Travis for our continuous integration server when we're working with
      GitHub.</p>
    <p>Now, before we commit our changes to fire off our first continuous integration, let's open up
      <code class="hljs">index.html</code> because I'm going to change line 7 to have a question mark instead of an
      exclamation point. This should break our JSDOM based test. In this way we can see what it looks like when a
      build fails on our CI Server.</p>
    <p>And now we ready to commit our changes which should fire off our first continuous integration build on TravisCI.
      Now there's Git integration built in to VSCode and many popular editors but I'm going to use Git via the command
      line because it will work the same for everyone. If I do a <code class="hljs">git status</code> right now I can
      see
      that I have all my changes for this module that are listed.</p>
    <img src="gitstatus"/>
    <p>I'm just going to add all of these as staged changes:</p>
    <pre><code class="hljs">git add .</code></pre>
    <p>The dot after add means "add all changed files". </p>
    <p>The minute that I do you can see that they are now listed as staged changes here in my editor. So now they are
      ready to commit so I will say:</p>
    <pre><code class="hljs">git commit -m "module 9 work in progress"</code></pre>
    <p>Hit enter and when I do this commits. Now this just committed locally. If we actually want to push our
      changes up to GitHub we need to issue:</p>
    <pre><code class="hljs">git push</code></pre>
    <p>We can now see that it wrote and pushed my changes up to gihub.com:</p>
    <img src="changes"/>
    <p>We can prove that that's true by coming back over to github.com and refreshing the page and seeing that there is
      now a new commit:</p>
    <img src="mod9wip"/>
    <p>Now the other thing that's interesting is we should be able to come over to Travis CI and see it working. We
      can see that it is running so it was watching for a commit. If I click on this we can see the details of the
      status in progress.</p>
    <img src="travisbuild"/>
    <p>We can see that it's installed the latest version of Node 6 and is displaying the version. Once this is
      complete we'll be able to see the results - so I'll just pause for a moment until this is done. Ok it finished and
      the build failed. And this is a great example of why a CI Server is useful. It installed node, ran our tests, and
      we can see that it failed. Remember I deliberately changed <code class="hljs">index.html</code> to make the
      CI build fail. And if I run <code class="hljs">npm t</code> to run our test we should see it fail in the editor
      as well. So if I open <code class="hljs">index.html</code> up and change the question mark back to an exclamation
      point, hit save and now if I run <code class="hljs">npm t</code> in the browser it succeeds. It should also
      succeed in TravisCI once I commit my change.</p>
    <p>So now if I issue a <code class="hljs">git add .</code> command and then issue a
      <code class="hljs">git status</code> command. The one file that I just added has been staged:</p>
    <pre>
<code class="hljs">
| => git status
On branch master
Your branch master is up-to-date with 'origin/master'.
Changes to be committed:
  (use "git reset HEAD &lt;file&gt;.." to unstage)

      modified srd/index.html
</code>
    </pre>
    <p>So I can now issue the command:</p>
    <pre><code class="hljs">git commit -m "Fix broken test"</code></pre>
    <p>And now I can issue the command:</p>
    <pre><code class="hljs">git push</code></pre>
    <p>This will push my change up to the server. So now if we browse back to travis-ci.org that Travis CI has kicked
      off again. So we'll see whether I have fixed my test.</p>
    <img src="TravisPass"/>
    <p>Great now we can see it came back green. It installed node, it ran my tests so everything looks good. Now we
      know that our application runs not just on my machine but on a separate integration server. This gives us more
      confidence that the application is ready for production.</p>
    <p>So now we know that our app builds and our tests pass within a Linux environment but what if your working in a
      Windows environment. For that let's check out an alternative CI server called Appveyor</p>
    <h2>Demo: Appveyor</h2>
    <p>We just saw how to setup continuous integration on a Linux server using Travis CI. Now it's time for Windows so
      we're going to use Appveyor as an alternative to Travis CI that runs one Windows. Now, as you can see, here on
      Appveyor.com you can click to sign up for free and once you do you can sign in using your existing GitHub account.
      You will need to authorized it by clicking Authorize application.</p>
    <p>Once logged in you can view your projects. I am going to click new project. As you can see Appveyor supports
      a number of different online repositories but we're going to look at GitHub.</p>
    <p>Now I can see a list of all my GitHub repos:</p>
    <img src="githubrepos"/>
    <p>We'll come down here, and select <code class="hljs">js-dev-env-demo</code> from the list and I'm just going to
      click add. So now we're redirected to a page where you can view the build history, deployments and settings. Let's
      click on settings. Here you can change a long list of settings but here, again, the defaults are just fine for
      our purposes. And just like Travis CI we need to jump back into the editor to finish configuring Appveyor.</p>
    <p>Now appveyor is configured with a file called <code class="hljs">appveyor.yml</code> and it should again
      reside out in the root of your project.</p>
    <p>The recommended Appveyor configuration is a little more involved so I'll just paste the following code in
      and we can talk through it:</p>
    <img src="Appv config"/>
    <p>As you can see we are telling Appveyor that we should be using nodejs version 6. We could add other versions
      below with another dash if desired. And the rest of this boilerplate is recommended by Appveyor so we can
      declare that we want to install our npm packages and also run our tests. So we are telling it the specific npm
      tasks it should run.</p>
    <p>And this output is just here because it useful to see the node and npm version that are being run when we
      are trying to debug. And with this file saved we should be able to open the terminal and say:</p>
    <pre><code class="hljs">git add .</code></pre>
    <p>to add this file to staging. Then if we say:</p>
    <pre><code class="hljs">git status</code></pre>
    <p>We should see that this is now staged for us:</p>
    <img src="appveyorstaged"/>
    <p>So let's commit this file:</p>
    <pre><code class="hljs">git commit -m "Add appveyor CI"</code></pre>
    <p>Now that's committed locally let's push that up to GitHub:</p>
    <pre><code class="hljs">git push</code></pre>
    <p>When we do let's go back to the AppVeyor web site and click on latest build. </p>
    <img src="buildprog"/>
    <p>Great and it looked like our build succeeded. If we scroll back to the top we can see the green bar. Just like
      Travis CI if our build had failed we would have received an email notifying us that we had broken the build.
      And just like Travis CI we can see that it installed node, installed our dependencies and ran our tests
      successfully. So we can now feel confident that our development environment runs properly on both Linux and
      Windows.
      Alright that's it for this module. Let's summarize what we just learned.</p>
    <h2>Summary</h2>
    <p>We just saw the long list of decisions you have to make to handle testing in JavaScript. You have to choose a
      testing framework but remember this is like choosing a gym. What's important is that you just pick one and start
      exercising. Then we quickly reviewed assertion libraries. Some frameworks like Jasmine come with assertions built
      in. Others, like Mocha, don't include an assertion library so we used Chai. And you may want to use some helper
      libraries to get things done like JSDOM which provides an in memory DOM and Cheerio which provides a jQuery like
      interface for querying the DOM. We discussed options for where to run your tests including the actual browser, a
      headless browser, or doing in memory testing. I showed the in memory approach using JSDOM. We also reviewed the
      merits of placing your tests alongside the file under test because it speeds navigation, increases visibility and
      avoids having to recreate and maintain a separate file structure for your tests. And remember that unit tests
      should be run every time you hit save. If it's too slow for that then it's likely an integration test which is
      also
      useful but should be handled separately. And we wrapped up this module by reviewing a few popular options for
      continuous integration. We set up both Travis CI and Appveyor so we know that our app builds on both Linux and
      Windows. Great so now we have testing and continuous integration configured so we know we're ready to build a
      quality JavaScript application. Our development environment is working great but some critical pieces are still
      missing. How do we make HTTP requests. And how do we generate Mock data for rapid development when we don't have
      a completed production API. Nearly every JavaScript application needs to handle these issues so let's explore this
      topic in the next module.</p>
    <h1>HTTP Calls</h1>
    <h2>Intro</h2>
    <p>As we saw in the first module JavaScript is eating the world and when it comes to protocols HTTP is eating the
      world too. Virtually every JavaScript application that we build today makes HTTP calls. So, in this module, let's
      explore the libraries for making HTTP calls via JavaScript. These libraries are essential for making Ajax calls in
      modern applications. Then we'll shift our focus to mocking HTTP calls. We'll discuss why mocking is useful so we
      can code without hitting actual API's, rapidly try different response shapes, code offline, and much more. And
      we'll wrap up by considering various approaches for doing mocking HTTP calls. We'll implement a compelling mock
      API that generates realistic fake data and simulates a database using a simple generated file full of JSON. This
      module is about making our development environment interact with the real world and these days the world talks
      HTTP so let's dive in.</p>
    <h2>HTTP Call Approaches</h2>
    <p>There are at least half a dozen popular ways to handle HTTP calls in JavaScript. The library options depend on
      where your running your app. Node provides a built-in package called http. It's a low level library that provides
      basic functionality for making HTTP requests. Now request is a popular higher level library that makes it simpler
      to make these calls in node. It provides a streamlined API that many prefer.</p>
    <p>Now if you're writing JavaScript for the browser you have a different set of options. You can of course use
      plain old XMLHttpRequest also know as XHR for short. This is the native and original way to get things done. It's
      hard to believe but the birth of XMLHttpRequest was over 17 years ago in 1999 and it's been broadly supported in
      browsers for well over a decade. Plain HTTP XMLHttpRequest looks like this:</p>
    <img src="plainhttpreq"/>
    <p>You have to manually check the ready state, use a verbose api to set the request header, and attach to the
      onreadystatechange and error events to get the job done. As you can see it's a lot of plumbing and it looks pretty
      old and creaky these days so most people prefer alternatives that offer a cleaner api.</p>
    <p>So for a long while people have often reached for jQuery to get this job done. jQuery's $.ajax object has been
      the workhorse for the web for years and if your'e already using jQuery in your project it remains a pretty logical
      way to handle http calls because it helps you avoid pulling in extra dependencies.</p>
    <p>These days some more full featured frameworks like angular include their own http service so you don't have
      to make this decision at all.</p>
    <p>But assuming your framework doesn't automatically handle http another increasingly popular option is Fetch which
      is a standard proposed by the Web Hypertext Application Technology Working Group. Yes that's a mouthful, that's
      why people typically call them the What Working Group for short. Now Fetch offers a streamlined API that
      elegantly handles HTTP calls. However, some browsers lack native support so you'll want to use a polyfill with
      this option. You can find polyfills for both the regular version of fetch and the isomorphic version of fetch
      which we're going to talk about in a moment. But it's also worth noting that Fetch is currently a streamlined api
      so it doesn't offer all the features of raw XMLHttpRequest or the other libraries that abstract XMLHttpRequest
      away that we'll talk about in a moment. So, for instance, you can't cancel a fetch at this time but this
      limitation
      is being actively worked. So although Fetch support is currently being added to popular browsers its feature set
      is
      expected to grow over time. Here's an example of using Fetch:</p>
    <!--Fetch example-->
    <p>As you can see, you create a request object, then pass that to Fetch. And since Fetch uses promises to handle
      results you provide a success and error handler to the then function. Full featured libraries like Axios and
      SuperAgent that we'll talk about in a moment are great but the new native Fetch api is likely to provide all the
      power that you'll need.</p>
    <p>So finally, some packages work on both the client and the server. isomorphic-fetch is an npm package that
      provides a fetch like api that runs on the server via node and in the browser. That's why it's called isomorphic
      Fetch. More recently the term Universal JavaScript has become popular to describe JavaScript that runs on both the
      client and the server.</p>
    <p>You can also choose to use XHR which is a package available on npm. XHR provides a subset of the request library
      that we talked about under the Node column but the subset of features that it supports run on both node and the
      browser.</p>
    <p>Now if your'e looking for full featured options. SuperAgent and Axios are popular libraries that run on both
      node.js and the browser. Both are elegant and popular but I personally prefer Axios because it offers a clean
      promise based api. Here's an example of a call using Axios. This code is simple, easy to read and nicely
      declarative. I enjoy the promise based api. </p>
    <!--some axios code-->
    <p>James K Nelson describes Axios as the XMLHttpRequest library which takes all the good parts from Angular's
      $http and throws out everything else. But SuperAgent is quite popular as well. It even has it's own plug in
      ecosystem.</p>
    <p>Now that's a lot of options so let's sum this up. If you're working only on node you'll probably want to use
      request unless you have a good reason to avoid taking on another dependency. If you're in the browser then Fetch
      with a polyfill is the most future-proof approach since you wont need a polyfill once all browsers finish adding
      support. However, this also assumes that you can live with the limitations of Fetch. Finally if your building an
      app that needs to render on both the client and the server then any of the libraries on the far right are a great
      choice. Choosing between them largely comes down to whether you prioritize file size or features. SuperAgent and
      Axios weigh a bit more but also offer more features. And now that you hopefully have a good feel for what library
      is
      the best fit for your needs, in the next clip let's discuss why it's important to centralize api calls.</p>
    <h2>Centralizing HTTP Requests</h2>
    <p>Here's an important key I often see people overlook when making api calls. Make sure they're handled in a single
      spot. So why is this important? Because it centralizes key concerns. First it gives you one place to configure all
      your calls. This way you can declare important configuration like base url's, preferred response type, and
      whether to pass credentials in a single spot. You can make sure all GET, PUT, POST and DELETE calls are handled
      consistently. When asynchronous calls are in progress it is important that the user is aware. This is often
      accomplished via a moving pre-loader icon, commonly called a spinner. By centralizing all your calls you can keep
      track of how many asynchronous calls are in progress. This ensures the pre-loader continues to display until all
      async calls are complete. Centralization also gives you a single place to handle all errors. This ensures that any
      time an error occurs your application can handle it in a standardized way. Perhaps you want to display an error
      dialogue or log the error via a separate http request. By centralizing your api calls a single method can assure
      that this occurs for all calls. Finally, centralizing your api calls gives you a single seam for mocking your api.
      Centralizing your calls means you can point to a mock api instead of a real one by changing a single line of code
      that points to a different base url. We'll discuss this more in an upcoming clip.</p>
    <h2>Demo: Fetch</h2>
    <p>Ok, enough talk, let's make some decisions. For this course, we're just building a web app so let's setup
      Fetch along with the associated polyfill from Github so that we can assure that our code can runs cross browser.
      And while we're doing so let's centralize our api calls for all the reasons that I outlined in the previous
      slide.</p>
    <p>As I mentioned in the slides I prefer to centralize my api calls. Centralization assures that I handle all api
      calls in a consistent way. It creates a clear seam for mocking and it makes displaying a pre-loader while
      asynchronous calls are in progress trivial. But we have a problem, our app doesn't currently have an api. So let's
      use express to create a single api call. For simplicity let's just serve the api using the same express instance
      that's serving our app during development. So let's open srcServer.js and add a new route. Let's create a
      simple endpoint that returns user data. As you can see, when we hit /users it should return a hard coded array
      of a few records. Of course, in a real app, this would hit a database and perhaps be served by a different
      machine on a different web server but I'm just going to hard code some data here instead of setting up a real
      database or a separate api. And now that we have this change we should be able to jump over to the browser
      and confirm this works. So let's say</p>
    <pre><code class="hljs">npm start</code></pre>
    <p>And if I put in /users we can see the JSON getting returned from express as we expected. Now let's update our
      app to call the api using Fetch and display the results on the page. Again, I like to keep all my api calls
      centralized so let's create a folder called api under the src folder. And inside let's create a file called
      userApi.js</p>
    <img src="userApi"/>
    <p>And we can see at the top I'm importing whatwg-fetch so this polyfill will assure that this code runs in
      browsers that don't yet have fetch support natively. And, as you can see, I'm only exporting one public function:
      getUsers(). All the other functions below are private. The actual call that's using Fetch occurs here:</p>
    <pre>
<code class="javascript">
function get(url) {
  return fetch(url).then(onSuccess, onError);
}
</code>
    </pre>
    <p>in the get() function but with this small amount of boilerplate setup adding other get requests will require
      very little code because we only need to provide the url. Fetch along with promise resolution and error handling
      are abstracted away behind this private get function. Right now, I'm only supporting GET but you might want to
      add functions for handling PUT, POST and DELETE requests as well. We'll add support for a DELETE request in a
      following clip. You can think of this file a bit like the repository pattern but in JavaScript. On the server
      the repository pattern is often used to abstract away data access using a course grained api and that's basically
      what we're doing here to except instead of abstracting away a database, we're abstracting away our web api from
      our application. And the beauty of centralizing our calls here is we have one place to consistently handle all of
      our Ajax calls. If one fails we have centralized error handling and if we want to show a pre-loader we have one
      spot to keep track of any calls in progress. And, as you'll see in a moment, if our base url changes in
      different environments we have a single place to configure. Ok, now we have our api setup to use Fetch, let's
      call it. To do so let's jump over to <code class="hljs">index.html</code>. Let's create a table inside
      <code class="hljs">index.html</code>. I'm going to remove the h1 that says "Hello World" and instead paste in
      a simple structure for a table. You can see that we have headers for id, firs name, last name and email because
      this
      is the data structure that we're expecting to receive from our api call. And the table body tag deliberately has
      an id of users because I'm going to reference this in some JavaScript that we write next. </p>
    <p>Now let's jump over the index.js and write some code to populate this table with the results from our api call.
      First, let's add a reference to the api call that we need which is getUsers() and then let's remove all the code
      and replace it with the following because our application is now just going to display user data. Here's some
      code to populate a table of users using our api call. We can see that on line 6 I make the call to getUsers
      and I use the then function on the promise to handle the result that we receive from our api call. I then loop
      through the list of users returned and return a string of html which I then place within the inner html of
      that users table body which we created in index.html. So this code will end up populating our HTML table. Of
      course in a real application I'd likely do this using React or Angular but I want to use plain vanilla
      JavaScript here to avoid adding the complexity of a framework since that's not the focus of this course. And
      now when I hit save we can see that our test fails. And our test fails, because remember, we wrote a rather
      silly test that was making sure that our "Hello World!" message was within the h1. So let's go ahead and change
      index.test.js to say 'should have h1 that says Users' and change the expect to say users.</p>
    <img src="index.test.js"/>
    <p>And the moment we hit save we can see our tests passing again. And yes this is a silly test but I want to leave
      it here so that we have a working example of how to interact with JSDOM. So now if we go over to the browser and
      load our application we can see that our data is coming back and if we inspect and go to the network tab. I'll
      just reload, so now we can see that request going through</p>
    <img src="reqgoingthrough"/>
    <p>This is the request to users on our api. We get a 200 ok and the response is the JSON that we're returning from
      express. So now we know that our call is going through using Fetch as we expected.</p>
    <p>Now you might be wondering about sending a polyfill to everyone. Let's discuss that next. </p>
    <h2>Selective Polyfilling</h2>
    <p>Now you might be wondering if Fetch is already supported natively in some browsers why are we sending our
      polyfill down to all browsers. Well, in short, we did so because it was easy and also quite common. The idea
      is that you can remove the polyfill altogether later when all the browsers that you care about have added support
      for fetch. But if you want to send a polyfill only to browsers that need it there's a handy service called
      Polyfill.io which does just that. It offers a wide array of polyfills. Here's an example of using Polyfill.io
      to polyfill only the Fetch feature. So if we put this at the top of index.html Polyfill.io will read the User
      Agent and use that information to determine if the browser requires a polyfill for the feature or features listed.
      Since I'm using Chrome it'll send back an empty response since my browser doesn't need it. Pretty slick. </p>
    <p>Now what if we need a wide variety of data to build our app and the services we need to call dont exist yet.
      That's just one of the reasons you might want a robust mock API so in the next clip let's discuss approaches for
      mocking apis and why it's so useful.</p>
    <h2>Why Mock HTTP?</h2>
    <p>We've now setup our development environment to handle making HTTP requests but it's often helpfull to mock HTTP.
      Why? Well maybe you want to unit test your code so that your tests run quickly and reliably or maybe the existing
      web services in your QA environment are slow or expensive to call. Mocking HTTP means that you can receive
      consistently instantaneous responses. Or maybe the existing server is unreliable. With a mock api you can keep
      working even when the services are down. Maybe you haven't event created any web services yet. If you haven't
      decided how to design your web services. Mocking allows you to prototype different potential response shapes and
      see how they work with your app. Perhaps a separate team is creating the services for your app. By mocking the
      service calls you can start coding immediately and switch to hitting real web services when they're ready. You
      just
      need to agree on the api's proposed design and mock it accordingly. Finally, maybe you need to work on a plane, on
      the road or in other places where connectivity is poor. Mocking allows you to continue working while your'e
      offline. </p>
    <h2>How to Mock HTTP</h2>
    <p>So does that sell you on the benefits of mocking HTTP? Assuming so, here's a few ways to get it done. If you're
      writing unit tests then Nock is a handy way to mock HTTP calls in your tests. You tell Nock the specific url
      that you want to Mock and what it should return. Nock will hijack any HTTP request to the Url that you
      specified and return what you specified instead. This way your tests become deterministic and no longer make
      actual HTTP calls. But if you're wanting to do day to day development against a Mock api you'll want
      something more. If you've already centralized all your api calls within your application then you can use this
      centralization to your advantage by pointing to a static file of JSON encoded data rather than making the
      actual HTTP call. Or you can, of course, create a web server that mocks out a real api. Thankfully there are
      libraries that make this easy, such as, api-mock and JSON server. With JSON server you create a fake database
      using static JSON. Then when you start up your JSON server it creates a webservice that works with your static
      JSON behind the scenes. So when you delete, add or edit records it actually updates the file. So this provides
      a full simulation of a real working api but against local mock data that's just sitting in a static file. This is
      really useful because the app feels fully responsive and you don't have to go through the work of standing up
      a local database and webserver by hand. What if you want to use dynamic data instead of the same hard coded
      data? Well that's way JSON Schema faker comes in handy. JSON Schema faker generates fake data for you. You
      specify the data type you like such as a string, number, or boolean and it will generate random data which you can
      write to a file. And you can specify various settings that determine how it generates the data such as ranges
      for numbers or useful generators that create realistic names and emails. Finally you can just go all out and
      wire up a fake api yourself using your development web server of choice such as Browsersync or express. Of course
      this is the most work but it also provides you with the most power. So, how do you decide between these options?
    </p>
    <img src="mockingoptions"/>
    <p>Well, in short, as you move to the right, you have to spend more time up front configuring but in return you
      enjoy a more realistic experience and more power to customize. See, with static JSON your app will load the same
      data every time. And if you try to manipulate that data in any way it wont be reflected upon reload. Now JSON
      Server actually saves the changes that you make to the data so it increases the realism of your mock api. Now,
      you can make your mock api more dynamic by using JSON Schema Faker. JSON Schema Faker can create different
      fake data every time you start the app. This can be really helpful for catching edge cases in your design
      such as pagination, overflow, sorting and formatting. And finally, setting up a full mock api from scratch
      using something like express and a real-time database filled with mock data of course gives you all the power
      to customize as desired. But if you don't already have a service layer and a database your'e on the hook to do
      all that hard work up front before you can enjoy a rapid front end development experience. In summary if there's
      already a solid service layer available then I suggest putting it to use. But if a separate team is building the
      service layer and you haven't built it yet I suggest trying a mock api so that you can move quickly without
      being reliant on a real api backend. The lessons you learn with your mock api can often help guide your api
      design.</p>
    <p>So now that we've talked about different decisions in the next clip let's talk about our plan for mocking
      HTTP in our starter kit.</p>
    <h2>Our Plan for Mocking</h2>
    <p>For this course, let's use a three step process to create a mock api. We'll put a few handy open source projects
      to use.</p>
    <ol>
      <li>Declare our schema using JSON Schema Faker. This will allow us to declare exactly what our fake api
        should look like. We'll declare the objects and properties that it will expose including the data types.
      </li>
      <li>Generate Random Data: JSON Schema Faker supports generating random data using a few open source libraries:
        <ul>
          <li>faker.js</li>
          <li>chance.js</li>
          <li>randexp.js</li>
        </ul>
        Faker and chance are very similar. Both of these libraries offer a wide variety of functions for generating
        random data including realistic names, address, phone numbers, emails and much more. randexp.js focuses on
        creating random data based on regular expressions. Now JSON Schema Faker allows us to use faker, chance and
        randexp within our Schema definitions so we'll declare exactly how each property in our mock api should be
        generated. This will ultimately produce a big chunk of JSON and the nice thing is that big chunk of JSON will
        contain different data every time that we run JSON Schema Faker and that's where I final piece comes in.
      </li>
      <li>JSON Server creates a realistic api using a static JSON file behind the scenes so we'll point JSON
        Server at the mock dataset that we dynamically generate. Now the beauty of JSON Server is it actually supports
        creates, reads, updates and deletes. So it saves changes to the JSON file that's been created by JSON schema
        faker. This way the api feels just like a real api but without having to make an actual over the net HTTP call
        or needing to stand up a real database. This means that, to get started on development, we just need to agree
        on the calls that we want to make and the data shape that those calls should return. Then the UI team can move
        ahead without having to wait on a service team to create those associated services. Everyone can code to an
        interface and get back together later. Now that we've talked about the high level plan let's explore the
        technologies that we're going to use in a little more detail in the next clip.
      </li>
    </ol>
    <h2>Mocking Libraries</h2>
    <p>JSON Schema is a standard for describing a JSON data format. Of course, since JavaScript is the Wild West
      this is just one of many so-called standards for describing JSON structures. There's also JSON content rules,
      JSON-LD, RAML and api related technologies like GraphQL, Falcor and Odata that specify their own standards for
      JSON structures but in this course we going to use the JSON schema standard that's being outlined here at
      json-schema.org.</p>
    <img src="json-schema"/>
    <p>This is the standard that we'll be following to declare the shape of our mock data. And here's why. JSON
      Schema Faker is a handy tool that uses the JSON Schema standard. It enhances it by using some open source
      libraries for generating mock data. As I mentioned the three libraries that we'll be using are:</p>
    <ul>
      <li>faker.js</li>
      <li>chance.js</li>
      <li>randexp.js</li>
    </ul>
    <p>All three of these libraries come bundled with JSON Schema Faker. For more intformation on Faker.js check
      out the Github repo and also the Github.io site for more documentation:</p>
    <a href="github.com/Marak/faker.js/wiki">gihub</a>
    <a href="marak.github.io/faker.js/index.html">github.io</a>
    <p>But I also recommend using Fakers interactive example. This is a great way to see all of the data that Faker
      can generate and each time that you click on the labels on this form you'll notice that it generates different
      data. </p>
    <p>Chance.js also has a nice dedicated web site with a long list of detailed examples as well:</p>
    <a href="http://chancejs.com/">Chance.js</a>
    <p>There's a lot of crossover between Faker and Chance.js but again they both come bundled with JSON Schema Faker
      so it's your choice which one you want to use for a given call. But that said, it's worth carefully reading the
      JSON Schema Faker docs on GitHub. They provide a long list of examples for how to call Faker and Chance within
      your schema definition.</p>
    <img src="faker.js email"/>
    <p>As you can see here their using Faker to generate an email address. The trickiest part is understanding how
      to convert the document and function calls for Chance and Faker into the JSON that you see here. Thankfully there
      are many examples within the JSON Schema Faker docs. Just be sure to carefully read the docs on faking values if
      you get tripped up on how to call faker, chance or regexp from within your JSON schema definition. This is the
      bit that confused me the most, but the examples helped me work it out. Also be sure to check out the JSON
      Schema Faker REPL online. This way you can easily try different schemas and instantly see what JSON they
      produce. I found this to be a great way to rapidly learn how to structure my schema. And here's the JSON server
      repository. As you can see from the number of stars, it's hugely popular. The sales pitch is simple. Get a full
      fake REST API with zero coding in less than 30 seconds (seriously). Now the biggest caveat of using this tool
      is that it has strong opinions on what a fake rest api should look like. If your'e a fan of hypermedia this
      wont do it. And if you have an existing api that has dramatically different assumptions than this makes then
      this wont be very helpful. But if you haven't built you service yet or if your service follows the same
      popular conventions as this library then json-server can help you stand up a mock api shockingly quickly. Your
      about to see how fast all these tools can be glued together into something seriously useful. I know there are
      a lot of moving pieces here but I think you'll be surprised how easily this all composes together.</p>
    <h2>Demo: Creating a Mock API Data Schema</h2>
    <p>Alright, it's time to mock some HTTP. Here's the plan. Let's use JSON Schema Faker to declare our Fake data
      schema. It comes bundled with three handy libraries that we'll use to generate our random data: faker, chance
      and regexp. We'll use JSON Server to serve it up and simulate a real api. Let's dive in. As we just discussed in
      the slides we're going to use a combination of useful open source products to create a mock api. To begin let's
      define a schema that describes what our mock data should look like. Let's create a file called mock data schema
      within our buildScripts folder. And I'll just paste in the schema then talk through the structure. If you
      don't want to type this you can grab the snippet from <a href="bit.ly/ps-mock-data-schema">this</a> url.</p>
    <p>Now, as you can see, I'm exporting a chunk of JSON and this JSON describes the shape of our mock data. I
      begin by declaring at the top level that our data structure is an object and that object has a set of
      properties. First property is users and that users property has a type of array. I'm specifying that I want
      that array to contain between 3 and 5 items and then below I define the shape of the items that should sit
      inside the users array. I'm saying that inside the users array I should find an object and then again I define
      the properties for that object. As you can see, there are 4 properties I am defining:</p>
    <ul>
      <li>id</li>
      <li>firstName</li>
      <li>lastName</li>
      <li>email</li>
    </ul>
    <p>The id should be a number, I am saying it should be unique because I am trying to mimic a unique key in a
      database and I want that minimum value to be 1, I don't want any negative numbers. Then I have a firstName with
      a type of string, and this is where things get interesting, I start using the faker library and I'm asking for
      a fake firstName, I do the same thing with lastName and then I also do the same thing on email to say that I
      would like a fake email address returned. Finally, down here at the bottom, I say that all 4 properties that we
      have defined above are required. That means that they will always be populated. If I forget and I leave one
      of these out of the array then it will occasionally leave one of these out so that we can simulate an api that
      doesn't always send a property if it's not populated. And I also specify that my one top level property which
      is users is also required. So in this case our schema will always return all of our properties since I've
      required them all. Pay close attention to these required properties. This really confused me at first when I
      was wondering why some of my properties were occasionally not showing up. And that's it with only 34 lines of
      JSON we've declared detailed rules about how our mock data should be generated. And now that we've declared
      how it should look let's use it to generate mock data in the next clip. </p>
    <h2>Demo: Generating Mock Data</h2>
    <p>We just wrote the schema that declares the shape of our mock data. Now we can use JSON Schema Faker to
      generate some mocke data using this schema. To do that let's create another file in buildScripts and we'll
      call it generateMockData.js. This file will use JSON Schema Faker to generate a mock data set and write it
      to a file. And as you can see I'm pulling in json-schema-faker, I'm referencing the mock data schema that
      we just created and then I'm using fs which comes with node and chalk to be able to colour our output.</p>
    <p>I begin by calling JSON.stringify on the results of JSON Schema Faker. As you can see I pass the Schema
      that we just defined to JSCON Schema Faker. So effectively JSON Schema Faker is going to look at that Schema,
      generate a lot of randomized data based on our schema and then I'm going to convert that into a JSON string
      using JSON.stringify. So now we have a string of JSON stored on line 14. Then I'm going to use nodes built in
      fs to be able to write our database file, which I'm going to place in the api folder and we'll call it db.json.
      If any error occurs then I'll log it to the console in red using chalk and if it succeeds then I'll output
      "Mock data generated." in green.</p>
    <p>And now that this is setup let's write an npm script that makes all of this easy to call. So we can jump
      over to package.json and inside let's create a new script called "generate-mock-data". I use babel-node
      to call my generateMockData file that we just created, and of course I need to use babel-node because I wrote
      it in ES6 just to make sure that node can parse it.</p>
    <p>When we run this script it should write a random dataset that matches the schema we defined to our api folder.
      So let's save our changes and see if this works.</p>
    <pre><code class="hljs">npm run generate-mock-data</code></pre>
    <p>We got our green message so that's a good sign. And now we can see that db.json was written to the api folder
      and if we open it up we can see that random data was generated that honours the shape that we just defined.
      We can see that we're getting randomized id's and realistic first, lastNames and email addresses. We can also
      see that there was an array of users generated as we requested. Great so we now have a simple repeatable way
      of generating random data that suits our specific needs. In the next clip let's put this to use on a mock api.</p>
    <h2>Demo: Serving Mock Data via JSON Server</h2>
    <p>Now that we have the mock data we need let's startup JSON Server and tell it to use our mock data. Now the
      great thing about JSON server is it will parse our JSON file and make a mock api for each top level object that it
      finds. So let's create a new npm script to start our mock api server:</p>
    <pre><code class="json">"start-mockapi": "json-server --watch src/api/db.json --port 3001"</code> </pre>
    <p>As you can see, I'm telling it to use the db.json file that we generated and to serve the api on port 3001.
      Again, pick a different port if 3001 isn't available on your machine but I'm deliberately choosing a different
      port than port 3000 which we're using to host our app. So let's open the command line and try it out:</p>
    <pre><code class="hljs">npm run start-mockapi</code> </pre>
    <p>When we do we can see the list of resources that JSON Server is exposing, in this case, it found our top level
      object - users, but if we'd added more top level objects it would create an endpoint for each one. Slick. Now
      let's take this url and go back to the browser, open up a new tab and paste it in and there we go. Awesome we
      can see an array of users is getting returned as expected. So this is the mock data that's sitting in db.json
      but now it's getting served up over HTTP on a mock api.</p>
    <p>Now, I prefer for my mock data to change every time that I open the app. This way we're constantly viewing
      different potential edge cases in the system. Randomized data helps simulate the real world and it catches
      issues in development such as:</p>
    <ol>
      <li>Emtpy lists</li>
      <li>Long lists</li>
      <li>Long values</li>
      <li>Testing</li>
      <li>Filtering</li>
      <li>Sorting</li>
    </ol>
    <p>So let's generate new mock data every time that we start the app. To do that let's go back to package.json
      and we'll create a script that should run before we start the mockapi, so I'll place it right before
      start-mockapi, we'll call it prestart-mockapi, and remember by convention because this starts with the word
      pre but otherwise has a matching name it will run before start-mockapi. And what we're telling it to do is
      generate mock data before it runs start-mockapi. Finally, let's update the start script to start the mock api
      each time we start the app.</p>
    <p>Simple enough. So now every time that I start the app it will generate new mock data and startup the mock api
      that serves the data. And the interesting thing about JSON Server is if we manipulate the data by making calls
      to edit or delete records it will actually manipulate the data files behind this scenes. This means you can
      even use this for integration tests or reload the page and see your changes reflected. It does a great job of
      mimicking an api with an actual database behind the scenes. Of course to see this in action we need to update the
      application to hit the new mock api instead of that express api call that we created earlier in this module. So
      let's assume that the express server that we setup here:</p>
    <p>And that the mock api that we setup is what we want to use during development. So what we need is for the
      application to intelligently point to the proper base url in each environment. To do that let's create a file
      called baseUrl.js in the api folder.</p>
    <img src="baseUrl"
    <p>This file will look at the hostname to determine if the application is running in development. If it is it
      will point at our mock api which is hosted on port 3001 and if it's in production it will point at that
      production api that we setup that is served from express. Great so let's put this new file to use in our
      userApi file. I'm going to add an import for getBaseUrl here at the top and then I will store that in a
      constant right here:</p>
    <br/>
    <p>And of course we need to use this information in the api call below. So I will say baseUrl + url. This way
      it will change based on the environment. Assuming this worked we should be able to start our app again and
      see that it's pointed at our mock api because we're in development. Now that it's up if we come over to the
      browser we can see that our user data is displaying. And a quick note, since we're starting express and the
      mock api at the same time the app may fail on the first load if it tries to call the mock api before it is up.
      If so, just hit F5 to refresh:</p>
    <img src="diffdata"/>
    <p>We can see that it is different data than we were seeing before so we know that we're hitting our mock api.
      We could also confirm this by coming over here to db.json and seeing that the first record is Kole Kessler and
      that is what we're seeing right here. So we know we're getting the data from db.json served up into our app. We
      can also see this if we reload that we're making a call to port 3001. So our application is hosted on port 3000
      and our mock api is on 3001 and it's returning that mock data that we just generated. Of course you'll have
      different mock data than me because every time we run the application now it's going to realistic looking mock
      data. Now do you notice that delete link? Well it doesn't work because we haven't wired it up yet but this is
      where things get really interesting. JSON Server supports manipulating data as well so if we submit a request
      to add or delete records it will write to the db.json file so our changes are persisted on reload. So the
      changes will remain in db.json until we restart the app. So let's wire up these delete links in the next clip.</p>
    <h2>Demo: Manipulating Data via JSON Server</h2>
    <p>To prove that we can save changes to the mock data I could just generate an HTTP request to the mock api that
      tries to delete some data. I could install a handy tool like Advanced Rest Client which is a Chrome app but let's
      go ahead and enhance the interface that you see here to support deleting a user when I click the delete link.
      First, let's go back to the code and create the necessary api call. So we'll open userapi and we'll export
      a new publc function: delete user. As you can see it looks at users and then passes the id that it receives. And
      you can see that it is delegating to a separate function called del which I haven't created yet. Let's go
      ahead and do that. And we'll place it down here below the private get function. Now this might seem rather
      redundant but this is the same pattern that we followed when we were setting up get abn getusers. This
      private del function gives us a centralized spot to handle all our delete calls. So if we add other deletion
      functions related to users, each public function call is nice and short. And I had to call this del because
      delete is a keyword in JavaScript. Now that we've setup the functions that we need within our api let's shift our
      focus over to the ui. We'll go to index.js and add some code to make these delete links work.</p>
    <p>After we make our call to getUsers we're currently populating the table but let's do a little more work
      in here. We'll go ahead and paste this in:</p>
    <pre>
<code class="javascript">
const deleteLinks = global.getElementById......
</code></pre>
    <p>What we now want to do is get a reference to all of the delete links on the page and to do that we're going
      to look for anything with a class name of deleteUser. As you can see, all the delete links have a class of
      deleteUser. So now I will have an array like structure that I can iterate through. So I'll use
      <code classs="hljs">Array.from</code> to be able to iterate through the list of delete links and then attach
      a click handler to each one. I'll prevent defaults so the click doesn't actually produce any change to the url.
      I'll call deleteUser and then I will remove the row that we just clicked from the dom. And again, this would all
      be potentially cleaner in React, Angular and other popular frameworks but I just want to use plain vanilla
      JavaScript here to avoid adding confusion. And one final touch since I'm calling delete user right here, we need
      to add it as an import from our user api. And with that the UI should now support us deleting a user from our
      mocked database. So let's give it a shot. DONT MISS THE CIRLY BRACE</p>
    <p>And now when I click delete we can see that it works. We can watch the network down here and see the call go
      through to delete the different users. If I click on one of these and look at the headers we can see we get a
      204 no content, we can see that the delete is going through as our request method as expected. And here's the
      cool part. If I hit refresh now only one record is here because when I hit delete on those two it really did
      write to db.json. If we come back over here we can see now that our db.json only has one record when before it
      had three. We can also see that JSON Server is continuing to log all the different calls that are being made
      to our mock api. This is really handy when your debugging calls along the way. The great thing is this data
      will persist until we restart the app and new random data is generated. And a quick note, you may have to hit
      ctrl c multiple times to kill the running process since now we running multiple processes on the same command
      line.</p>
    <p>You'll notice the JSON Server throws an error when you kill it this way but there's no impact so you can
      ignore it. If you prefer you can kill the terminal and open a new instance. So, Wow, we just covered alot of
      moving parts but the result sure is handy. Now let's close out this module with a short summary.</p>
    <h2>Summary</h2>
    <p>In this short module we began by reviewing how to choose a HTTP library. We saw that HTTP is the low-level
      option in node but you'll probably want to use request with node due to it's streamlined api. In the browser
      you can choose the old standards like:</p>
    <ul>
      <li>XMLHttpRequest</li>
      <li>jQuery</li>
      <li>Fetch</li>
    </ul>
    <p>But Fetch is probably what you should reach for since it's the new standard that streamlines the clunkiness
      of old XMLHttpRequest. Just remember to pull in the Fetch polyfill so it will work properly cross browser. Or
      if your looking for a full featured library especially one that works in node or the browser Isomorphic Fetch is
      the most future friendly approach since it utilizes the browser's built-in Fetch support if available. However
      you can also consider using the XHR library on npm, SuperAgent or Axios. All of these are excellent options
      regardless of whether you need to run on both node and the browser. And we closed out this module by exploring
      HTTP call mocking. If your'e testing the way to get that done is Nock and if your'e need to mock an api for
      development the simplest way to get that done is likely just some hard coded JSON. If you have a small app that's
      perhaps all that you'll need. But if you want to simulate interactivity then a custom web server approach
      involving JSON Schema Faker, and JSON Server likely makes more sense. We saw that JSON schema faker is quite
      powerful and includes enough built-in intelligence to create realistic fake data for a wide variety of
      scenarios and of course if you want to go fully custom you can configure your development web server of choice
      to simulate a real api. This is certainly the most work but also offers the most complete flexibility. Now that
      we have HTTP requests taken care of we have a powerful foundation for building real applications. So in the next
      module let's put all this to use. We'll discuss key principles for project structure, we'll learn why demo apps
      are so important, and we'll build a quick demo app that helps convey best practices. And in the final module we'll
      wrap up the course by creating an automated production build.</p>
    <h1>Project Structure</h1>
    <h2>Intro</h2>
    <p>It's been a long road but we're well on our way to enjoying a seriously robust, comprehensive and luxurious
      JavaScript development experience. But we still have two major pieces left to consider in these final two modules.
      In this module let's discuss how to put all this to use by discussing project structure. In this short module I
      want to begin by explaining why I believe your teams starter kit should include a demo application. Then
      we'll move on to discuss three specific project structure tips to keep in mind as your building not just demo
      app but any future JavaScript app. Alright let's begin with my sales pitch on why your team needs a demo app.</p>
    <h2>Why a Demo App?</h2>
    <p>I believe a demo app should be considered a critical piece of your teams starter kit. Why? Because many people
      work best by example so a working example really helps. Let's just review some decisions which are clearly
      conveyed via an example app. A demo app conveys expectations around suggest directory structure and file
      organization. It helps encourage consistency by codifying patterns for how developers should work with the
      libraries and frameworks that you've selected. It shows example tests that are passing so developers have a point
      of reference for various testing scenarios, naming conventions, file placement and mocking strategies. It
      provides a realistic example of a mocking api working in your domain. This gives developers a big head start
      since referencing examples is often quicker than pulling up docs for disparate packages. It provides a
      working automated deployment so there's already a clear recipe for use for future apps. It gives you a single
      place to codify your decisions the demo app should reflect your coding standards. It's a place to update as you
      learn new techniques and patterns that your want to share with the team. Finally and perhaps most importantly it
      offers an interactive example of the starter kit working in a realistic scenario. This helps new team members
      understand what life is like working in the selected stack. Ok so assuming your on board with creating a demo
      app for your teams starter kit let's consider a few tips for structuring JavaScript projects in the next
      clips.</p>
    <h2>Tip 1: JS Belongs in a .js File</h2>
    <p>Before we build the demo app let's take a moment to consider project structure. I have three important tips I
      want to share to help you avoid some of the pitfalls that I often see in JavaScript code bases. My first tip is
      simple</p>
    <ol>
      <li>JavaScript belongs in a .js file.</li>
    </ol>
    <p>But when doing web development some of you are tempted to simply slap JavaScript code in a script tag. Of
      course you'd never do that right? It's understandable. It's so easy just to slap a script tag onto the HTML
      page and start coding. But let's pause for a moment and consider why this should be avoided. There's a long
      list of downsides. When I do something like this:</p>
    <pre>
<code>&lt;script&gt;
  // slap code here
&lt;/script&gt;
</code>
    </pre>
    <p>The answer is I'm losing a ton of power to do my job better. I'm losing the ability to leverage all of the
      goodness that I just setup. See if I do this how do I write automated tests for this, how do I lint this code,
      how do I reuse this. Oh and if you just mumbled copy and paste - wrong answer! Now what if I want to use ES6,
      TypeScript or some alternative language that transpiles to JavaScript. What if I want to be explicit about
      my codes dependencies by using ES6's import keyword. The answer to all of these questions is - you can't. By
      putting your code inline within HTML your'e losing all of this power. And these are just a few of the
      fundamental issues with this approach. So my advice comes down to a simple maxim. JavaScript belongs in a .js
      file. Writing JavaScript inline within an HTML file should be avoided. And please for the sake of everyone's
      sanitydon't use some server side language to generate your JavaScript. This example from StackOverflow makes me
      want to cry tears of mourning for the maintenance programmer:</p>
    <img src="stackoverflow"/>
    <p>Let's just think about the developer experience here. All of the code is the same color, there is no auto
      completion support. Your editor can't highlight any typos so you won't find out about your mistakes until
      runtime. And you don't get to enjoy any of the goodness that we've worked so hard to setup in this course. Yet
      I see this pattern all too often when server side developers who are new to JavaScript need to perform some
      custom logic based on data that's in the database. So let me clarify a better way to get that done. If you need
      your code to respond differently for different users, instead inject JSON from the server into your application. I
      call this the object configuration pattern. Now I'm far from the only person suggesting this pattern. As you can
      see StackOverflow is using it here:</p>
    <img src="soinject"/>
    <p>Since StackOverflow is using C# behind the scenes you can imagine that there's a C# class that contains all of
      this data, likely pulled from their database. And they use a JSON serializer in C# likely JSON .net to generate
      this JSON. Their static JavaScript code looks at this JSON and this is what customizes my StackOverflow
      experience.
      Bottom line: avoid dynamically generating JavaScript code. Instead, dynamically generate some data that your
      JavaScript code can use. And this really shouldn't come as any surprise. We don't generate custom C#, Java, Ruby
      or
      Python and so on just to dynamically provide custom behaviour for each user. Instead we pull data from the
      database
      and we use that data to determine what logic should run. JavaScript is no different. The solution is to use data
      from the server to fork your codes logic as necessary.</p>
    <h2>Tip 2: Consider Organizing by Feature</h2>
    <p>Time for my second tip. On larger more complex projects consider organizing by feature instead of by file type.
      There are two popular ways to organize your code.</p>
    <p>By file type</p>
    <pre>
<code class="hljs">
/components
/data
/models
/views
</code>
    </pre>
    <p>or by feature:</p>
    <pre>
<code class="hljs">
/authors
/courses
</code>
    </pre>
    <p>When you organize by file type all files that serve the same purpose are placed together. This is a popular
      approach when working with MVC frameworks which commonly expect you to use Model, View and Controller folders
      to organize your application. However the downside of this approach is you end up having to bounce around the
      file system to open and work with related files. So I recommend organizing by feature on larger projects. The
      larger the project the more organizing by feature pays off because you can go directly to the feature that
      you are working with and all the related files are sitting inside.</p>
    <h2>Tip 3: Extract Logic to POJOs</h2>
    <p>My third tip is to strive to extract as much logic as possible into plain old JavaScript. Some would call this
      POJO's. When I say POJO I mean Plain Old JavaScript Objects. Java and C# developers would recognize this term
      since Java developers also use the term POJO to describe Java classes that have plain logic inside and no
      framework specific concerns. .Net developers use the term POCO to describe the same thing, though in that case it
      stands for Plain Old CLR Object. The point is these files should contain plain logic that isn't tied to any
      framework. When structuring your application strive to place as much logic as possible in plain JavaScript. For
      instance if your working in React much of your logic should exist outside of React components. This makes your
      logic easy to test, easy to reuse and helps minimize your ties to the framework you've selected. This minimizes
      the impact of switching to a different framework down the road. Because, hey, we're in JavaScript, let's face the
      facts - we'll probably be doing that.</p>
    <p>To see an example of this philosophy check out the demo app in the React slingshot starter kit on GitHub. You'll
      see that key logic such as date formatting and core calculations are handled in plain JavaScript in a folder
      called utils. Although this project uses React these are plain JavaScript functions that are'nt tied to React in
      anyway.</p>
    <h2>Summary</h2>
    <p>Let's wrap up. It's really helpful to include a working example app in your starter kit. This gives everyone
      clarity on recommended approaches for directory structure, file naming, framework usage, testing, api calls,
      deployment and more. It provides an interactive example of what it's like to work on your team. Then we moved
      on to a few ground rules for structuring your project. Put y.our JavaScript in a JavaScript file. This
      principle is foundational. If you place JavaScript inline within HTML you lose all the benefits we've worked so
      hard to setup throughout the course. </p>
    <p>Consider organizing your demo app by feature instead of file type. Especially if your team typically builds
      large and complex JavaScript applications.</p>
    <p>And third, extract your logic into Plain Old Javascript Objects. Avoid embedding to much logic in framework
      specific files. Extract your logic to pure functions that are easily testable. I recommend building your demo
      app using your preferred JavaScript frameworks and libraries and be sure to select a domain that's related to
      your business. Keep these tips in mind as you structure your demo app. Alright, there's one very important piece
      left. We need to prepare our app for production. So in the next module let's create an automated production build
      including minification, bundling, bundle splitting and more.</p>
    <h1>Production Build</h1>
    <h2>Intro</h2>
    <p>Of course our application isn't very useful until we actually prepare it for production. So in this module
      let's create an automated production build. We'll cover a variety of considerations including minificationto
      to speed page loads with Sourcemaps generated to support debugging in production - hey, let's be honest you and
      I both know this happens. We'll setup dynamic HTML handling for production specific concerns and Cache busting to
      make sure that users receive the latest version of our code upon deployment. We'll setup bundle splitting so that
      users don't have to download the entire application when just part of it changes. And finally we'll setup error
      logging so that we know when bugs sneak their way into production. Now this sounds like a lot of work but as
      you'll see this moves fast. Alright let's dig in.</p>
    <h2>Minification and Sourcemaps</h2>
    <p>Let's begin by discussing minification. Minification is about speeding page loads and saving bandwidth. So how
      does minification work? Well a JavaScript minifier uses a number of tricks:</p>
    <ul>
      <li>It will shorten variable and function names.</li>
      <li>Remove comments</li>
      <li>Remove whitespace, newlines, and more.</li>
    </ul>
    <p>The code still functions the same but the resulting file size is much smaller which helps speed page loads.
      Minification basically removes all the things that only humans care about and leaves all the things that computers
      need. And some of the newer bundlers like rollup and webpack 2 go even further by elimanating unused code by a
      process that's commonly called tree shaking. We're using webpack 1 for this course but this feature will soon help
      reduce the size of bundles by excluding any unused code from our final bundles. And through the beauty of
      sourcemaps we can still debug our minified code. As you'll see in a moment in the demo we'll continue to see
      our original source code in the browser as we debug. For more info on sourcemaps refer to the bundling module
      earlier in this course.</p>
    <h2>Demo: Production Webpack Configuration with Minification</h2>
    <p>Now let's put webpack to work to bundle and minify our app code for production. To begin configuring our app
      code for production let's make a copy of our development webpack config and we'll call it web.config.prod.js and
      now let's tweak some settings for production. First we'll change the dev tool setting. Remember that this setting
      specifies how sourcemaps should be generated. We explored sourcemaps earlier in the bundling module. Let's
      change the devtool setting to sourcemap since that's whats recommended for production. It's a little bit slower
      to build but it provides the highest quality source map experience. This will assure that we can still see our
      original source code in the browser even though it's been minified, transpiled and bundled. That's the beauty of
      source maps. And we're going to write our production build to a folder called dist so let's change the output
      path. When building the app for production we'll write physical files to a folder called dist. This is a popular
      convention and it stands for distribution. </p>
    <p>Next, let's setup minification. We want to minify our code for production so let's add our first plugin to
      the array of plugins. We're going to use a plugin called UglifyJs and I like to put a comment above each
      of my plugin references. So you can see webpack.optimize.UglifyJsPlugin. And now that we're calling some specific
      webpack features we need to add the import at the top.</p>
    <p>And before we minify let's use another handy webpack plugin that will eliminate any duplicate packages when
      generating the bundle. This bundle is called the DedupePlugin. So this will look through all the files that
      we're bundling and make sure that no duplicates are bundled.</p>
    <p>Now we'll enhance the webpack config with additional features throughout this module but this is a good start.
      Let's now shift our focus to writing a script that will run our production webpack config build. So we'll go
      over to buildScripts and create a new file called build.js. And here's all it takes to run our webpack build for
      production:</p>
    <pre>
<code class="javascript">
/*eslint-disable no-console */
import webpack from 'webpack';

</code>
    </pre>
    <p>We're importing webpack, our production config that we just defined and chalk so that we can color our output.
      And then we're calling webpack and passing it that webpack config. We're handling any errors that might occur
      otherwise we return 0 which signifies success. So this is pretty simple but in the real world you'll likely
      want to add a little more to this. So let's enhance this script a little bit.</p>
    <p>First, above the call to webpack, declare that we are running node in production mode: </p>
    <pre><code class="javascript">process.env.NODE_ENV = 'production';</code></pre>
    <p>Although it's not neccessary for our setup I'm adding this line here because this is important if you create a
      dev specific configuration for babel in your .babelrc file. See Babel, and potentially other libraries you may
      use,
      look for this environment variable to determine how they are built. And before we get started running the
      production build I like to output to the console just so we can see the production build has started:</p>
    <pre>
<code class="javascript">
console.log(chalk.blue('Generating minified bundle for production. This will take a moment..'));
</code></pre>
    <p>Since we're doing minification, as you'll see, the production does take quite a few seconds to run so it's
      nice to get some notification that it is doing the job.</p>
    <p>I like to display some stats to the command line:</p>
    <pre>
<code class="javascript">
const jsonStats = stats.toJson();
</code>
    </pre>
    <p>Now this looks like a lot of code and it isn't required but this assures that warnings, errors and stats
      are displayed to the console and at the bottom we display a success message if everything worked. </p>
    <p>So this is for displaying any errors that occur:</p>
    <pre>
<code class="javascript">
if (jsonStats.hasErrors) {
  return jsonStats.errors.map(error => console.log(chalk.red(error)));
}
</code>
    </pre>
    <p>This is for displaying warnings that occur:</p>
    <pre>
<code class="javascript">
if (jsonStats.hasWarnings) {
  console.log(chalk.yellow('Webpack generated the following warnings: '));
  jsonStats.warnings.map(warning => console.log(chalk.yellow(warning)));
}
</code>
    </pre>
    <p>I display the stats right here:</p>
    <pre><code class="javascript">console.log(`Webpack stats: ${stats}`);</code></pre>
    <p>Finally we just output a message so that we know that our production build has succeeded:</p>
    <pre>
<code class="javascript">
// if we got this far, the build succeeded.
console.log(chalk.green('Your app has been built for production and written to /dist!'));
</code>
    </pre>
    <p>Great, so there's quite a bit of code here but it's really conceptually simple. This just runs our production
      webpack config. I've added some extra goodness here just to improve our experience. Let's save our changes and
      in the next clip let's try this out by setting up an automated production build.</p>
    <h2>Demo: Configure Local/dist Server</h2>
    <p>This isn't required but I like to run the final production version of the app on my local machine just so I
      can make sure everything looks good. This can be really helpful when you need to debug an issue with the
      production build. So let's a create a file called distServer.js in the buildScripts folder. We already have
      a srcServer that serves our src folder, now we'll have a dist server that serves our dist folder. So let's just
      copy the content of our srcServer.js and paste it over into distServer.js because we're only going to make
      a few minor changes here. First let's remove any webpack calls here because we're no longer going to be
      interacting with webpack for our dist server. We're going to be serving up just the static built files. So we'll
      remove the two webpack related imports at the top:</p>
    <pre>
<code class="javascript">
import webpack from 'webpack';
import config from '../webpack.config.dev';
</code>
    </pre>
    <p>We'll also remove the call to the compiler:</p>
    <pre><code class="javascript">const compiler = webpack(config);</code></pre>
    <p>And the calls to configure webpack-dev-middleware:</p>
    <pre>
<code class="javascript">
app.use(require('webpack-dev-middleware')(compiler, {
  noInfo: true,
  publicPath: config.output.publicPath
}));
</code>
    </pre>
    <p>So our file gets simpler. And then the other thing that we need to add is to now add support to express for
      serving static files:</p>
    <pre><code class="javascript">app.use(express.static('dist'));</code></pre>
    <p>So we'll add one line saying app.use and we'll tell it to serve static files in the dist directory. And for
      production we'll be serving index.html from the dist folder rather than the src folder. One final tweak that I
      like to make for our dist server is enabling GZip compression. Your production web server should be using GZip and
      if it's not pause this video and go turn it on! Now anyway I like to enable GZip so I can see the final GZip file
      sizes when I'm serving the app locally. This gives me a clear understanding of the file sizes that will be sent
      over the wire to the user. To do this let's import compression:</p>
    <pre><code class="javascript">import compression from 'compression';</code></pre>
    <p>And then, above our call to express.static we'll add a line in to use compression:</p>
    <pre><code class="javascript">app.use(compression());</code></pre>
    <p>Make sure you add parenthesis so it's invoked. And with those two lines of code we've now enabled GZip
      compression in express. And that's all we need to do to configure our dist server for serving up our production
      app locally. Again this is not for use in a real production scenario. I'm only creating this so I can serve the
      app on my local machine just to confirm that the production build works locally. Then it's a separate decision
      to move all these files up to serve them on some host, perhaps a cloud provider. And, also, yes, I'm leaving in
      the hardcoded data for users. Again, just pretend that this is hitting real data in production. And speaking of
      API calls we also need to decide what API we'd like to use when we're checking out our production build locally.
      So let's work on that next. </p>
    <h2>Demo: Toggle Mock API</h2>
    <p>I prefer to hit the real api when we're testing out the production build locally. So let's open up
      baseUrl.js which is in the api folder:</p>
    <img src="baseurl.js"/>
    <p>Remember this file contains logic that points the api to either our mock api or the real api that's getting
      served by express. To do so it currently checks whether the app is running on localhost. Let's rework this logic
      so it instead looks for the querystring parameter use mockapi. So I'm going to replace the exising function
      with a one liner:</p>
    <pre>
<code class="javascript">
export default function getBaseUrl() {
  return getQueryStringParameterByName('useMockApi') ? 'http://localhost:3001/' : '/';
}
</code>
    </pre>
    <p>And this one liner will say getQueryStringParameterByName and it's going to look for a query string parameter
      of useMockApi and if that exists in the query string then it's going to point to our mock api otherwise it
      will point to the real api that's being hosted by express. Now this getQueryStringParameterByName doesn't exist
      so let's paste that in:</p>
    <pre>
<code class="javascript">
function getQueryStringParameterByName(name, url) {
  if (!url) url = window.location.href;
  name = name.replace(/[\[\]]/g, "\\$&");
  var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$"),
    results = regex.exec(url);
  if (!results) return null;
  if (!results[2] return '';
  return decodeURIComponent(results[2].replace(/\+/g, " "));
}
</code>
    </pre>
    <p>Yes, sadly this big chunk of code is necessary for getting a parameter from the Url. Honestly I just grabbed
      this function off of StackOverflow so cross your fingers let's hope it works. Of course there's other ways to
      do this with libraries but I wanted to use just plain JavaScript here. There are easier ways to get this done for
      instance with jQuery but this function will do the trick. Now with this change we should be able to easily swap
      between the real and the mock api during development by just adding useMockApi to the query string. So let's open
      up the terminal and make sure that this tweak worked:</p>
    <pre><code class="hljs">npm start -s</code></pre>
    <p>And our a pp starts up successfully. As we can see, it's not using the mock data it's using the hardcoded data
      that's coming from our express based service - so this is our production api. We can confirm that by coming over
      to srcServer.js and seeing our hardcoded data here. Again it's not an actual production api but pretend this is
      our production api - we're just hosting it locally here.</p>
    <p>So I should be able to come up here add a querystring saying:</p>
    <pre><code class="hljs">localhost:3000?useMockApi=true</code></pre>
    <p>And when I do, now we end up seeing our mock data instead. So now we have an easy way to switch between our
      mock api and our production api. I find this can be really handy, not just for working with our production build,
      but also for development on a day to day basis. Being able to switch between the production api and the mock api
      gives us flexibility that we need throughout the day.</p>
    <p>And now we're almost ready to run our production build but we can't quite yet because we haven't written the
      necessary npm scripts to automate it. So let's set that up in the next clip.</p>
    <h2>Demo: Production Build npm Scripts</h2>
    <p>In order to automate the production build process let's add some npm scripts to tie all this goodness together.
      I'm going to use four small npm scripts to orchestrate the production build. Let's add them here at the
      bottom of the list:</p>
    <pre>
<code class="json">
"clean-dist": "rimraf ./dist && mkdir dist",
"prebuild": "npm-run-all clean-dist test lint",
"build": "babel-node buildScripts/build.js",
"postbuild": "babel-node buildScripts/distServer.js"
</code>
    </pre>
    <p>Ok let's talk through how this works. The command that will run to build our app for production is
      <code class="hljs">npm run build</code>. This will run the build script that we setup earlier and by convention
      <code class="hljs">prebuild</code> will run before that will clean our dist folder which we can see deletes the
      dist folder and then recreates it. This way any previous files that were there are wiped away before we write to
      that folder. We also want to run our tests and lint our code. Once all that's done the build occurrs and then
      after the build is completed the <code class="hljs">postbuild</code> step will run and that is where we will
      start our dist server. So after our build is completed we will start it up and serve it up locally. And now
      that we have our script setup we're all set to give this a shot. Let's try running our production build and
      see if anything bursts into flames. Let's say:</p>
    <pre><code class="hljs">npm run build -s</code></pre>
    <p>We can see our tests pass, our linting runs, we get our notification and our final confirmation in green
      but BOOM flames indeed. We get a 404 because if we go back over to our dist folder there is no index.html
      inside and if you think about it that makes sense. Webpack is currently configured to write our JavaScript
      files and handle our css but we never setup anything to handle writing out index.html to the dist folder. But the
      good news is we can see that our minified JavaScript is being written as we asked. If we click on the bundle
      that's not very readable at all but that's a good thing in this case because the sourcemap makes it all
      readable in the browser. And although you don't see any css files here remember css is being bundled into
      our JavaScript and thus being generated via JavaScript. If you prefer to generate a separate css file I'll show
      how to do that a little later in the module. Let's take a look at the detailed output on the command line.
      Webpack shows all the files that were bundled including their size. This output is handy because we can see
      precisely what files are being bundled and we can see the size of your final bundle. It's also common to see
      some warnings down here but this isn't code that we wrote, it's code from the libraries that we're using so
      we'll go ahead and ignore these warnings. Now, clearly, we need to decide how to handle our HTML for the
      production build. There are multiple ways to handle this so let's explore this topic next.</p>
    <h2>Dynamic HTML Generation</h2>
    <p>When you bundle your code you obviously need to reference it and if you're doing web development then of
      course you'll end up referencing your bundle in an HTML file. But what if your want to run some slightly
      different HTML in production than development. So why would you want to manipulate HTML for production. Well
      there are many potential reasons. If your bundler is generating a physical file for you wouldn't it be nice
      to automatically reference the bundle in your HTML file. And as you'll see in a moment we'd like to
      generate dynamic bundled names so that we can set four expires headers in production in order to save HTTP
      requests. When bundle names are dynamic we need a way to reference the dynamic bundle name in our HTML. And
      what if we want to inject some scripts or resources only for production. We'll see an example of this in a
      moment when we see error logging. Finally, maybe we just like to save a little bandwidth by minifying our
      HTML. The point is there are a variety of reasons to manipulate HTML for production. Now when you're
      generating a bundle a common question is how to setup your index.html file to reference the bundle. This
      example shows the simplest approach - a hard coded reference to bundle.js:</p>
    <pre>
<code class="html">
&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;You mom&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;script src="bundle.js"&gt;&lt;/script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code>
    </pre>
    <p>This has been working great for us so far but there are other more powerful approaches to consider for
      handling all the issues that we just discussed. I see three specific options for handling your HTML:</p>
    <p>If you have a simple setup you might just want to hard code in a reference to bundle.js as we've done so
      far in the course. This is the simplest approach. However maybe you want to dynamically add some other
      code to the page for production. That's when you want to dynamically generate your HTML file. One obvious way
      to do so is via Node. You can write a Node script that copies your HTML file and uses regular expressions to
      manipulate sections or replace place holders. Or if you choose webpack as your bundler there's a powerful
      approach that I prefer to use called html-webpack-plugin. This plugin simplifies the creation of your applications
      HTML file. It can create the traditional HTML boilerplate for you or you can declare a template which is my
      preferred approach. This plugin is especially useful if your doing cache busting in webpack by generating a
      different filename each time your assets change. We'll set that up in a moment.</p>
    <h2>Demo: Dynamic HTML Generation</h2>
    <p>Alright let's get back to the code and dynamically generate our HTML using the HTML webpack plugin. As you
      just saw in the previous demo our webpack build isn't handling our HTML so we can't actually load the production
      build in the browser yet - let's fix that. The simplest way to handle this is to simply copy index.html from the
      src folder to the dist folder using a command call or node. We could even use node to tweak the HTML in
      various ways for production but doing all that manually will just hold us back because we're going to use some
      other powerful webpack features in the coming clips and the features we're going to use will be a lot easier
      to pull off if we setup webpack to handle our HTML. To do that we're going to use html-webpack-plugin. This
      plugin will dynamically generate our index.html in all environments. So to begin let's update our production
      webpack config to use html-webpack-plugin. First let's add the import to the top for HtmlWebpackPlugin:</p>
    <pre><code class="hljs">import HtmlWebPackPlugin from 'html-webpack-plugin';</code></pre>
    <p>And then let's add a reference to our plugins array - don't forget the comma after defining it. So this
      will create an HTML file that includes a reference to our bundled JavaScript. We're going to declare that
      the index.html that's in our source directory is our template and then I'm setting inject to true which
      tells webpack to inject any necessary script tags for me. This means that we can remove the script tag from
      our index.html. So let's go ahead and do that now. We don't need this anymore because the webpack plugin is
      going to dynamically add in any necessary script tags for us. You'll see why dynamically generating our
      script reference is important in a later clip as we setup bundle splitting and cache busting. And since we're
      dynamically generating our index.html now we need to update our development webpack config to use the
      HtmlWebpackPlugin as well. So let's copy the work that we did in our production config over to our dev config.
      I'll need the reference to the plugin and I'll also need to call the plugin here in the array. Make sure to
      remove the unnecessary trailing comma and save both of our webpack configs. And now that we have
      webpack configured to handle our HTML in both our production and development builds we're ready to run our
      app in production mode. Let's open up the terminal and give it a go:</p>
    <pre><code class="hljs">npm run build -s</code></pre>
    <p>And I like to put the -s just to engage silent mode so that we don't get all the noise to the terminal. And
      see our tests run, linting pass and great our app loads in production mode. If we open the network tab we
      should be able to see the results. Let's go to network and hit relosd and we can see that our minified and
      GZipped bundle is only 4.6k - now that's tiny. Note that the actual bundle size is 12k as we can see over
      here in the output. So GZip compression is helping us out. As we can see here GZip is enabled so our work to
      enable GZip in express is operating as expected. And again we've enabled GZip on express so that the size that
      we see here is an accurate representation of the typical production web server configuration. Since your
      production web server should have GZip enabled. Oh and just remember these delete links wont actually work when
      we're pointed at our real api since we never wired up the real api to handle delete calls. Now let's jump back
      over to our webpack config and make another tweak. We can enhance the configuration of HtmlWebpackPlugin to
      save us some more bandwidth. Since we're dynamically generating our HTML we can configure HtmlWebpackPlugin
      to minify our HTML so let's add some configuration to further tweak it's output:</p>
    <pre>
<code class="javascript">
minify: {
  removeComments: true,
  collapseWhitespace: true,
  removeRedundantAttributes: true,
  useShortDoctype: true,
  removeEmptyAttributes: true,
  removeStyleLinkTypeAttributes: true,
  keepClosingSlash: true,
  minifyJS: true,
  minifyCSS: true,
  minifyURLs: true,
},
</code>
    </pre>
    <p>There's a long list of settings here but I've enabled them all. We're going to:</p>
    <ul>
      <li>Remove Comments</li>
      <li>Collapse Whitespace</li>
      <li>Remove any redundant attributes</li>
      <li>Remove empty attributes</li>
      <li>And more...</li>
    </ul>
    <p>So let's save our changes and see how this works. We should now be able run the build and see our HTML
      minified. If we view page source we can now see that our HTML has been minified and we can also see that the
      reference to the bundle is getting injected by the HtmlWebpackPlugin as we'd expect. Very nice! I love how
      easy and declaritive webpack makes this. So excellent, we have a simple functional automated build but there's
      still room for improvement. What if we were building a large app? In that case we'd likely want to split our
      bundle to help speed page loads and save bandwidth. Let's handle that next. </p>
    <h2>Bundle Splitting</h2>
    <p>As you build larger apps you may find it useful to split your JavaScript into multiple bundles instead of
      creating a single JavaScript file. This is commonly called bundle splitting or code splitting. So why bundle
      splitting? Well what if we build a large app or an app with many pages that are routed client side. If we
      split our bundle into separate files for each client side page we can speed the initial page load by only
      requiring the user to download the JavaScript necessary to render that page. And if we push updates to our
      application source code it would be nice if users were'nt forced to download all vendor libraries weve
      chosen to use in our app. For example, if we build an application with Angular or React or use utility
      libraries like LoDash it would be nice if those items were bundled in a separate file that's cached separately.
      This way when we update the app our users will only have to download the updated app code. So bundle
      splitting saves bandwidth and also assures that our users have a higher performance experience. We shouldn't
      expect our users to download a large bundle on initial page load or to re-download a huge bundle every time a
      small portion of the apps source code changes.</p>
    <h2>Demo: Bundle Splitting</h2>
    <p>Of course, the way that you split bundles will depend on the bundler that you select. But since we're using
      webpack let's configure it to do bundle splitting as part of our production build. So far, we've setup
      webpack to bundle all of our JavaScript into a single bundle. This works great on smaller apps but as your
      application grows it's helpful to split the bundle so the user only downloads the JavaScript they need for the
      current section of the app that they've loaded. Sometimes people split the bundle per page. Imagine we built
      a single page application with three pages. We could split our bundle per page. But another approach to
      consider on smaller apps is to split third party libraries into a separate bundle from our application code.
      The reasoning here is if the application code changes our users won't have to download all of the third
      party libraries that we're using again. Their browsers will continue to use the cached version.</p>
    <p>Now webpack supports defining multiple entry points. We've only defined one here but you can see that an
      array infers that we can add multiple. Now instead of defining an array I'm going to define an object
      because we are going to define keys for each one of the entry points that we define. For this one I'm going
      to call it main because this will be our main javascript bundle:</p>
    <pre>
<code class="javascript">
entry: {
  main: path.resolve(__dirname, 'src/index')
}
</code>
    </pre>
    <p>But we also want to define a secondary entry point, and I'm going to call it vendor because it will
      contain all of our vendor code. We haven't created our vendor file yet so let's go over to our src directory
      and create a new file called vendor.js. And here I'm going to paste in a whopping one line of code. Ok, that
      looks like more than one line but line 17 is the only actual code. I'm disabling eslint's warning for
      no unused variables since there's no usage of this variable here. And then I've just added a comment at the
      top so that people are clear about why we've created this vendor.js file. Of course the only library that we're
      using for our silly demo app is the what working group fetch polyfill but you get the idea. In a real
      application you'd likely reference:</p>
    <ul>
      <li>JQuery</li>
      <li>Angular</li>
      <li>React</li>
      <li>Bootstrap</li>
      <li>And so on...</li>
    </ul>
    <p>Any third party tools that you use could be listed in this file. And everything that we define here will be
      bundled up separately by web pack into a file called vendor.js. Of course if you wanted separate bundles for
      different pages of your app you could declare those using the same pattern. Just add one entry point to your
      production webpack config per page. So let's close vendor and go back to our production webpack config
      because we're not quite done configuring code splitting. To actually perform code splitting we have to referencea
      another builtin webpack optimization that's called commons chunk plugin. So let's add it to the list of
      plugins down here:</p>
    <pre>
<code class="javascript">
plugins: [
  // Use CommonsChunkPlugin to create a separate bundle
  // of vendor libraries so that they're cached separately
  new webpack.optimize.CommonsChunkPlugin({
    name: 'vendor'
  }),
</code>
    </pre>
    <p>Here, we're telling webpack to generate a separate chunk using the code that's referenced in our vendor
      entry point. So note that this name corresponds with the key that we defined in the entry point up here:</p>
    <p>Make sure that these two are in sync or this will just end up generating an empty file. Now here's how
      this works. The Commons Chunk Plugin moves modules that occur in multiple entry chunks to a new chunk and
      to clarify I'm now using the terminology that webpack uses. They call these chunks. I tend to call these
      bundles and talk about splitting bundles. Webpack typically talks about splitting chunks. But chunks and
      bundles are synonymous in my mind. So in this case the Commons Chunk Plugin will intelligently look at the
      items that we imported in vendor.js and it will leave them out of the separate bundle main.js. So since
      we reference the fetch polyfill in vendor.js this plugin will assure that the fetch polyfill is placed in
      our vendor.js file and omitted from main.js. And to clarify without this plugin splitting would'nt actually
      help because our vendor libraries would still be in our main bundle as well causing people to download our
      vendor libraries twice. Anb there's one final detail. Now that we're generating multiple bundles we can
      no longer hard code the name of the file that we're outputting here. Instead we need to declare a place holder
      by using square brackets. And we'll call this placeholder name:</p>
    <pre><code class="javascript">filename: '[name].js'</code> </pre>
    <p>So this tells webpack to use the name that we defined in the entry point. So it will now generate a
      main.js and a vendor.js and since we're using HtmlWebpackPlugin it will automatically write references
      to both these files in our html file. Nice, that's all it takes so let's save webpack config and give it a
      shot. </p>
    <pre><code class="javascript">npm run build -s</code></pre>
    <p>We can see the app starts up just fine. And if we view the output in the terminal and scroll up you now
      see that we generated two separate chunks: main.js and vendor.js. Remember before that main.js was 12k but now
      vendor.js is holding 7.3k so we can see that the size has been split between the two. We've also generated
      mapping files for both main.js and vendor.js. And of course if we come over here and view page source. We
      can see that vendor.js is referenced as well as main.js in our code just as we'd expect. And if we
      inspect here and look at the network tab we should see them both getting requested and we can see that
      there size after being GZipped is 2.9k and 2.6k. So this is a handy way to speed page loads and save
      bandwidth by avoiding requiring your users to download all of your JavaScript when only some of it has
      changed. But there's more that we can do. In the next clip let's explore cache busting. </p>

    <h2>Cache Busting</h2>
    <p>To save bandwidth and avoid unnecessary http requests. You can consider configuring your production web
      server so your JavaScript bundle doesn't expire for up to a year. If you go this route you need to enable
      Cache busting. Why bust cache? Well first you can save http requests because as long as you know you can
      bust cache you can set headers that tell your users browsers not to request your assets for up to a year.
      This means that after someone downloads your JavaScript file they wont make another http request for that
      file for up to one year. So this both speeds page loads and saves bandwidth. Also when it's time to
      deploy an update to your app you can assure that the user immediately receives the new bundle by generating
      a new filename for that bundle. You can force a request for the latest version. So here's our two step
      plan for busting cache. First we need to hash the bundle filename. This way the filename will only change
      if the bundle actually changes. This assures that if we rebuild the app and there are no changes to the
      JavaScript bundle it will continue to have the same filename. Second, since the filename is now generated
      dynamically we need to make sure that the filename reference in the corresponding html file is set
      accordingly. So we'll generate our html dynamically and inject the proper filename as part of the build
      process. To make all this happen we'll continue to use the HtmlWebPackPlugin.</p>
    <h2>Demo: Cache Busting</h2>
    <p>Time for more command line fun. Let's use HtmlWebPackPlugin along with a new tool called WebpackMD5Hash
      to setup cache busting.</p>
    <p>To save bandwidth and avoid needless http requests it can be helpful to configure your web server to send
      far future expiration headers. This way your customers browsers won't request your assets again for up
      to a year. I won't get into how to configure your web server to accomplish setting far future headers but
      the basic idea is to configure your web server to send headers that specify that your applications
      JavaScript files should'nt expire until some date in the distant futur e. But the problem is when you do this
      how do you update your app later. The answer is you have to bust cache by deploying your application with
      a reference to a new filename. Webpack can make cache busting straightforward by generating a deterministic
      cache for each bundle and appending it to the filename. This way the filename only changes when the code
      actually changes. To accomplish this we'll use WebPackMD5Hash. This package hashes files and creates a
      deterministic filename so that our filename will only change when our code changes. To make this happen we'll
      make three small changes in our production webpack config. First let's add the neccessary import at the top
      of webpack.config.prod.js:</p>
    <pre><code class="javascript">import WebpackMD5Hash from 'webpack-md5-hash</code></pre>
    <p>Second we'll add the related plugin down in the array of plugins:</p>
    <pre>
<code class="javascript">
// Hash the files using MD5 so that their names change when the content changes.
new WebpackMd5Hash(),
</code>
    </pre>
    <p>And finally we can put it to use by updating our filename format to use the hash that WebpackMd5Hash
      generates. I'm going to do that by referencing a variable that it generates called chunkhash:</p>
    <pre>
<code class="javascript">
target: 'web',
output: {
  path: path.resolve(__dirname, 'dist'),
  publicPath: '/',
  filename: '[name].[chunkhash].js'
},
</code>
    </pre>
    <p>So this format says name each bundle with a prefix that we defined up in the entry point, then add a dot,
      then add a hash and finally add a .js on the end. And with this setup now our filename's will change only when
      we change the code. So let's run the production build and see this in action. Now we can see that our filename's
      have hashes placed in the middle of them:</p>
    <img src="filenameswithhashes"/>
    <p>And since we're using HtmlWebPackConfig if we open index.html we can see that the references were
      dynamically written for us:</p>
    <img src="dynamicreference"/>
    <p>So there's the vendor reference and there's the main reference. This is a huge benefit of choosing a
      comprehensive tool like webpack. All of these concerns are handled in a cohesive manner with just a little
      declarative code. Remember we just added three lines of code to make this happen. And if you rerun the build
      you'll see that the filename's stay the same but if you change a line of code and rebuild the associated
      bundle's name will change because it will hash to a new value. Slick. Now earlier I mentioned that our
      CSS is getting bundled in our JavaScript file so you may prefer to deploy a separate traditional CSS file
      with the same cache busting setup in production. So let's make taht happen in the next clip.</p>
    <h2>Demo: Extract and Minify CSS</h2>
    <p>Right now, Webpack is embedding all CSS into the JavaScript bundle. That's why we don't see a CSS file
      generated in the dist folder. Our CSS is dynamically generated using JavaScript. Now I prefer to generate
      a traditional separate CSS file for production so that I can utilitise the same cach busting techniques that
      we just saw. I've also noticed a flash of un-styled content when embedding CSS via JavaScript so I believe it's
      worth taking a moment to configure webpack to generate a CSS file for the production build. To extract our CSS
      let's use the extract text plugin. Setting it up requires just three lines of code. First we'll go to the top
      and add the import:</p>
    <pre><code class="javascript">import ExtractTextPlugin from 'extract-text-webpack-plugin'</code></pre>
    <p>Second, let's call the plugin down here in the array of plugins:</p>
    <pre>
<code class="javascript">
// Generate an external css file with a hash in the filename
new ExtractTextPlugin('[name].[contenthash].css'),
</code></pre>
    <p>And we're using the same syntax here as we did for naming our bundle. This way cache busting is enabled for
      our css as well. We'll only get a new filename when the css has actually changed. </p>
    <p>And finally we have to update our css loader down here at the bottom so it will actually call the extract
      text plugin. So I'll replace the current loader with this:</p>
    <pre><code class="javascript">{ test: /\.css$/, loader: ExtractTextPlugin.extract('css?sourceMap')}</code></pre>
    <p>It's important to note that we don't need the style loader anymore and our CSS will be minified to save
      bandwidth so adding the query string sourceMap here on the end declares that webpack should generate a CSS
      sourcemap. And if we open the terminal and run the build let's make sure it still works:</p>
    <pre><code class="hljs">npm run build -s</code></pre>
    <img src="stillworks"/>
    <p>Great, we can see our styles are still applied and if we look over here in our dist folder:</p>
    <img src="distoflder"/>
    <p>We can see our CSS files are created along with a map. And if we look in index.html we can see that the
      reason that our styles are working is because html webpack plugin is also automatically adding in the CSS link
      that's necessary up here at the top. Not bad for three lines of code. So if we look at the dist folder we now
      have seven files:</p>
    <img src="sevenfiles"/>
    <p>An html file, a bundle of our app's JavaScript, a bundle of our vendor libraries and source maps for our
      CSS and our JavaScript. Our entire app is handled by these seven files. And I'd say this app is ready to
      deploy. But wait, before we do. How do we know if we made a mistake? So in the next clip let's setup
      error logging so we're aware when JavaScript errors occur in our users browsers. </p>
    <h2>Error Logging</h2>
    <p>What happens today when your app throws a JavaScript error? Are you aware when JavaScript errors occur in
      production. There's a long list of services available to help in this area: Sentry, TrackJS, New Relic and
      Raygun are a few worth considering. Some services like TrackJS are specific to JavaScript while others like
      New Relic provide broader performance related information. Personally I use TrackJS on a few production apps
      and have been quite happy. So how do you choose among all these options? Well when you're evaluating error
      logging services here are some key concerns to consider. Does it provide good error metadata? For example,
      does it tell me what browser the error occurred in? Does it capture stack traces? Does it capture previous
      actions that the user was performing so that I can reproduce the issue? Does it offer a custom api so I can
      augment error logging with my own contextual data? Does it offer notifications so I can receive emails when
      errors occur? And can I integrate with other popular platforms like Slack so we're notified there instead?
      Can I filter our the noise by aggregating errors together, filtering the list and setting rules for when
      I should be notified using specific thresholds? And finally how much does it cost? Most offer a free trial
      but ultimately you'll end up paying by the month. When you consider all these concerns it's easy to
      understand why people are increasingly paying for services rather than trying to handle this alone and I don't
      recommend tyring to solve this yourself. Doing JavaScript error logging well is a much harder problem than you
      think because errors are very hard to reproduce and fix without the rich metadata and filtering that
      these tools provide. </p>
    <h2>Demo: Error Logging</h2>
    <p>Alright, let's setup error tracking via my preferred error tracking service TrackJS. There are many ways
      to handle error logging in JavaScript but I prefer to use TrackJS because it's easy to setup. Offers
      configurable notifications and boasts an excellent web based interface. That said there are many strong
      players in this market. So much like our conversation on testing the important part here is to just
      pick one. So let's setup TrackJS to log our errors. To get started with TrackJS you need to sign up on their
      web site. And after you sign up there's just two lines of code that you need to inject in your production
      app. The TrackJS docs suggest adding our tracking code in the head of the page to assure that it's loaded
      before any other JavaScript. This assures that it's loaded before any JavaScript errors occur. So
      let's paste this into the head of our index.html:</p>
    <p>And that's all it takes to get rolling. Let's start the build and try this out:</p>
    <img src="tryout"/>
    <p>And the TrackJS docs show how to throw our first error. We can just call TrackJS and tell it to track
      something. So let's open up the console and paste this statement in:</p>
    <pre><code class="hljs">trackjs.track('ahoy trackjs!');</code> </pre>
    <p>And if we did our job right this error should show up in TrackJS. And there it is:</p>
    <img src="trackjs"/>
    <p>We can see all the metadata about the error including the browser, time, url and even the telemetry of any
      previous activities that the users performed like clicking on a button or making an ajax call. This sort
      of information is really helpful for debugging issues. But there's an important tweak to make. Right now
      TrackJS will run in development as well which would just add noise to our error log. So in the next clip I'll
      show how to use conditionals in your html so you can dynamically inject portions of html for different
      environments.</p>
    <h2>Demo: HTML Templates via EmbeddedJS</h2>
    <p>We now have TrackJS logging errors but it would be nice if it only ran in production since logging
      errors in our development environment isn't useful and would just add noise to our error logging reports.
      I want to use this opportunity to show you a way to add conditional logic to your html so that this code:</p>
    <img src="onlytoindex"/>
    <p>Is only added to index.html in production. So instead let's use the templating engine support that's built
      in to HtmlWebpackPlugin to add conditionals to our template. HtmlWebpackPlugin supports a number of templating
      languages out of the box including Jade, EJS, Underscore, Handlebars and HTML Loader. And if you don't
      specify a loader then it defaults to EmbeddedJS or EJS for short. So let's just use EJS since it's the default
      and it's easy to use. You can read about the EJS syntax at EmbeddedJS.com and there's a handy repl on this
      page so you can play around with the syntax and learn from rapid feedback that will display in this box. But for
      our purposes we just need to declare a simple conditional. We want to inject the TrackJS code but only during
      our production build. Let's do that with a little bit of EJS. First let's store the TrackJS token that we
      were just assigned on the web site in our webpack config. We need to add it right here below the call to
      inject:</p>
    <pre>
<code class="javascript">
// Properties you define here are available in index.html
// using htmlWebpackPlugin.options.varName
trackJSToken: 'obsfucated token'
</code>
    </pre>
    <p>Any properties that you define here within the HtmlWebpackPlugin will be available within our index.html
      file. You'll see how to call this as we shift our focus over to index.html. And the token that your defining
      here is the token that you should have received right here when you setup TrackJS. And now let's shift our
      focus over to index.html. In here we're going to use EJS to declare that this section should only be
      rendered when we have a TrackJS token defined in webpack config. So let's say:</p>
    <pre>
<code class="javascript">
<% if (htmlWebpackPlugin.options.trackJSToken) { %>
  &lt;! BEGIN TRACKJS --&gt;
  &lt;! END TRACKJS --&gt;
  &lt;% } %&gt;
</code>
    </pre>
    <p>And now we can reference this variable instead of the actual token right here.</p>
    <p> Of course to reference it as a variable we need to wrap it in the angle bracket percent syntax:</p>
    <pre>
<code>
&lt;script type="text/javascript"&gt;window.trackJs = { token: &lt;%= htmlWebpackPlugin.options.trackJSToken%&gt;' };
  &lt;/script&gt;
</code>
    </pre>
    <p>And be sure to wrap this in single quotes. So now if a TrackJS token is defined within our webpack config this
      section of code will be rendered into our index.html otherwise it won't exist. And since we've only defined
      our TrackJS token within our production config. This section of code will only render for production. So this
      way our errors are only tracked for production. Now of course once you've added this code your html file
      arguably isn't an html file anymore. It's now an EJS file. So you can consider changing the file extension to
      .ejs but I prefer to keep the extension html so that editors will properly apply code coloring to the file
      contents. Let's go ahead and add a useful comment up here to the top of our html file:</p>
    <img src="topcomment"/>
    <p>It just explains what's going on. Rather than changing the extension to EJS I figured this comment is
      sufficient and yeah it's a big comment but who cares this will be stripped out by the build process anyway. Ok,
      and with that setup we should be able to:</p>
    <pre><code class="hljs">npm run build -s</code> </pre>
    <p>And make sure that this is getting injected as we expected. And if we look in the browser and View page source
      we can see now that our call to TrackJS is here and the token is getting injected into our page as expected.
      And with this I think it's safe to say that we're at a point that we can confidently talk about shipping.</p>
    <p>Let's wrap up this module with a short summary.</p>
    <h2>Summary</h2>
    <p>In this final module we wrapped our development environment by creating a robust automated production build.
      We configured webpack for production and enabled minification, we generated sourcemaps so we can still debug
      the app when we're in production and we used HtmlWebpackPlugin to minify our html and dynamically insert the
      necessary script references into index.html. We setup cache busting using HtmlWebpackPlugins so that we
      can save bandwidth and still assure that users get the latest JavaScript when it changes. We enabled bundle
      splitting so users don't have to download the entire application when only part of the code changes. And we
      setup error logging so we know when bugs are thrown in production and we have rich metadata to help us
      reproduce any errors that occur. Finally we saw how to use EmbeddedJS for conditionally rendering portions
      of our html for production. We used EJS to ensure that our error logging only runs in production. And all
      of this goodness prepares us for the final module. We're headed for production. In the next module we'll setup
      an automated deployment process and we'll also discuss methods for keeping your applications updated with your
      latest starter kit changes over time.</p>
    <h1>Production Deploy</h1>
    <h2>Intro</h2>
    <p>Congratulations on making it to the final module. We're finally ready to discuss the last missing piece:
      production deployment. We'll begin this final module by discussing the merits of separating the user interface
      from your applications api into completely separate projects. We'll briefly discuss the wide variety of
      cloud hosting providers. Then we'll create an automated deployment for both the UI and the API using popular
      cloud service providers. We'll wrap up the course by discussing approaches for keeping existing projects
      updated as your starter kit is enhanced over time. And I'll quickly provide some tips for further inspiration
      as you start designing your own development environment. And I'll close out the course with a short challenge.
      Alright, let's dig in and wrap this up. </p>
    <h2>Separating the UI from the API</h2>
    <p>Throughout the course we've hosted our production api in the same project as our front end. This was useful
      for keeping our demo app simple but I don't necessarily recommend doing this in a real app. Instead, I prefer
      to keep the front end and the api in completely separate projects. Here's why:</p>
    <ol>
      <li>First, a static front end is easy to deploy. You just need to upload the static files that we wrote to
        the dist folder to a public web server. And since your only uploading the UI you don't have to worry about
        regressing your service layer in anyway - they're completely separate.
      </li>
      <li>Second, it separates concerns. It provides you the ability to have separate teams building the front and
        the back end in parallel without stepping on each others toes. The UI team can code against the mock api that
        we setup and when the real api is ready they can point there instead. Assuming that you have separate people
        focus on the UI and the API your application becomes easier to understand because a given developer can
        focus on one of these two concerns in isolation. This clear separation also means that you can scale the
        backend separately. This is especially useful when you create an api that's going to be consumed by
        multiple applications since the traffic for the api may dramatically differ from the UI's traffic.
      </li>
      <li>A static UI is also cheap to host. When your front end is just static html, javascript and CSS you can
        select virtually any host in the world because all you need is a host that can server static files. In fact
        a static front end can be served from a Content Delivery Network also known as a CDN for short. Content
        Delivery Networks handle caching and scalability for you.
      </li>
      <li>A CDN is especially useful for high traffic sites and applications that are used around the world.
        Since CDN's intelligently serve assets from the closest physical server to speed downloads.
      </li>
      <li>Finally hosting separately of course means that your free to use whatever technology that you like for
        the backend.Our API is currently built using JavaScript but if your team prefers to build API's in a
        different language like C#, Java, Ruby and so on. Keeping the UI and API separate gives you that option.
      </li>
    </ol>
    <p>And now that I have set the stage for separating the UI and the API let's talk about automated deployments.</p>
    <h2>Cloud Hosting</h2>
    <p>Of course, the first question when considering an automated deployment is - where should we host the app?
      Virtually any cloud host can serve a JavaScript app these days with minimal configuration. Popular services
      include:</p>
    <ul>
      <li>Amazon Web Services</li>
      <li>Microsoft Azure</li>
      <li>Heroku</li>
      <li>Firebase</li>
      <li>Google Cloud Platform</li>
      <li>Netlify</li>
      <li>Github pages</li>
      <li>Surge</li>
    </ul>
    <p>Most of these services offer the power to do far more than just host a JavaScript app. Although Netlify,
      Github pages and Surge are unique options on this list because they're focused solely on serving static files.
      The process for automated deployment will differ by hosting provider. For this module I'm going to host the
      API and User Interface separately. For the API I'll use Heroku because it's popular, powerful and has an
      elegant automated deployment process. For the user interface I'll use Surge because it's simple to setup
      and unlike many of these other options it's focused solely on hosting static files which is what our
      automated build for the UI generates. </p>
    <h2>Demo: Automated API Deploy via Heroku</h2>
    <p>Alright, back to the editor. Let's setup an automated deployment of our API to Heroku. We're going to create
      a completely separate project for handling this to show how we can host and manage our UI and API separately. As
      I just discussed there are many benefits to completely separating your UI and API projects. In our example app,
      we created an API end point hosted via express. So we need to select a node friendly host for the API. Let's
      host our API on Heroku. Heroku offers a really slick setup for automated deployments that integrates with
      github and it offers a free option that's perfect for showcasing an automated deployment. Heroku's docs already
      do an excellent job of walking you through setting up an account and creating a new NodeJS project. If you go to
      their docs here on heroku.com and click on NodeJS and then click on Getting Started on Heroku with Node.js.
      So if you want to follow along with me please pause this video and go through the introduction and setup
      steps on the Node.js getting started page then come back here to continue. Ok for the rest of this clip I'm
      going to assume that you signed up for Heroku and walked through the introduction and setup steps for Node.js.
      On step three of their setup process which is called prepare the app Heroku provides a link to a sample app
      which you can clone. However instead of using this I created a separate starter kit for you that will work
      well with Heroku. This repository contains a slightly modified version of Heroku's starter kit that includes
      our api so this should help you get started quickly. If you want to use this repository to follow along just
      click Fork up here to fork the repository. This will make sure that you have a completely separate copy that
      you can work with to make sure that you have the proper rights to work with it in Heroku. Now I already have
      this repository pulled down on my local machine so let's jump back to VSCode and walk through it.</p>
    <p>As usual be sure to run <code class="hljs">npm install</code> after forking the api repo. There are only
      five files in this repository so let's review each.</p>
    <p>First package.json contains only two dependencies: express and cors. We'll use cors to enable cross origin
      calls since we'll be calling our Heroku based api from a different domain. Make sure that repository field
      down here points to your repository if you create your own:</p>
    <img src="repo"/>
    <p>And there's only one npm script necessary that starts the app.</p>
    <pre><code class="hljs">"start": "node index.js"</code> </pre>
    <p>index.js should look quite familiar to you. It's a slightly modified version of the dist server that we
      created in the previous module. To keep things simple I'm using the CommonJS style up here on line 1 to
      require express since that's the syntax that node understands:</p>
    <pre><code class="javascript">var express = require('express');</code></pre>
    <p>I'm also referencing the cors package which we're enabling down here to make ensure that we can call
      the Heroku based api from our UI which will be hosted on a different Url. To clarify this is
      necessary because Cross Origin Resource Sharing must be enabled to make Ajax calls to a different domain.
      We could of course transpile and bundle our code in this project but I'm keeping this project as simple
      as possible so that you can see how to work with Heroku. </p>
    <p>If you diff this file with the diff server that we setup in the previous module you'll also notice that
      we're calling open to open a browser there. However this just starts up express and displays a message to
      the console. So to try this out you'll need to open the Url in your browser manually. I also left out
      GZip compression. Again just to try and keep this as simple as possible. Ok this leaves us with two new
      files that help us configure our app for Heroku. The first is app.json which describes our app to Heroku.
      There are many potential properties that we can define here but we're going to keep our app.json simple.
      We'll just define the name, a description, the repository where our project can be found and then a few
      keywords. </p>
    <p>The other new file here is Procfile. The Procfile declares the command that Heroku should run.
      That's why there's just one line here. We're telling Heroku to run node on our index.js file. And this is all
      that Heroku needs to host our Node and express based API. Now, of course, Im deliberately leaving out all
      the complexities of testing, transpiling, linting, bundling and more in this project so that you can
      clearly see what it takes to create an automated deployment to Heroku. But of course you can feel free to
      start adding those in once your comfortable with hosting on Heroku. So now we should be ready to complete
      an automated deploy to Heroku. I've already signed up for Heroku and installed the Heroku CLI. So now
      let's open up the terminal and type</p>
    <pre><code class="hljs">heroku login</code></pre>
    <p>At this point you'll be prompted for your email and your Heroku password. And after entering your
      credentials you should see your email listed in blue which shows that you are logged in successfully. And note
      that you may receives some warnings about file permissions so consider tightening file permissions for
      security if you like. Now it's time to configure our app to work with Heroku so we can type:</p>
    <pre><code class="hljs">heroku create</code></pre>
    <p>This will prepare Heroku to receive our app. This command returns a Url and if we load it we can see a
      welcome message. Heroku generates a random name for your app or you can pass a parameter to specify your
      own name.</p>
    <p>Now that we've created our app we need to configure a git remote that points to our Heroku application.
      So let's go back to the command line and we'll say:</p>
    <pre><code class="hljs">heroku git:remote -a mysterious-dawn-16770</code></pre>
    <p>We'll pass it the name of the app that we were assigned right up here. And now that we've set the git
      remote we should be ready to publish the app. We can say:</p>
    <pre><code class="hljs">git push heroku master </code></pre>
    <p>We can see the deployment output as it builds the source and pushes our app up to Heroku it displays
      the random url where our app is hosted so you'll have a different url than me if you're following along. And
      of course for a real app you'll want to specify a domain name that you've registered. But for now we should
      be able to take this url and go over to the browser and when I load it up:</p>
    <img src="heloworld"/>
    <p>There we go. We can see our Hello World. And if I go to /users I can see the Json coming back for our
      users so we have our api now hosted in production. And anytime that we make changes to our API we'll just
      commit our changes and then run:</p>
    <pre><code>git push heroku master</code></pre>
    <p>To be able to push our changes up to Heroku. Heroku will take the code from Github and deploy it to our
      Url. Slick. And now that we have our API running in production via Heroku let's jump over to our UI project
      and update it so that it will hit our Heroku hosted API.</p>
    <p>To do that let's open up baseUrl.js. Note tha right now we are either using the mock api which is hosted
      at 3001 or we were assuming that we were hosting express locally for production. Now instead we have a new
      Url to use for production. So I'll just paste in the Heroku Url that I was assigned in the previous step:</p>
    <pre>
<code class="javascript">
return getQueryStringParameterByName('useMockApi') ? 'http://localhost:3001/' :
  'https://mysterious-dawn-16770.herokuapp.com/';
</code>
    </pre>
    <p>And make sure that you include the trailing slash on the end. And now we can also open up distServer.js and
      we can remove this section:</p>
    <img src="thissection"/>
    <p>Because we're going to be hitting Heroku instead of local when we do our production build. This way our
      production build is more realistic. We know that our production api will be hosted on Heroku and we're
      going to host our UI on a separate service here in a moment. But now when we do a production build of our
      UI it will hit our production API hosted on Heroku. In the next clip let's setup an automated deployment
      for the user interface so that we can see all this work together.</p>

    <h2>Demo: Automated UI Deploy via Surge</h2>
    <p>Ok, now it's time to code our automated UI deployment. Here's our goal for the process that we're going to
      setup for the front end. It's a three step process to get code into production. First we run:</p>
    <pre><code class="hljs">npm start</code></pre>
    <p>to do our development. Once we're done coding we type:</p>
    <pre><code class="hljs">npm run build</code></pre>
    <p>to build the application in production mode. This opens the application's production build on our local machine
      so we can verify that everything looks good. If we're happy then it's time to deploy to production so we can
      type:</p>
    <pre><code class="hljs">npm run deploy</code></pre>
    <p>This will automatically push the final built application up to our production host. Of course we already
      setup steps one and two in the previous modules so now it's time to focus on this final step. Alright it's time
      for our final coding session in this course but this one is critical - it takes our front end public. To do
      so we'll host our static front end on sure. Let's make it happen.</p>
    <p>I'm a big fan of surge because it's a low friction front end and for all the reasons I mentioned earlier
      I strive to build static front ends. Getting started with Surge couldn't be easier. Typically you'd install it
      globally using npm. But we don't need to because we already installed it at the beginning of the course since
      it's listed in our package.json right down here. And we also don't need to install it globally since we're going
      to run it via an npm script. Remember node's bin folder is added to the path automatically for npm scripts. So
      to setup surge I'm going to add one whopping line of code here in package.json:</p>
    <pre><code class="json">"deploy": "surge ./dist"</code></pre>
    <p>Yes, it's seriously that easy. And that's why I love Surge. Now first of course we need to call:</p>
    <pre><code class="json">npm run build -s</code></pre>
    <p>So that we have something to push out to production. And when our app starts up in production mode we can
      see the data coming back as expected. If we come in and inspect the network we should see, when we refresh, that
      we're making a call to Heroku as expected. And with this setup we can now hit Ctrl+C and type:</p>
    <pre><code class="hljs">npm run deploy</code></pre>
    <p>We can see surge get's called. It assigns a random domain and when we hit enter it says success. And now
      we know our app is up in production at this random url. So if I open a new tab and load it - there you go.
      We can see our app loading in production and using the Heroku api for the data. Success. Of course the delete link
      wont work since we never added that functionality to our api but if you want a challenge you could certainly
      add a database behind the scenes to support adds and deletes. If we open the browser tools and go to the network
      tab we can see that Surge serves our assets using GZip by default. And of course you'll likely want to setup
      a custom domain and Surge will let you use your own domain for free. Or if you just want to request a subdomain
      you can specify it via a command line flag. There are quite a few nice tweaks that you can make but you get the
      idea. We should strive to build static front end's and if you do Surge is awesome. Of course, your starter kit
      is likely to change over time so in the next clip let's talk about how to keep existing projects updated as our
      starter kit changes.</p>
    <h2>Starter Kit Update Approaches</h2>
    <p>Now once youv'e created your teams starter kit how do you keep existing projects updated over time as you
      enhance your development environment down the road. Let's review a few approaches. Let me first run through
      a common scenario to help clarify the problem that we're discussing. Imagine that your team watches this
      course and creates a development environment that works great. In the first quarter you launch your first project
      using your new starter kit. Then, in the second quarter, you launch another project successfully using the same
      starter kit. In quarter three, you learn some lessons, upgrade some libraries and tweak your starter kit with
      various enhancements and bug fixes. The question is, how do you easily get these enhancements into the products
      that you launched earlier this year. Of course, one way is to simply manually update these previous projects
      by hand. And that works but we're developers so let's talk about some more automated approaches.</p>
    <p>Here are three more automated ways to handle updates to starter kits over time: Yeoman, Github and npm. In the
      next few clips let's discuss each of these options in more detail.</p>
    <h2>Option 1: Yeoman</h2>
    <p>Yeoman is a handy scaffolding tool for starting new projects so once your happy with your development
      environment you can create a Yeoman generator. This makes it easy for people to create new projects by typing
      yo and then the name of your generator. Yeoman hosts a long list of generators that will give you a head start on
      some of what we've covered in this course. Few will cover all of the features that we just implemented but it's
      another great place to check for inspiration or as a good starting point for your framework or library.</p>
    <p>Assuming that you've created a Yeoman generator for your development environment it's a three step process
      to update an existing project later. First, be sure that you've committed all your code to your source control
      system, then re-run the generator on your existing project. Yeoman will prompt you for each file that's being
      overwritten. Then you can diff the files and manually resolve any conflicts that occur. Of course there's much
      more to learn about Yeoman so if you want to know more check out the Yeoman fundamentals course by Steve
      Michelotti.</p>
    <h2>Option 2: Github</h2>
    <p>Another approach for updating projects is to use Github. With this process you begin by hosting your project
      , of course, on Github and then you fork the starter kit when you start any new projects. This way you can
      pull changes from master as the starter kit is enhanced over time. </p>
    <h2>Option 3: npm</h2>
    <p>Another approach is to wrap your starter kit in an npm package. With this approach you abstract away the
      configuration and build scripts behind and npm package and then you update the npm package to receive the
      latest changes. This approach has the advantage of abstracting complexity away and it's also the simplest
      to update since you don't need to resolve conflicts like the other two approaches. However, this advantage
      also has an obvious downside. Your restricted from tweaking anything inside the npm package for a given project.
      So this approach is great if you want to programmatically enforce that all projects use the exact same config
      but some may find this approach overly restrictive. Let's talk about this approach in a little more detail
      because it's becoming increasingly popular.</p>
    <p>Depending on the complexity of your starter kit updating an existing project manually isn't much work so you
      might consider this hybrid approach. Let's walk through the files that are in our demo's starter kit. The
    most significant piece is in buildScripts. This is the easiest thing to centralize. You can move all of
    buildScripts to an npm package. The other big piece is package.json. There are two sources of complexity here:
      the scripts and the dependencies. For the scripts you can streamline your npm scripts to just call
       buildScripts instead. Since you can put all your build scripts in an npm package this means that your scripts
      and package.json are nothing but a list of function calls to your separate scripts. This effectively
      centralizes your build scripts allowing for bug fixes and tweaks in the future. Webpack's config is just a
      chunk of JSON so it need not be stored in a webpack config file. Instead you can move it into a build scripts
      npm package as well so that it's centralized. The ESLint configuration can be centralized by creating your
      own preset. This way each project can define it's own .eslintrc but use the preset that's stored in npm
      as the baseline. So this covers most of the moving parts in our starter kit. So what's left?
    </p>
    <p>Well the approaches I outlined on the previous slide centralized all the items on the left using npm.
    That's a significant win since it's the vast majority of the starter kit's code. So what files would we still
    have to update manually? </p>
    <ul>
      <li>.editorconfig which is unlikely to change much over time</li>
      <li>.babelrc which contains very little code</li>
      <li>CI config (.travis.yml Appveyor.yml) which is also unlikely to change over time.</li>
      <li>Package references in package.json. These are easy to update with existing npm tooling so again not
      necessarily that big of a deal</li>
    </ul>
    <p>I'm a fan of this hybrid approach. It provides most of the benefits of centralization without the cost of
    creating and maintaining a Yeoman generator. And it gives us a lot of flexibility as well since we can decide
    what's worth centralizing and what isn't. So that's three different ways to keep your projects updated over
    time. Let's close out the course by discussing some sources for inspiration as you move forward on creating your
    own starter kit. </p>
    <h2>Inspiration</h2>
    <p>In this course, you saw a massive list of choices and I shared my recommendations but there are litterally
    dozens of other ways to build a JavaScript development environment. So let me share some sources for further
    inspiration.</p>
    <p>If you work in React check out Andrew Farmer's excellent list of React boilerplates and starter kits.
    Andrew has catalogued over a hundred starter kits into a curated filterable list. My starter kit: React Slingshot
    is just one of many in this list and I walk through how to build it in module 2 of my React and Redux in ES6
    course. If you work in Angular check out the awesome AngularJS list. AngularJS developers often call their starter
    kits seed projects so check out the full list at this address.</p>
    <p>Of course there are many more popular JavaScript frameworks and libraries out there so if your looking for
    inspiration just type the name of your favourite JavaScript framework into Google along with one of these
      terms:</p>
    <ul>
      <li>Devedlopment environment</li>
      <li>Boilerplate</li>
      <li>Starter kit</li>
      <li>Starter project</li>
      <li>Seed</li>
    </ul>
    <p>The point is I've just shown you one way of getting this done. So start searching. Let's wrap up the course
    now with my final challenge.</p>
    <h2>Challenge</h2>
    <p>I like to end my courses with a relevant challenge and this one is no different. My challenge to you is easy.
      Just send a meeting invitation to your team. Why? Because the first step in this process is to start a
      conversation. In this meeting you need to answer some questions.</p>
    <ul>
      <li>Would you benefit from a starter kit? If you expect to start any new projects using he same technology
      stack in the near future then it's worth discussing creating a starter kit.</li>
      <li>What pain points do we have in JavaScript today? Are we struggling with testing, broken builds, time
      consuming manual or faulty deployments, inconsistent coding styles. We just discussed a long list of options
      for solving all of these pain points.</li>
      <li>Would we benefit from a demo application? Are people that join our team clear about how we operate. Do they
      understand our opinions on directory structures, file naming, api calls and more. A demo app clearly
      conveys many of these decisions. These questions should help provide your team with a clear vision on how to
      move forward.</li>
    </ul>
    <p>So, hey, this is a pretty easy challenge. Just send an email and get the meeting scheduled.</p>
    <h2>Summary</h2>
    <p>And that's a wrap. In this final module we started by discussing why the UI and the API can be useful
    including the ability to deploy the UI and the API alone, separation of concerns (which allows separate teams
    to manage each of these), the ability to select cheap hosts that handle static assets and the flexibility to
    select whatever API technology that you like.</p>
    <p>We reviewed a list of potential cloud hosting providers but ultimately created an automated deployment using
    Heroku to host the API and Surge to host the UI. We discussed approaches for keeping your projects updated
    with bug fixes and enhancements as your starter kit improves over time including:</p>
    <ul>
      <li>Yeoman</li>
      <li>Github</li>
      <li>npm</li>
    </ul>
    <p>And I quickly mentioned a few resources for inspiration including some terms that you can use to help you
    search for starter kits that are specific to your technology stack. And I wrapped up with my very simple
    challenge. Set up a meeting with your team to discuss the path forward. And that's a wrap. Please share
    your links to your starter kit in the course discussion. I'm excited to see what you create. Thank's so much
    for watching.</p>
    <script src="bundle.js"></script>


  </body>
</html>
